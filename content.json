{"pages":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/about/about.html"},{"title":"gallery","text":"","link":"/album/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"分布式存储管理","text":"为什么直接采用关系模型的分布式数据库并不能适应大数据时代的？ NoSQL 特点 ## 为什么直接采用关系模型的分布式数据库并不能适应大数据时代的？ 1. 规模效应所带来的压力 - 传统数据库倾向于纵向扩展(Scale-Up)，即增加单台计算机的性能 - 适应大数据的数据库系统的应该具有良好的横向扩展(Scale-Out)，即为集群增加一台计算机 2. 数据类型的多样化 传统数据类型： - 结构化数据 大数据时代的数据类型： - 结构化数据 - 半结构化数据 - 非结构化数据 3. 设计理念所带来的冲突 - 关系型数据库 One size fits all ,即面对不同问题不需要重新考虑数据管理问题 简单来说就是，单一模式可以适应所有变化 - 新理念 \"One size fits one\" 和 \"One size fits domain\" 数据库的事务特性 传统数据库ACID特性 A(Atom，原子性)，C(Consistency，一致性)，I(Isolation,隔离性)，D(Durability,持久性) 大数据时代的数据库BASE。 Basically Available(基本可用)，Soft State(柔性状态)，Eventually Consistency(最终一致性)根据分布式领域著名的CAP理论来看，ACID追求一致性C,而BASE更加关注A一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）新型数据库Spanner NoSQL 特点$ 模式自由（Schema-free） 支持简易备份（Easy Replication Support) 简单应用程序接口（Simple API） 最终一致性（或说支持BASE特性，不支持ACID特性） 支持海量数据（Huge Amount of Data）","link":"/post/4faba951.html"},{"title":"DOM解析","text":"DOM 的工作方式是： 处理步骤 hadoop配置文件解析的特别说明 hadoop 配置文件解析完整代码 说明：这里主要分析hadoop的DOM解析 DOM 的工作方式是：$ 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档处理步骤$ 获得用于创建DOM解析器的工厂对象1DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); 可以设置一下参数[可选]12345678docBuilderFactory.setIgnoringComments(true);docBuilderFactory.setNamespaceAware(true);boolean useXInclude = !wrapper.isParserRestricted();try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28);} 获得解析XML的DocumentBuilder对象1DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); 获取根节点下的所有节点1NodeList props = root.getChildNodes(); 遍历节点123456789101112for(int i = 0; i &lt; props.getLength(); ++i) { //获取节点 Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; //prop.getTagName()获取节点内的值 if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { ... } hadoop配置文件解析的特别说明$ 对DocumentBuilderFactory做的处理 忽略XML文档中的注释1docBuilderFactory.setIgnoringComments(true); 支持XML命名空间1docBuilderFactory.setNamespaceAware(true); 支持XML包含机制12345try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \")for parser \" + docBuilderFactory, var28);} XInclude机制允许将XML文档分解为多个可管理的块，然后将-一个或多个较小的文档组装成一个大型文档。 hadoop 配置文件解析完整代码$123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157private Configuration.Resource loadResource(Properties properties, Configuration.Resource wrapper, boolean quiet) { String name = \"Unknown\"; try { Object resource = wrapper.getResource(); name = wrapper.getName(); DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); docBuilderFactory.setIgnoringComments(true); docBuilderFactory.setNamespaceAware(true); boolean useXInclude = !wrapper.isParserRestricted(); try { docBuilderFactory.setXIncludeAware(useXInclude); } catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28); } if (wrapper.isParserRestricted()) { docBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true); } DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); Document doc = null; Element root = null; boolean returnCachedProperties = false; if (resource instanceof URL) { doc = this.parse(builder, (URL)resource); } else if (resource instanceof String) { URL url = this.getResource((String)resource); doc = this.parse(builder, url); } else if (resource instanceof Path) { File file = (new File(((Path)resource).toUri().getPath())).getAbsoluteFile(); if (file.exists()) { if (!quiet) { LOG.debug(\"parsing File \" + file); } doc = this.parse(builder, new BufferedInputStream(new FileInputStream(file)), ((Path)resource).toString()); } } else if (resource instanceof InputStream) { doc = this.parse(builder, (InputStream)resource, (String)null); returnCachedProperties = true; } else if (resource instanceof Properties) { this.overlay(properties, (Properties)resource); } else if (resource instanceof Element) { root = (Element)resource; } if (root == null) { if (doc == null) { if (quiet) { return null; } throw new RuntimeException(resource + \" not found\"); } root = doc.getDocumentElement(); } Properties toAddTo = properties; if (returnCachedProperties) { toAddTo = new Properties(); } if (!\"configuration\".equals(root.getTagName())) { LOG.fatal(\"bad conf file: top-level element not &lt;configuration&gt;\"); } NodeList props = root.getChildNodes(); Configuration.DeprecationContext deprecations = (Configuration.DeprecationContext)deprecationContext.get(); for(int i = 0; i &lt; props.getLength(); ++i) { Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { if (wrapper.isParserRestricted() &amp;&amp; \"http://www.w3.org/2001/XInclude\".equals(prop.getNamespaceURI())) { throw new RuntimeException(\"Error parsing resource \" + wrapper + \": XInclude is not supported for restricted resources\"); } LOG.warn(\"Unexpected tag in conf file \" + wrapper + \": expected &lt;property&gt; but found &lt;\" + prop.getTagName() + \"&gt;\"); } NodeList fields = prop.getChildNodes(); String attr = null; String value = null; boolean finalParameter = false; LinkedList&lt;String&gt; source = new LinkedList(); //遍历所有节点，并根据情况设置对象的成员变量properties和finalParameters for(int j = 0; j &lt; fields.getLength(); ++j) { Node fieldNode = fields.item(j); if (fieldNode instanceof Element) { Element field = (Element)fieldNode; if (\"name\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { attr = StringInterner.weakIntern(((Text)field.getFirstChild()).getData().trim()); } if (\"value\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { value = StringInterner.weakIntern(((Text)field.getFirstChild()).getData()); } if (\"final\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData()); } if (\"source\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { source.add(StringInterner.weakIntern(((Text)field.getFirstChild()).getData())); } } } source.add(name); if (attr != null) { if (deprecations.getDeprecatedKeyMap().containsKey(attr)) { Configuration.DeprecatedKeyInfo keyInfo = (Configuration.DeprecatedKeyInfo)deprecations.getDeprecatedKeyMap().get(attr); keyInfo.clearAccessed(); String[] arr$ = keyInfo.newKeys; int len$ = arr$.length; for(int i$ = 0; i$ &lt; len$; ++i$) { String key = arr$[i$]; this.loadProperty(toAddTo, name, key, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } else { this.loadProperty(toAddTo, name, attr, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } } } } if (returnCachedProperties) { this.overlay(properties, toAddTo); return new Configuration.Resource(toAddTo, name, wrapper.isParserRestricted()); } else { return null; } } catch (IOException var29) { LOG.fatal(\"error parsing conf \" + name, var29); throw new RuntimeException(var29); } catch (DOMException var30) { LOG.fatal(\"error parsing conf \" + name, var30); throw new RuntimeException(var30); } catch (SAXException var31) { LOG.fatal(\"error parsing conf \" + name, var31); throw new RuntimeException(var31); } catch (ParserConfigurationException var32) { LOG.fatal(\"error parsing conf \" + name, var32); throw new RuntimeException(var32); }}","link":"/post/e9fcf702.html"},{"title":"大数据介绍","text":"什么是大数据 数据单位 大数据的特征 大数据相关技术 大数据带来的变革 大数据小故事 其他故事 大数据初步学习路线 大数据处理流程 大数据处理模型 科普 什么是大数据# 百度百科的定义,大数据（BIG DATA），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 在目前的业界尚未对大数据由清晰明确的定义, 它的第一次出现是在麦肯锡公司的报告中出现的, 在维基百科上的较为模糊的定义是很难运用软件的手段获取大量的内容信息, 对其处理后整理得出的数据集合。其他计算机学科的学者给出的定义是数据的尺度极为巨大, 常规的数据处理软件无法对数据识别、存储和应用的海量数据信息 维基百科的定义，大数据是指无法在可承受的时间范围内用常规软件工具进行捕捉、管理和处理的数据集合。 研究机构Gartner定义，“大数据”是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 数据单位#1MB = 1024KB、1GB = 1024MB1TB = 1024GB、1PB = 1024TB 大数据的特征# 容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息； 种类（Variety）：数据类型的多样性； 速度（Velocity）：指获得数据的速度； 可变性（Variability）：妨碍了处理和有效地管理数据的过程。 真实性（Veracity）：数据的质量 复杂性（Complexity）：数据量巨大，来源多渠道 价值（value）：合理运用大数据，以低成本创造高价值 大数据相关技术# 数据采集： OLAP(联机分析处理)和数据挖掘的基础。ETL工具负责将分布的、异构的数据源进行抽取，抽取到中间层，进行清洗、转换、集成，（不过对于负责的逻辑处理不会这么干，用Spark或者其他的进行处理），最后放到数据仓库中存储，如Hive 数据存取： 关系型数据库、NoSQL(Not Only SQL,泛指非关系型数据库)，SQL等 基础架构： 云存储、分布式文件存储等 数据处理： 自然语言处理(Natural Language Processing, NLP) 数据分析： 假设检验、显著性检验、差异检验、差异分析、相关性分析、T检验、方差分析、卡方分析、偏相关性分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、预测和残差分析、岭回归、Logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、判别分析、对应分析、快速聚类和聚类法、对应分析、多元对应分析等 数据挖掘： 分类（Classification）、估计（Estimation）、预测（Prediction)、相关性分析或关联规则（Association）、复杂数据类型挖掘（Text、Web、图形图像、视频、音频等） 模型预测： 预测模型、机器学习、建模仿真 结果呈现： 云计算、标签云、关系图等 大数据带来的变革#大数据小故事# &emsp;&emsp;最早关于大数据的故事发生在美国第二大的超市塔吉特百货（Target）。孕妇对于零售商来说是个含金量很高的顾客群体。但是他们一般会去专门的孕妇商店而不是在Target购买孕期用品。人们一提起Target，往往想到的都是清洁用品、袜子和手纸之类的日常生活用品，却忽视了Target有孕妇需要的一切。那么Target有什么办法可以把这部分细分顾客从孕妇产品专卖店的手里截留下来呢？ &emsp;&emsp;为此，Target的市场营销人员求助于Target的顾客数据分析部的高级经理Andrew Pole，要求他建立一个模型，在孕妇第2个妊娠期就把她们给确认出来。在美国出生记录是公开的，等孩子出生了，新生儿母亲就会被铺天盖地的产品优惠广告包围，那时候Target再行动就晚了，因此必须赶在孕妇第2个妊娠期行动起来。如果Target能够赶在所有零售商之前知道哪位顾客怀孕了，市场营销部门就可以早早的给他们发出量身定制的孕妇优惠广告，早早圈定宝贵的顾客资源。 &emsp;&emsp;可是怀孕是很私密的信息，如何能够准确地判断哪位顾客怀孕了呢？Andrew Pole想到了Target有一个迎婴聚会（baby shower）的登记表。Andrew Pole开始对这些登记表里的顾客的消费数据进行建模分析，不久就发现了许多非常有用的数据模式。比如模型发现，许多孕妇在第2个妊娠期的开始会买许多大包装的无香味护手霜；在怀孕的最初20周大量购买补充钙、镁、锌的善存片之类的保健品。最后Andrew Pole选出了25种典型商品的消费数据构建了“怀孕预测指数”，通过这个指数，Target能够在很小的误差范围内预测到顾客的怀孕情况，因此Target就能早早地把孕妇优惠广告寄发给顾客。 &emsp;&emsp;那么，顾客收到这样的广告会不会吓坏了呢？Target很聪明地避免了这种情况，它把孕妇用品的优惠广告夹杂在其他一大堆与怀孕不相关的商品优惠广告当中，这样顾客就不知道Target知道她怀孕了。百密一疏的是，Target的这种优惠广告间接地令一个蒙在鼓里的父亲意外发现他高中生的女儿怀孕了，此事甚至被《纽约时报》报道了，结果Target大数据的巨大威力轰动了全美。 &emsp;&emsp;根据Andrew Pole的大数据模型,Target制订了全新的广告营销方案，结果Target的孕期用品销售呈现了爆炸性的增长。Andrew Pole的大数据分析技术从孕妇这个细分顾客群开始向其他各种细分客户群推广，从Andrew Pole加入Target的2002年到2010年间，Target的销售额从440亿美元增长到了670亿美元。 &emsp;&emsp;我们可以想象的是，许多孕妇在浑然不觉的情况下成了Target常年的忠实拥泵，许多孕妇产品专卖店也在浑然不知的情况下破产。浑然不觉的背景里，大数据正在推动一股强劲的商业革命暗涌，商家们早晚要面对的一个问题就是：究竟是在浑然不觉中崛起，还是在浑然不觉中灭亡 其他故事# Google根据搜索关键字分析流感病毒H1N1 2008年金融危机 大数据初步学习路线# 技术 工具 JAVA 面向对象的编程语言 Linux 类Unix操作系统 Hadoop生态圈 1、HDFS 解决存储问题存储极大数目的信息（terabytes or petabytes），将数据保存到大量的节点当中。支持很大单个文件。提供高可靠性，是指一个或多个节点故障，系统仍然可以继续工作提供数据快速访问 2、MapReduce 解决计算问题它有个特点就是不管多大的数据只要给它时间它就能把数据跑完，但是时间可能不是很快所以它叫数据的批处理 3、Yarn 资源调度器 4、ZooKeeper 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 5、Flume 数据采集工具 6、Hive 基于Hadoop的数据仓库工具 7、Hbase 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 8、Sqoop 数据传递工具，如将数据从关系型数据库导入Hive Scala 多范式编程语言、面向对象和函数式编程的特性 Spark 目前企业常用的批处理离线数据/实时计算引擎它是用来弥补基于MapReduce处理数据速度上的缺点，它很是流氓，直接将数据存在内存中 【注意】 MapReduce运行时也是需要将代码数据加载到内存中的，只不过Spark都是基于内存操作 Flink 目前最火的流式处理框架、既支持流处理、也支持批处理 Elasticsearch 大数据分布式弹性搜索引擎 大数据处理流程# 大数据处理模型#按照数据的三状态定义 水库里一平如镜的水—&gt;静止数据(data at rest) 水处理系统中上下翻滚的水—&gt;正在使用的数据(data in use) 汹涌而来的新水流—&gt;动态的水(data in motion) “快”说的是两层面 “动态数据” 来得快 “正在使用的数据” 处理得快 批处理 MapReduce 流处理 Spark Streaming 科普# 根据国际数据公司（IDC）的《数据宇宙》报告显示：2008年全球数量为0.5ZB，2010年为1.2ZB，人类正式进入ZB时代。更为惊人的是，2020年以前全球数据量仍将保持每年40%多的高速增长，大约每两年就翻一倍，这与IT界的摩尔定律极为相似，姑且称之为“大数据爆炸定律”。","link":"/post/2260.html"},{"title":"Java NIO","text":"Java基本套接字 基本操作 Java NIO基础 NIO与IO的区别 缓冲区 通道 对象的创建 数据的读写 是否支持工作在非阻塞状态 选择器 选择器的打开关闭 获取/设置标志位 SeverSocketChannel/SocketChannel配合工作的API 注销/获取选择器 select方法 获取相关建的方法 判断通道上等待操作的方法 与附件有关的另外两个方法 >[Java NIO实例](https://lyhcc.github.io/post/93c86522.html) ## Java基本套接字 ### 基本操作 - 连接远程机器。 - 发送数据。 - 接收数据。 - 关闭连接。 - 绑定端口。 - 监听入站数据。 - 在所绑定端口.上接受来自远程机器的连接。 其中，前四项用于客户端，后六项用于服务器，最后三项只有服务器才需要，即等待客户端的连接，这些操作通过ServerSocket类实现。 Java NIO基础$概述 NIO解决一客户一线程所带来的开销大的问题所谓一客户一线程是指，多个客户端访问同一个服务器进程时，服务器需要为每个请求创建一个服务线程 并且非堵塞是NIO实现的重要功能之一，为了实现非堵塞，NIO引入了 选择器 Selector 通道 Channel通道表示到实体(如硬件设备、文件、网络套接字或者可以执行一个或多个不同的的IO操作)的程序组件开放连接 通道可以注册一个选择器实例，通过该实例的select方法，用户可以询问“在一个或一组通道中，哪一个是当前需要的服务（即被读、写或被接受）“在一个准备好的通道执行相应的I/O操作，就不需要等待，也就不会堵塞了 NIO与IO的区别$ I0 NIO 面向流(Stream Oriented) 面向缓冲区(Buffer Oriented) 阻塞I0(Blocking I0) 非阻塞I0(Non Blocking I0) (无) 选择器(Selectors) 缓冲区$ NIO中一个主要的特性是java.nio.Buffer。缓冲区(Buffer)提供了一个比流抽象的、更高效和可预测的I/O。Buffer 代表了一个有限容量的容器一其本质是一个数组，通道Channel使用Buffer实例来传输数据Buffer包含4个索引 capacity: 缓冲区总容量，可通过Buffer.capacity获取,并且是不可修改 position: 缓冲区位置，即下一个要写入或读取的索引，获取/设置通过position()/position(int) limit: 缓冲区限制，即第一个不应该读取或写入的位置，获取/设置通过limit()/limit(int) mark: 缓冲区位置标记，通过mark()设置一个位置，reset()方法被调用后，position被置为mark 遵循如下规则 0 ≤ mark ≤ position ≤ limit ≤ capacity ByteBuffer的创建$1234567//直接创建缓冲区public static ByteBuffer allocate (int capacity) //在某个字节数组上创建缓冲区public static ByteBuffer wrap (byte[] array)//上面的调用下面的//上一个方法的返回 wrap(array, 0, array.length)public static ByteBuffer wrap (byte[] array, int offset, int length) 这种方式与流的区别：流是单向的，而这种方式看读可写 ByteBuffer的读写$ put()/get()方法：基于相对位置和绝对位置的读写基于相对位置就是基于目前缓冲区位置position的当前值，从“下一个”位置读取或存放数据，并为position增加适当的值。 绝对位置的put)/get()方法，必须提供写入/读出的位置, 该方式的读写操作，不改变position的值 1234567891011//相对位置public byte get ()public ByteBuffer get (byte[] dst)public ByteBuffer get (byte[] dst, int offset，int length)public ByteBuffer put (byte b)public final ByteBuffer put (byte[] src)public ByteBuffer put (byte[] src, int offset, int length)public ByteBuffer put (ByteBuffer src)//绝对位置public byte get (int index)public ByteBuffer put (int index, byte b) 需要注意的是 ，部分数据的get()/put()是不允许的。以写人为例，如果要写入的数据量超过当前缓冲区允许写入的数据量(可通过Buffer.remaining()方法获得该值)，则所有的数据都不会写入缓冲区，position的位置不变，put()方法抛出BufferOverflowException异常。 工具方法$ clear() 通过clear()方法，缓冲区的poistion被设置为0, limit 设置为capacity,这样，缓冲区准备好接收新数据。后续的put()/readO调用，将数据从第-一个元素开始填入缓冲区，最多直到填满该缓冲区，达到limit位置(等于capacity)。 flip()方法用于将缓冲区准备为数据传出状态，该方法将limit设置为position后，将position 设置为0。后续的get()/write()方法将从缓冲区的第一个元素开始传出数据，直到limit位置。通过fip0方法和get(/write()方法配合，可以将前面利用put()/read)方法放入缓冲区的所有数据读出(一直读到limit)。 rewind)方法将position设置为0，但不改变limit的值，如果需要多次读取缓冲区里的数据，可以在两次读取间使用rewind()方法。 compact()方法将position和limit间的数据复制到缓冲区的开始位置，为后续的put()/read()调用让出空间。调用结束后，poistion的值被设置为数据的长度，也就是原来的limit减去position的值，而limit则设置为capacity。和clear()、fip() 等方法不同，compact() 不但改变了position 和limit的位置，还改变了缓冲区中的数据。 compact()主要用于在缓冲区中还有未写出的数据时，为读入数据准备空间:即在write()方法调用后和添加新数据的read)方法前调用compact()方法，将未写出的“剩余”数据移动到缓冲区前面，为后面read()方法提供释放空间。在图中，假设write()方法调用后，缓冲区处于“某工作状态”，这时，position 到limit间的数据为未写出的“剩余”数据，而limit到capacity的空间则是read)方法可以使用的空间，通过compact()操作，position 到limit 间的数据被挪到缓冲区的前面，position 的位置也被设置为“剩余”数据长度。接下来，开发人员就可以直接调用read0/put0方法，从position位置开始放入数据。该数据和原来的“剩余”数据- -起，构成了连续的可用数据。 Buffer还支持一些其他功能，如直接缓冲区(directbuffer)、Java基本类型的put()/get()、缓冲区共享、复制、透视、字符编码转换等 通道$ 一个Channel的实例代表一个 和设备的 连接 对象的创建$ ServerSocketChannel/SocketChannel通过工厂方法创建 1234public static SocketChannel open() throws IOExceptionpublic static SocketChannel open(SocketAddress remote) throws IOExceptionpublic static ServerSocketChannel open() throws IOException SocketChannel创建后，可以通过connect()连接到远程机器，通过close()关闭连接，这些操作和Socket的没有什么差别。 数据的读写$123456public int read (ByteBuffer dst)public long read (ByteBuffer[] dsts, int offset, int length)public final long read (ByteBuffer[] dsts)public int write (ByteBuffer src)public long write (ByteBuffer[] srcs, int offset, int length)public final long write (ByteBuffer[] srcs) 和socket类似的操作，可以通过socket方法获取ServerSocket对象 12public SocketChannel accept ()public ServerSocket socket () 是否支持工作在非阻塞状态$ Buffer使用阻塞方法相对于基本套接字没有什么优点 12public SelectableChannel configureBlocking (boolean block) //设置堵塞public boolean isBlocking() //是否堵 非阻塞的SocketChannel 的connect()方法会立即返回，用户必须通过isConnected()判断连接是否已经建立，或者通过finishConnect()方法在非阻塞套接字上阻塞等待连接成功:非阻塞的read(),在Socket上没有数据的时候，立即返回(返回值为0)，不会等待;非阻塞的acceptO,如果没有等待的连接，将返回null 12public boolean isConnected ()public boolean finishConnect () 选择器$ 选择器(Selector) 的使用方法:通过静态的工厂方法创建Selector实例，通过Channel的注册方法，将Selector实例注册到想要监控的Channel实例上，最后调用选择器的select()方法。该方法会阻塞等待，直到有一个或多个通道准备好I/O操作或超时。select() 方法将返回可进行I/O操作的通道数量。现在，在一个单独的线程中，就可以检查多个通道是否可以进行I/O操作，不需要为每一一个通道都准备一个线程了。 选择器的打开关闭$123public static Selector open ()public boolean isOpen ()public abstract void close () 获取/设置标志位$选择器注册标记SelectionKey维护联的信息保存在java.nio.channels.SelectionKey实例中 OP_ READ (通道上有数据可读) OP_ WRITE (通道已经可写) OP_ CONNECT (通道连接已建立) OP_ ACCEPT (通道上有连接请求) 12public int interestOps ()public SelectionKey interestops (int ops) SeverSocketChannel/SocketChannel配合工作的API$1234public SelectionKey register (Selector sel, int ops) // 注册public SelectionKey register (Selector sel, int ops, object att)//带附件注册public SelectionKey keyFor (Selector sel) // 根据选择器，查找对应的selectionKeypublic boolean isRegistered() // 判断通道是否已经注册 选择器实例selector.上注册了- -个SocketChannel对象，支持读操作 1SelectionKey readKey = channel . register (selector, SelectionKey.OP_READ) 注销/获取选择器$123Selector selector () //获取选择器selectableChannel channel () //获取相应的channelvoid cancel () //注销选择器 select方法$1234/**如果发现select()的返回值大于0，表明有需要处理的I/O事件发生**/public int select()// 阻塞等待，直到一个注册通道有感兴趣的操作就绪public int select (1ong timeout)// 等待一段时间，或一个注册通道有感兴趣的操作就绪public int selectNow()// 非阻塞版本 获取相关建的方法$12public Set&lt;SelectionKey&gt; keys () //selector上已注册的所有键public Set &lt;SelectionKey&gt; selectedKeys()// 已选键集 Hadoop IPC上的使用 判断通道上等待操作的方法$12345public int readyOps ()public boolean isReadable ()public boolean iswritable ()public boolean isConnectable ()public boolean isAcceptable () 与附件有关的另外两个方法$12public Object attach (object ob) //添加附件public object attachment () //获取附件 以上内容来自《Hadoop技术内幕 深入解析HADOOP COMMON和HDFS架构设计与实现原理》","link":"/post/f2d80d11.html"},{"title":"Java动态代理","text":"概述 代理接口的创建 java.lang.eflct.Proxy比较重要的方法 调用转发 Java动态代理实例 ## 概述 >Hadoop远程过程调用实现使用Java动态代理和新输入1输出系统(NewInput/Output,NIO) Java动态代理类位于java.lang.reflect包下，主要包括java.lang rlfct.Proxy和java.lang.reflect.InvocationHandler 代理对象两大任务 创建代理接口 实现由java.lang,reflect.Proxy完成 调用转发通过java.lang.reflect.InvocationHandler的实例完成 代理接口的创建$ 在Java中，代理对象往往实现和目标对象-致的接口，并作为目标对象的代替，接收对象用户(Client) 的调用，并将全部或部分调用转发给目标对象 代理也是拥有和目标对象一样的权利的‘人’， 简单的说， 代理可以越俎代庖。实际上他是调用转发，将这个任务交给有权利实施的人 代理时序图 java.lang.eflct.Proxy提供了用于创建动态代理类和对象的静态方法。也就是说，通过java.lang.reflect.Proxy可以动态地创建某个接口实现 java.lang.eflct.Proxy比较重要的方法$ public static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces) 获得代理类的java.lang.Class对象。该代理类将定义在指定的类加载器（参数loader）中，并将实现参数interfaces指定所有接口 注意： 这个类只创建一次，如果再次传入相同的loader和interfaces给newProxyInstance()方法，获得的也只是第一次调用创建的那个java.lang.Class对象 通过Proxy.getProxyClass()获得的代理类都包含一个构造函数,该构造函数需要一个java.lang.reflect.InvocationHandler 的实例 如何获取Proxy对象？，请看以下代码 123456Class clss = Proxy.getProxyClass(loader, interfaces);//获得构造函数Constructor constructor = clss.getConstructor(new Class[]{InvocationHandler.class});//创建代理对象Object proxy = constructor.newInstance(new Object[]{invocationHandler}); public static boolean isProxyClass(Class&lt;?&gt; cl) 判断java.lang.Class对象是否是代理类 public static InvocationHandler getInvocationHandler(Object proxy) throws IllegalArgumentException 获取代理实例对应的调用处理程序（即构建代理传入的InvocationHandler实例） 调用转发$ InvocationHandler调用实例也叫调用句柄实例 12345678910111213141516/** * * @param proxy 代理对象本身 * @param method 用户调用的代理对象上的方法 * @param args 传递给该方法的参数 * @return 代理对象方法调用结果 * @throws Throwable * * java.lang.reflect.Method * 它提供了关于类或接口上某个方法以及如何访问该方法的信息 * 其中的invoke方法可以在指定对象上调用对象的方法 */@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return null;} Method类中的invoke方法声明如下 123public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException 假设目标对象为target，实现转发代码如下 123public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return method.invoke(target, args);s} Java动态代理实例$实例类图 代码 12345678910111213/**DPStatus.java**/public class DPStatus { String name; public DPStatus(String name) { this.name = name; } @Override public String toString() { return \"Hello, \" + name + \"!\"; }} 12345678/** * PDQueryStatus.java * * 动态代理机制与java远程调用不同，不需要继承什么接口 */public interface PDQueryStatus { public DPStatus getStatus();} 1234567891011/** * DPQueryStatusImpl.java * * PDQueryStatus的简单实现 */public class DPQueryStatusImpl implements PDQueryStatus{ @Override public DPStatus getStatus() { return new DPStatus(\"Bitty\"); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.text.MessageFormat;import java.util.Arrays;/** * DPInvocationHandler.java * * 转发由该类实现 * 该类中最重要的一部分是invoke方法， * * 如果代理对象调用某个方法时，DPInvocationHandler.invoke将会被调用， * 传入invoke的method中，也就是说，PDQueryStatus.getStatus()就是是method对象， * 而getStatus()没有参数 * * 在这里完成代理转发的是 * Object res = method.invoke(dpqs, args); * 当method是PDQueryStatus.getStatus()时，其效果就相当于 * res = pdqs.getStatus() * * * */public class DPInvocationHandler implements InvocationHandler { //目标对象 private DPQueryStatusImpl dpqs; public DPInvocationHandler(DPQueryStatusImpl dpqs) { this.dpqs = dpqs; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //实现附加功能，在控制台输出调用参数的String表示 String msg = MessageFormat.format(\"Calling method\", method.getName(), Arrays.toString(args)); System.out.println(msg); //调用转发 Object res = method.invoke(dpqs, args); //其他附加功能 return res; }} 1234567891011121314151617/** * DPMain.java * * 在这个类中使用create方法创建代理 */public class DPMain { public static PDQueryStatus create(DPQueryStatusImpl dpqs){ //newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) //参数准备 Class&lt;?&gt;[] interfaces = new Class[]{PDQueryStatus.class}; DPInvocationHandler handler = new DPInvocationHandler(dpqs); return (PDQueryStatus) Proxy.newProxyInstance(dpqs.getClass().getClassLoader(), interfaces, handler); }} 123456789public class Demo { public static void main(String[] args) throws Exception { PDQueryStatus pdqs = DPMain.create(new DPQueryStatusImpl()); System.out.println(pdqs.getStatus()); }}","link":"/post/e63a7813.html"},{"title":"Java远程调用","text":"概述 远程方法调用实例 Java远程调用实例类图 ## 概述 >Java远程方法调用(Remote Method Invocation, RMI)是Java的一个核心API和类库, 允许一个Java虚拟机上运行的Java程序调用不同虚拟机上运行的对象中的方法，即使这两个 虚拟机运行于物理隔离的不同主机上。在某种程度上，RMI可以看成RPC的Java升级版。 和RPC一样,存在服务端和客户端典型服务器端应用程序 创建多个远程对象（Remote Object）,使这些对象能被客户端引用，并等待客户端调用远程对象的方法典型的客户端程序 从服务器获得一个或多个远程对象的引用，然后调用远程对象的方法 Java远程方法调用依赖于Java序列化，调用远程方法传的参数、返回值都是序列化对象 远程方法调用实例$123456789/**RMIQueryStatus.java**/import java.rmi.Remote;import java.rmi.RemoteException;public interface RMIQueryStatus extends Remote { String getStatus(String name) throws RemoteException;} RMIQueryStatus的定义要求 远程接口必须声明为public，否则客户端试着装载“实现远程接口”的远端对象时，会收到错误的消息。 远程接口必须继承自java.rmi.Remote。 远程接口中的每-一个方法，除了自定义的异常之外，必须将java.rmi.RemoteException声明于其throws子句中。 在远程方法声明中，作为参数或者返回值的远程对象，或者包含在其他非远程对象中的远程对象，必须声明为其对应的远程接口，而不是实际的实现类。(这点在String类中并没有体现) RMIQueryStatus的实现类 123456789101112/**RMIQueryStatusImp.java**/import java.rmi.RemoteException;import java.rmi.server.UnicastRemoteObject;public class RMIQueryStatusImp extends UnicastRemoteObject implements RMIQueryStatus { protected RMIQueryStatusImp() throws RemoteException { } public String getStatus(String name) throws RemoteException { return \"I'm \" + name + \".\"; }} 客户端和服务端的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/**RMIDemoServer**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.RemoteException;import java.rmi.registry.LocateRegistry;/** * @author lyhcc */public class RMIDemoServer { public static void main(String[] args) throws RemoteException, MalformedURLException { //1. 创建RMIQueryStatus对象 RMIQueryStatusImp queryService = new RMIQueryStatusImp(); //2. 设置服务端口 LocateRegistry.createRegistry(12090); //3. 绑定远端对象名 Naming.rebind(\"rmi://localhost:12090/queryTest\", queryService); System.out.println(\"Server is running!\"); }}/**客户端的代码**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.NotBoundException;import java.rmi.RemoteException;public class RMIDemoClient { public static void main(String[] args) throws RemoteException, NotBoundException, MalformedURLException { //1. 创建RMIQueryStatusImp对象 RMIQueryStatus queryStatus = (RMIQueryStatus) Naming.lookup(\"rmi://localhost:12090/queryTest\"); //2. 调用远程方法 String status = queryStatus.getStatus(\"KiKi\"); System.out.println(status); }} 先运行服务端然后运行客户端查看结果 Java远程调用实例类图$ 客户端RMIQueryStatusClient的工作依赖于RMI存根(Stub)，这个存根是通过Java的代理机制 java.lang.reflect.Proxy","link":"/post/55cde24e.html"},{"title":"Hadoop介绍","text":"起源 Google 在大数据方面的三大论文 （谷歌三宝） Hadoop 三大发行版本 硬件要求 ## 起源 Google 在大数据方面的三大论文 （谷歌三宝） [在github大的当前目录下](https://github.com/lyhcc/NoteBook/tree/master/bigdata) [三宝的介绍](https://www.cnblogs.com/javhu/archive/2013/03/25/cyue_hadoop_google.html) Hadoop 三大发行版本$ Apache、Cloudera、Hortonworks Apache版本最原始、最基础：适合零基础 大公司在用 Cloudera Cloudera’s DistributionIncluding Apache Hadoop 简称CDH中小型公司用、简单方便、自带可视化 Hortonworks 文档较好 注：Cloudera 和Hortonworks 在2018年10月，国庆期间宣布合并硬件要求$内存$ 最大支持内存查询：win + R输入 wmic memphysical get maxcapacity计算 MaxCapacity/1024/1024GB 硬盘:500G+$","link":"/post/8a9ad9ba.html"},{"title":"Java NIO实例","text":"概述 回显服务器是指接收到客户端的数据，原封不动的返回给客户端 小提示 &nbsp;在阅读这里之前，希望先看一个视频和相关API的介绍，API的介绍不是很详细，当然也可以遇到不知道的API在百度/Google搜索视频：https://www.bilibili.com/video/av57390893?t=5655相关API: https://lyhcc.github.io/post/f2d80d11.html#more 源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/**NIOServer.java**/import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;public class NIOServer { public static void main(String[] args) throws IOException { //1. 打开一个选择器 Selector selector = Selector.open(); //2. 打开一个ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //3. 配置异步模式 serverSocketChannel.configureBlocking(false); //4. 绑定到TCP端口上，注意ServerSocketChannel不提供bind方法 //需要使用ServerSocketChannel 内部的socket对象对应的bind方法 serverSocketChannel.socket().bind(new InetSocketAddress(\"127.0.0.1\",12122)); //5. 注册 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //6. 服务器循环，调用Selector.select()方法等待IO事件，如果返回值为0，表明没有事件发生 while (true) { //如果select()带参数，它将不会堵塞到等有感兴趣的数据过来 if (selector.select() == 0) { System.out.println(\"Nothing to do!\"); continue; } //获得连接已选键 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); //如果是事件是”通道上有请求“ if (key.isAcceptable()) { //相应的处理是通过accept()操作获得SocketChannel对象，并配置对象的异步工作方式 SocketChannel channel = serverSocketChannel.accept(); //设置异步工作模式、注册到选择器中，注册事件为通道可读 SelectionKeys.OP_READ channel.configureBlocking(false); SelectionKey connkey = channel.register(selector, SelectionKey.OP_READ); //根据注册分到的SelectionKey 对象构造连接对象，并将对象作为SelectionKey对象附件 NIOConnection conn = new NIOConnection(connkey); connkey.attach(conn); } //key有效，即通道未关闭，并且为可读的OP_READ if (key.isValid() &amp;&amp; key.isReadable()) { NIOConnection conn = (NIOConnection) key.attachment(); conn.handleRead(); } //key有效，即通道未关闭，并且为可写的 OP_WRITE if (key.isValid() &amp;&amp; key.isWritable()) { NIOConnection conn = (NIOConnection) key.attachment(); conn.handleWrite(); } } } }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/**NIOConnection.java**/import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;public class NIOConnection { private SelectionKey key; private SocketChannel channel; private ByteBuffer buffer; public NIOConnection(SelectionKey key) { this.buffer = ByteBuffer.allocate(1024); this.key = key; this.channel = (SocketChannel) key.channel(); } /** * 读操作 * @throws IOException */ public void handleRead() throws IOException { int byteRead = channel.read(buffer); if (byteRead == -1) { //对方已关闭socket，服务器就将通道关闭 channel.close(); }else { //有数据可读，此时设置感兴趣的I/O事件为读或写， //读出了一部分数据，就说嘛有空间可以写了，当然还有可能有其他数据可读 key.interestOps(SelectionKey.OP_READ | SelectionKey.OP_WRITE); } } public void handleWrite() throws IOException { //要开始读数据了，就得先把数据的开始索引，也就是当前索引position，改为数据的开始位置， //在此之前，得先把limit的改为position,position所在位置是数据的下一个写入位置， // 把限制limit设置为position // 结合上面position ~ limit就是当前的全部数据的位置 buffer.flip(); //开始写出 System.out.println(\"收到的数据：\" + Charset.forName(\"UTF-8\").decode(buffer).toString()); channel.write(buffer); //之后判断是否还有数据存在，如果没数据了，就可以将其设置为只可以读了 if (!buffer.hasRemaining()) { key.interestOps(SelectionKey.OP_READ); } //写完后，有可能还有数据剩余，就将数据移到buffer的最前面 buffer.compact(); } 注意：NIO是没有专门的客户端的，你可以使用Socket进行连接，也可以使用telnet进行连接 Selector的使用步骤 创建一个Selector实例: 将该实例注册到各种通道，指定每个通道上感兴趣的I/O操作; 重复执行(选择器循环): 调用一种select()方法; 获取已选键集; 对于已选键集中的每-一个键: 将已选键从键集中移除; 获取信道，并从键中获取附件(如果需要); 确定准备就绪的操作并执行;对于accept操作获得的SocketChannel对象，需将信道设置为非阻塞模式，并将其注册到选择器中; 根据需要，修改键的兴趣操作集。 错误总结 这里的connkey用错会报错Exception in thread &quot;main&quot; java.lang.ClassCastException: sun.nio.ch.ServerSocketChannelImpl cannot be cast to java.nio.channels.SocketChannel","link":"/post/93c86522.html"},{"title":"Hadoop通信机制和内部协议之RPC","text":"Hadoop RPC RPC简介 RPC模型 RPC特性 RPC例子 其他开源RPC架构 ## Hadoop RPC ### RPC简介 >简要地说，RPC就是允许程序调用位于其他机器上的过程(也可以是同一台机器的不同进程)。 >RPC调用过程是透明的 传统过程调用：传统的过程调用中，主程序将参数压人栈内并调用过程，这时候主程序停止执行并开始执行相应的过程。被调用的过程从栈中获取参数，然后执行过程函数;执行完毕后，将返回参数入栈(或者保存在寄存器里)，并将控制权交还给调用方。调用方获取返回参数，并继续执行。 而RPC调用是进程间的过程调用 RPC模型$ 通行模块： 请求-响应 Stub程序： 用于保证RPC的透明性。在客户端，不在本地调用，而是将请求信息通过网络模块发送给法服务器端，服务器接收后进行解码。服务器中，Stub程序依次进行 解码（请求的参数）、调用相应的服务过程、编码返回结果等处理 调度程序： 调度来自通行模块的请求信息，根据其中标识选一个Stub程序运行 客户程序： 请求发出者 服务过程： 请求接收者 一个RPC的旅游： 客户端以本地调用方式产生本地Stub程序 该Stub程序将函数调用信息按照网络通信模块的要求封装成消息包，并交给通信模块发送到远程服务器端。 远程服务器端接收此消息后，将此消息发送给相应的Stub程序 Stub程序拆封消息，形成被调过程要求的形式，并调用对应函数 服务端执行被调用函数，并将结果返回给Stub程序 Stub程序将此结果封装成消息，通过网络通信模块逐级地传送给客户程序。 RPC特性$ 透明性 调用过程就像本地调用，察觉不到它的经历 高性能 ：Hadoop各个系统（如HDFS、MapReduce、YARN等）均采用了Master/Slave结构，其中，Master实际上是一个RPC server，它负责响应集群中所有Slave发送的服务请求。RPC Server性能要求高，为的是能够让多个客户端并发方位 易用性/可控性 Hadoop系统不采用Java内嵌的RPC（RMI,Remote Method Invocation）框架的主要原因是RPC是Hadoop底层核心模块之一，需要满足易用性、高性能、轻量级等特性 RPC例子$执行过程： CalculateClient对象本地调用产生Stub程序 经通信模块上传至服务器CalculateServer对象，在创建Server时设置了协议和业务逻辑（服务过程），处理过后根据上述RPC过程返回 客户端接收后打印到日志中 先定义一些常量$ 这里不需要太多的在意，直接使用在代码里面也行，在大的项目中为了使程序易于修改而这样设置 1234567891011/** * 静态变量声明类 */public interface Constants { public interface VersionID { public static final long RPC_VERSION = 7788L; } public static final String RPC_HOST = &quot;127.0.0.1&quot;; public static final int RPC_PORT = 8888;} 定义一个Service接口，协议类$12345678910111213import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.ipc.ProtocolInfo;@ProtocolInfo(protocolName = &quot;&quot;, protocolVersion = Constants.VersionID.RPC_VERSION)public interface CalculateService { //真实业务逻辑，加减法， public IntWritable add(IntWritable a, IntWritable b); public IntWritable sub(IntWritable a, IntWritable b); public Text echo(Text mt);} @ProtocolInfo(protocolName = “”, protocolVersion = Constants.VersionID.RPC_VERSION) 没有这句就不能将该类设置为协议，不过也可以通过继承VersionProtocol接口 Service接口的实现类$1234567891011121314151617181920212223242526272829303132333435import java.io.IOException;public class CalculateServiceImpl implements CalculateService { /** * 该方法没有也行 * */ public ProtocolSignature getProtocolSignature(String arg0, long arg1, int arg2) throws IOException{ return this.getProtocolSignature(arg0, arg1, arg2); } /** * 校验hadoop RFC版本号 * @param arg0 * @param arg1 * @return */ public long getProtocolVersion(String arg0, long arg1) throws IOException { return Constants.VersionID.RPC_VERSION; } @Override public IntWritable add(IntWritable a, IntWritable b) { return new IntWritable(a.get() + b.get()); } @Override public IntWritable sub(IntWritable a, IntWritable b) { return new IntWritable(a.get() - b.get()); } @Override public Text echo(Text mt) { return mt; }} Server和Client类$123456789101112131415161718192021222324252627282930import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;public class CalculateServer { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { try { //构造Server,并设置协议接口，主机、端口，真实业务逻辑 RPC.Server server = new RPC.Builder(new Configuration()) .setProtocol(CalculateService.class) .setBindAddress(Constants.RPC_HOST) .setPort(Constants.RPC_PORT) .setInstance(new CalculateServiceImpl()) .build(); //启动Server server.start(); LOG.info(&quot;Server has Started!&quot;); } catch (IOException e) { LOG.error(&quot;Server has Error&quot;); } }} 123456789101112131415161718192021222324252627282930313233343536import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.net.InetSocketAddress;public class CalculateClient { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { //格式化IP和端口 InetSocketAddress addr = new InetSocketAddress(Constants.RPC_HOST, Constants.RPC_PORT); //校验Hadoop RPC版本号 long protocolVersion = RPC.getProtocolVersion(CalculateService.class); try { //获取Server连接 CalculateService proxy = RPC.getProxy(CalculateService.class, protocolVersion, addr, new Configuration()); IntWritable add = proxy.add(new IntWritable(1), new IntWritable(2)); IntWritable sub = proxy.add(new IntWritable(3), new IntWritable(2)); LOG.info(&quot;1+2 = &quot; + add); LOG.info(&quot;3-2 = &quot; + sub); } catch (IOException e) { LOG.error(&quot;Client has error!&quot;); } }} 注意： 查看本程序运行结果需要一个日志文件，如果不想加，把LOG的相关语句换为打印输出就行在resource文件夹下创建 log4j.properties 12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 客户端运行结果$12342019-10-31 18:59:28,499 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-10-31 18:59:28,619 WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 1+2 = 3 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 3-2 = 5 其他开源RPC架构$ Java RMI Apache Thrift Google Protocol Buffer","link":"/post/56192.html"},{"title":"hadoop序列化","text":"序列化介绍 Java序列化 Hadoop 不使用Java序列化原因 Hadoop 序列化 Hadoop序列化机制的特征 Hadoop Writable机制 Hadoop序列化的其它几个接口 Hadoop 序列化的类 序列化介绍$ 序列化是一种将对象的状态信息转化成可以存储或者传输的过程，与之相反的为反序列化不是某一种编程语言所独有的特性序列化的用途 作为一种持久化格式。对象序列化后存盘 作为一种通信的数据格式。如虚拟机之间通信 作为一种拷贝、克隆机制。放缓存 Java序列化$ Java通过实现Serializable接口Java序列化后放入对象，通过对象流进行IO操作，ObjectInputStream/ObjectOutputStream 1234567import java. io.Serializable ;／＊＊定义一个可以序列化的 App 信息类. */public class Appinfo implements Serializable{ ／／序列化标识 private static final long serialVersionUID = 11 ;} Hadoop 不使用Java序列化原因$ Java 自带的序列化机制占用内存空间大，额外的开销会导致速度降低，Hadoop对序列化的要求较高，需要保证序列化速度快、体积小、占用带宽低等特性 Hadoop 序列化机制是将对象序列化到流中，而 Java 序列化机制是不断创建新对象，对于MapReduce应用来说，不能重用对象 Java序列化在反序列化时，有可能需要访问前一个数据，这将导致数据无法分割来通过MapReduce来处理 Hadoop 序列化$ 在 Hadoop 序列化机制中，org.apache.hadoop.io包中定义了大量的可序列化对象 Hadoop 序列化机制通过调用write方法（它带有一个类型为DataOutput的参数），将对象序列化到流中 Hadoop 反序列化通过对象的readFields从流中读取数据 Hadoop序列化机制的特征$ 对于处理大数据的Hadoop平台，其序列化需要具备以下特征 紧凑。这样可以充分利用Hadoop集群的资源，hadoop集群中最稀缺的是资源 快速。进程通信时会大量使用序列化机制，因此需要减少序列化开销 可扩展性。为适应发展，序列化机制也需要支持这些升级和变化 互操作。支持不同语言开发 Hadoop Writable机制$ Hadoop序列化都必须实现该接口 均实现Wriable接口的两个函数， 12(1) write：将对象写入字节流：(2) readFields：从字节流中解析出对象。例子 123456789101112131415161718192021222324252627282930313233343536373839/** * BlockWritable有三个对象， * write方法将三个对象写到流中 * readFields从流中读出三个对象 */public class BlockWritable implements Writable { private long blockId; private long numBytes; private long generationStamp; /** * 输出序列化对象到流中 * @param out * @throws IOException */ @Override public void write(DataOutput out) throws IOException { out.writeLong(this.blockId); out.writeLong(this.numBytes); out.writeLong(this.generationStamp); } /** * 从流中读取序列化对象 * 为了效率，尽可能复用现有对象 * @param in 从该流中读取数据 * @throws IOException */ @Override public void readFields(DataInput in) throws IOException { this.blockId = in.readLong(); this.numBytes = in.readLong(); this.generationStamp = in.readLong(); if (this.numBytes &lt; 0L) { throw new IOException(&quot;Unexpected block size: &quot; + this.numBytes); } }} Hadoop序列化的其它几个接口$ WritableComparable RawComparator RawComparator允许执行者 比较 流中读取的未被反序列化为对象的 记录，从而省去创建对象所带来的开销 1234567891011/** * * @param var1 字节数组1 * @param var2 字节数组1的开始位置 * @param var3 字节数组1的记录长度 * @param var4 字节数组2 * @param var5 字节数组2的开始位置 * @param var6 字节数组2的记录长度 * @return */int compare(byte[] var1, int var2, int var3, byte[] var4, int var5, int var6); WritableComparator 在RawComparator中WritableComparator是个辅助类，实现了RawComparator接口 以DoubleWritable为例 1234567891011public static class Comparator extends WritableComparator { public Comparator() { super(DoubleWritable.class); } public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) { double thisValue = readDouble(b1, s1); double thatValue = readDouble(b2, s2); return thisValue &lt; thatValue ? -1 : (thisValue == thatValue ? 0 : 1); } } WritableComparator是RawComparator对WritableComparable类的一一个通用实现。提供两个主要功能。首先，提供了一个RawComparator的compare()默认实现，该实现从数据流中反序列化要进行比较的对象，然后调用对象的compare()方法进行比较(这些对象都是Comparable的)。其次，它充当了RawComparator实例的一个工厂方法,通过DoubleWritable获得RawComparator的代码如下 1RawComparator&lt;DoubleWritable&gt; comparator = WritableComparator.get(DoubleWritable.class); RawComparator和WritableComparable的类图 Hadoop 序列化的类$java基本类型的封装$ 说明： 这些类实现了WritableComparable接口 VIntWritable和VLongWritable是只可变长 可变长的格式更空间 VIntWritable可用VLongWritable读入 变长整型分析 writeVLong ()方法实现了对整型数值的变长编码，它的编码规则如下:&emsp;&emsp;如果输入的整数大于或等于-112同时小于或等于127，那么编码需要1字节:否则，序列化结果的第一个字节，保存了输入整数的符号和后续编码的字节数。符号和后续字节数依据下面的编码规则(又一个规则): 如果是正数，则编码值范围落在-113和-120间(闭区间)，后续字节数可以通过-(v+112)计算。 如果是负数，则编码值范围落在-121和-128间(闭区间)，后续字节数可以通过-(v+120)计算。 后续编码将高位在前，写入输入的整数(除去前面全0字节)。代码如下: 1234567891011121314151617181920212223242526272829public static void writeVInt(DataOutput stream, int i) throws IOException { writeVLong(stream, (long)i); } public static void writeVLong(DataOutput stream, long i) throws IOException { if (i &gt;= -112L &amp;&amp; i &lt;= 127L) { stream.writeByte((byte)((int)i)); } else { int len = -112; if (i &lt; 0L) { i = ~i; len = -120; } for(long tmp = i; tmp != 0L; --len) { tmp &gt;&gt;= 8; } stream.writeByte((byte)len); len = len &lt; -120 ? -(len + 120) : -(len + 112); //后续编码 for(int idx = len; idx != 0; --idx) { int shiftbits = (idx - 1) * 8; long mask = 255L &lt;&lt; shiftbits; stream.writeByte((byte)((int)((i &amp; mask) &gt;&gt; shiftbits))); } } } ObjectWritable$ 针对Java基本类型、字符串、枚举、Writable、空值、Writable的其 他子类,ObjectWritable提供了一个封装，适用于字段需要使用多种类型。ObjectWritable 可应用于Hadoop远程过程调用中参数的序列化和反序列化; ObjectWritable 的另一个典型应用是在需要序列化不同类型的对象到某-个字段，如在一个SequenceFile 的值中保存不同类型的对象( 如LongWritable值或Text值)时，可以将该值声明为ObjectWritable。 ObjectWritable的实现比较冗长，需要根据可能被封装在ObjectWritable中的各种对象进行不同的处理。ObjectWritable 有三个成员变量，包括被封装的对象实例instance、该对象运行时类的Class对象和Configuration对象。 123private Class declaredClass;private Object instance;private Configuration conf; ObjectWritable的write 方法调用的是静态方法ObjectWritable.writeObject()，该方法可以往DataOutput接口中写入各种Java对象。 writeObject()方法先输出对象的类名(通过对象对应的Class对象的getName()方法获得)， 1UTF8.writeString(out, declaredClass.getName()); 然后根据传入对象的类型，分情况系列化对象到输出流中，也就是说，对象通过该方法输出对象的类名，对象序列化结果对到输出流中。在ObjectWritable. writeObject(的逻辑中，需要分别处理null Java 数组、字符串String、Java 基本类型、枚举和Writable的子类6种情况，由于类的继承，处理Writable时，序列化的结果包含对象类名，对象实际类名和对象序列化结果三部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public void write(DataOutput out) throws IOException { writeObject(out, this.instance, this.declaredClass, this.conf); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf) throws IOException { writeObject(out, instance, declaredClass, conf, false); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf, boolean allowCompactArrays) throws IOException { if (instance == null) { instance = new ObjectWritable.NullInstance(declaredClass, conf); declaredClass = Writable.class; } if (allowCompactArrays &amp;&amp; declaredClass.isArray() &amp;&amp; instance.getClass().getName().equals(declaredClass.getName()) &amp;&amp; instance.getClass().getComponentType().isPrimitive()) { instance = new Internal(instance); declaredClass = Internal.class; } UTF8.writeString(out, declaredClass.getName()); /****************此处****************/ if (declaredClass.isArray()) { int length = Array.getLength(instance); out.writeInt(length); for(int i = 0; i &lt; length; ++i) { writeObject(out, Array.get(instance, i), declaredClass.getComponentType(), conf, allowCompactArrays); } } else if (declaredClass == Internal.class) { ((Internal)instance).write(out); } else if (declaredClass == String.class) { UTF8.writeString(out, (String)instance); } else if (declaredClass.isPrimitive()) { if (declaredClass == Boolean.TYPE) { out.writeBoolean((Boolean)instance); } else if (declaredClass == Character.TYPE) { out.writeChar((Character)instance); } else if (declaredClass == Byte.TYPE) { out.writeByte((Byte)instance); } else if (declaredClass == Short.TYPE) { out.writeShort((Short)instance); } else if (declaredClass == Integer.TYPE) { out.writeInt((Integer)instance); } else if (declaredClass == Long.TYPE) { out.writeLong((Long)instance); } else if (declaredClass == Float.TYPE) { out.writeFloat((Float)instance); } else if (declaredClass == Double.TYPE) { out.writeDouble((Double)instance); } else if (declaredClass != Void.TYPE) { throw new IllegalArgumentException(\"Not a primitive: \" + declaredClass); } } else if (declaredClass.isEnum()) { UTF8.writeString(out, ((Enum)instance).name()); } else if (Writable.class.isAssignableFrom(declaredClass)) { UTF8.writeString(out, instance.getClass().getName()); ((Writable)instance).write(out); } else { if (!Message.class.isAssignableFrom(declaredClass)) { throw new IOException(\"Can't write: \" + instance + \" as \" + declaredClass); } ((Message)instance).writeDelimitedTo(DataOutputOutputStream.constructOutputStream(out)); } } 和输出对应，ObjectWritable 的readFields()方法调用的是静态方法ObjectWritable.readObject()，该方法的实现和writeObject()类似，唯一值得研究的是Writable对象处理部分，readObject ()方法依赖于WritableFactories类。WritableFactories 类允许非公有的Writable子类定义一一个对象工厂，由该工厂创建Writable对象，如在上面的readObject()代码中，通过WritableFactories的静态方法newInstance()，可以创建类型为instanceClass的Writable子对象。具体查看org.apache.hadoop.io.WritableFactories类 注：ObjectWritable它比较浪费资源，可以使用静态数组来记录数据类型以提高效率 Hadoop序列化优势 1231. 减少垃圾回收：从流中反序列化数据到当前对象，重复使用当前对象，减少了垃圾回收GC ;2. 减少网络流量 ： 序列化和反序列化对象类型不变 ，因此可以只保存必要的数据来减少网络流量；3. 提升 I/O 效率 ： 由于序列化和反序列化的数据量减少了，配合Hadoop压缩机制，可以提升I/O效率。","link":"/post/20979.html"},{"title":"Hadoop通信机制和内部协议之协议","text":"概述 ClientProtocol通信协议 RefreshUserMappingProtocol RefreshAuthorizationPolicyProtocol ResourceManagerAdministrationProtocol ## 概述 MapReduce核心协议 名称 描述 ClientProtocol 继承于Version基类，查看作业情况监控当前集群等 RefreshUserMappingProtocol 刷新用户到用户组映射关系到超级用户代理组列表 RefreshAuthorizationPolicyProtocol 刷新HDFS和MapReduce服务几倍访问控制列表 ResourceManagerAdministrationProtocol 继承于GetUserMappingProtocol基类，刷新队列列表，节点列表 ## ClientProtocol通信协议 ClientProtocol协议是JobClient和JobTracker之间进行交流的枢纽。JobClient 可以使用该协议中的函数来提交-一个作业(Job) 并执行，以了解当前系统的状态 提交作业协议中JobClient通过Hadoop RPC的submitjob()函数提交作业(Job)，函数所包含的参数有作业ID (JobID)，然后JobClient通过getNewJoblD0函数为作业(Job) 获得一个唯一的ID。 操作作业当用户提交作业(Job) 后，可以通过调用函数来控制该作业的执行流程，如设置提交作业的优先级(setlobPriority()函数) 、停止一个作业(killJob()函数) 、停止一个任务(illTask()函数)。 查看状态从实现源代码来看，该通信协议还提供了一系列函数来 查看状态，如查看集群当前状态(getClusterMetrics()函数)、查看当前任务状态(getJobTrackerStatus()函数) 、获取所有任务(getllobs()函数)等。 RefreshUserMappingProtocol$ RefreshU serMappingsProtocol 协议用于更新 HDFS 和 MapReduce 级别的用户到用户组映射关系及超级用户代理组列表 refreshUserToGroupsMappings() 函数和refreshSuperUserGroupsConfiguration()函数来实现，这两个函数均是通过调用Hadoop RPC来完成具体的逻辑。 RefreshAuthorizationPolicyProtocol$ RefreshAuthorizationPol icyProtocol 协议用于刷新当前使用的授权策略 通过调用 Hadoop RPC 远程调用 refreshServiceAcl（）函数，实现基于 HDFS 和MapReduce 级别的授权策略 ResourceManagerAdministrationProtocol$ ResourceManagerAdministrationProtocol 协议用于更新队列列表、节点 列表 、节点资源等 该协议继承于 GetUserMappingsProtocol 基类 ，通过 Hadoop RPC 远程调用来实现节点更新、资源更新 、添加标签等操作 说明：在IDE中导入hadoop源码加载进去后，按Ctrl+鼠标左键进入即可查看源码","link":"/post/13550.html"},{"title":"Hadoop 压缩","text":"压缩 Hadoop文件切片 Hadoop压缩 压缩格式选择 Hadoop 压缩案例 ## 压缩 >压缩是指将数据转换为比原来的格式占用空间更小的格式来存储，以达到减小存储空间 >解压是压缩的反过程 Hadoop文件切片$ Hadoop MapReduce是通过划分切片来处理得，这样就使得支持分割的压缩格式更适合Hadoop 针对ss.txt文件大小为300M 计算公式computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M（Hadoop 1.x中块大小为64M） 默认情况下，切片大小=blocksize 开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） 注意：切片主要由这几个值来运算决定 12mapreduce.input.fileinputformat.split.minsize=1 默认值为1mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认Long.MAXValue 因此，默认情况下，切片大小=blocksize。 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 Hadoop压缩$ Hadoop作为一个叫通用的海量数据处理平台，在压缩方面主要考虑压缩速度和压缩的可分割性 小提示： 使用gzip压缩文件时 -9表示空间优先，也就是先考虑压缩空间的减小 -1表示时间优先，也就是压缩速度要快 Hadoop支持的压缩格式 压缩格式 工具 算法 扩展名 多文件 可分割性 换成压缩格式后，原来的程序是否需要修改 DEFLATE 无 DEFLATE .deflate 不 不 和文本处理一样，不需要修改 GZIP gzip DEFLATE .gzp 不 不 和文本处理一样，不需要修改 ZIP zip DEFLATE .zip 是 是，在文件范围内 BZIP2 bzip2 BZIP2 .bz2 不 是 和文本处理一样，不需要修改 LZO lzop LZO .lzo 不 是 需要建索引，还需要指定输入格式 压缩算法及其编码/解码器 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 性能压缩比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 压缩格式选择$Gzip压缩$ 优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 Bzip2压缩$ 优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native(java和c互操作的API接口)；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 Lzo压缩$ 优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。 Snappy压缩$ 优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。 Hadoop 压缩案例$12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Demo { public static void main(String[] args) throws IOException, ClassNotFoundException { //压缩为bz2格式 compress(\"org.apache.hadoop.io.compress.BZip2Codec\"); } private static void compress(String method) throws ClassNotFoundException, IOException { //1. 获取resources下的资源文件流 InputStream in = Demo.class.getClassLoader().getResourceAsStream(\"properties.xml\"); //2. 通过Java反射机制创建对应得编码名称 Class&lt;?&gt; codeClass = Class.forName(method); //3. 通过编码名称找对应得编码/解码器 Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codeClass, conf); //4. 指定压缩后的文件,codec.getDefaultExtension()获得相应的扩展名 File fileOut = new File(System.currentTimeMillis() + codec.getDefaultExtension()); //如果文件存在，删除，否则什么也不做 fileOut.delete(); //5. 创建输出流 FileOutputStream out = new FileOutputStream(fileOut); //6. 通过编码/解码器创建对应得输出流 CompressionOutputStream cout = codec.createOutputStream(out); //7. 压缩输出 /** * in 输入流 * cout 压缩输出流 * 1024 缓冲大小 * false 不关闭相应流，true则关闭 */ IOUtils.copyBytes(in,cout,4096,false); in.close(); cout.close(); } /** * 解压缩 * @param file * @throws IOException */ private static void decompress(File file) throws IOException { Configuration conf = new Configuration(); CompressionCodecFactory factory = new CompressionCodecFactory(conf); //通过扩展名获得编码/解码器 CompressionCodec codec = factory.getCodec(new Path(file.getName())); //通过编码解码器获得输入流 CompressionInputStream in = codec.createInputStream(new FileInputStream(file)); IOUtils.copyBytes(in, System.out, 4096, true); }} 说明：snappy压缩格式在Windows运行失败，不过打包放到集群里面是可以的，前提是你的hadoop集群支持snappy压缩格式","link":"/post/420bf138.html"},{"title":"MapReduce介绍","text":"MapReduce的定义 MapReduce优缺点 优点 缺点 MapReduce核心思想 MapReduce进程 MapReduce编程规范 第一代和第二代MapReduce的区别 ## MapReduce的定义 > &emsp;&emsp;Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。 &emsp;&emsp;Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 ## MapReduce优缺点 ### 优点 1. **MapReduce 易于编程**。它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 2. **良好的扩展性**。当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 3. **高容错性**。MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 4. **适合PB级以上海量数据的离线处理**。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。 ### 缺点 >MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 MapReduce核心思想$ 分布式的运算程序往往需要分成至少2个阶段。第一个阶段的maptask并发实例，完全并行运行，互不相干。 第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 MapReduce进程$ 一个完整的mapreduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调。 MapTask：负责map阶段的整个数据处理流程。 ReduceTask：负责reduce阶段的整个数据处理流程。 MapReduce编程规范$ 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper阶段 12345（1）用户自定义的Mapper要继承自己的父类 （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） （3）Mapper中的业务逻辑写在map()方法中 （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） （5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 Reduce阶段 1234（1）用户自定义的Reducer要继承自己的父类 （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV （3）Reducer的业务逻辑写在reduce()方法中 （4）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 第一代和第二代MapReduce的区别$","link":"/post/5429.html"},{"title":"配置文件","text":"Windows操作系统配置文件 Java配置文件 Hadoop Configuration详解 hadoop 配置文件格式 Configuration类介绍 Configuration类的过程 Configurable接口 ## Windows操作系统配置文件 >配置设置文件（INI）文件是windows操作系统中的一种特殊的ASCII文件，以ini为文件扩展名,作为它的主要文件配置文件标准。该文件也被称为初始化文件initialization file和概要文件profile。 >应用程序可以拥有自己的配置文件，存储应用设置信息，也可以访问windows的基本系统配置文件win.ini中存储的配置信息 INI配置信息分为两部分 节，节标题放在方括号中, [section] 项，一个等式，key=value 1234567;注释;节 [section] ;参数（键=值） name=value INI文件片段 1234[0x0419]1100=Ошибка инициализации программы установки1101=%s1102=%1 Идет подготовка к запуску мастера %2, выполняющего установку программы. Ждите. Windows 提供的API 12345678910111213141516DWORD GetPrivateProfileString( LPCTSTR lpAppName, // If this parameter is NULL, the GetPrivateProfileString function copies all section names in the file to the supplied buffer. LPCTSTR lpKeyName, // If this parameter is NULL, all key names in the section specified by the lpAppNameparameter are copied to the buffer specified by the lpReturnedString parameter. LPCTSTR lpDefault, // If the lpKeyName key cannot be found in the initialization file, GetPrivateProfileString copies the default string to the lpReturnedString buffer. LPTSTR lpReturnedString, // destination buffer DWORD nSize, // size of destination buffer LPCTSTR lpFileName // The name of the initialization file);UINT GetPrivateProfileInt( LPCTSTR lpAppName, //节 LPCTSTR lpKeyName,//项 INT nDefault, //The default value to return if the key name cannot be found in the initialization file. LPCTSTR lpFileName //INI文件名); Java配置文件$ JDK提供了java.util.Properties类，用于处理简单的配置文件。Properties继承自Hashtable相对于INI文件，Properties处理得配合文件格式非常简单 Properties的使用 非XML文件格式12345678//通过指定的键搜索属性public String getProperty(String key)//功能同上，参数defaultValue提供了默认值public String getProperty(String key, String defaultValue)//最终调用Hashtable 的方法putpublic synchronized object setProperty (String key, String value) Properties中的属性通过load)方法加载，该方法从输入流中读取键-值对，而store()方法法则将Properties表中的属性列表写入输出流。使用输入流和输出流，Properties对象但可以保存在文件中，而且还可以保存在其他支持流的系统中，如Web服务器。 123456789101112131415161718192021222324/** * log4j.properties内容如下 * log4j.rootLogger=INFO, stdout * log4j.appender.stdout=org.apache.log4j.ConsoleAppender * log4j.appender.stdout.layout=org.apache.log4j.PatternLayout * log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n * log4j.appender.logfile=org.apache.log4j.FileAppender * log4j.appender.logfile.File=target/spring.log * log4j.appender.logfile.layout=org.apache.log4j.PatternLayout * log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n * @param args * @throws IOException */ public static void main(String[] args) throws IOException { Properties properties = new Properties(); //获取resources目录下的文件流 InputStream stream = MyConfiguration.class.getClassLoader().getResourceAsStream(\"log4j.properties\"); //加载文件获取并获取配合信息 properties.load(stream); String property = properties.getProperty(\"log4j.rootLogger\"); System.out.println(property); /*输出： INFO, stdout */ } Java 1.5之后支持XML配置文件,Properties中的数据也可以以XML格式保存，对应的加载和写出方法是loadFromXML()和storeToXML() storeToXML() 12345Properties props = new Properties();props.setProperty(\"Length\", \"100\");props.setProperty(\"Width\", \"50\")FileOutputStream fos = new FileOutputStream(\"properties.xml\");props.storeToXML(fos, null); loadFromXML() 1234567Properties props = new Properties();InputStream in = MyConfiguaration2.class.getClassLoader().getResourceAsStream(\"properties.xml\");props.loadFromXML(in)String length = props.getProperty(\"Length\");System.out.println(length); xml有指定格式 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;entry key=\"Width\"&gt;50&lt;/entry&gt; &lt;entry key=\"Length\"&gt;100&lt;/entry&gt;&lt;/properties&gt; java.util.Properties提供的能力有限，其他配置信息读写方法，如Apache Jakarta Commons工具集提供的Commons Configuration 【说明】：上面的MyConfiguration都是自定义的类,并且这些配置文件都在resources文件夹下 Hadoop Configuration详解$ Hadoop没有使用java.util.Properties管理配置文件，也没有使用Apache JakartaCommons Configuration 管理配置文件，而是使用了一套独有的配置文件管理系统，并提供自己的API，即使用org.apache.hadoop.conf.Configuration处理配置信息。 hadoop 配置文件格式$12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.9.2/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hadoop 配置文件参数说明$配置参数类型说明 参数名 String 参数值 boolean int long float，也可以是其他类型 参数说明 根元素 configuration configuration下的property元素 property下 name 参数名 value 参数值 description 参数描述 final 相当于java的final关键字，在资源合并时可以防止配置项被覆盖 合并资源$ 合并资源是是指将多个配置合并，产生一个配置文件，如core-site.xml和core-defualt.xml通过addResources()方法合并 1234567Configuration conf = new Configuration();//加载resources文件夹内容ClassLoader classLoader = HadoopConfiguaration.class.getClassLoader//添加合并资源conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-site.xml\")));conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-default.xml\")System.out.println(conf.get(\"fs.defaultFS\")); 【注意】如果第一个配置中存在final，则会以下出现警告,并且值不发生更改 1232019-11-12 17:31:34,548 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@cac736f:an attempt to override final parameter: fs.defaultFS; Ignoring. 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@1d7acb34:an attempt to override final parameter: fs.defaultFS; Ignoring. 测试用的core-default.xml文件 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://vmaster:8200&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.3.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;123&lt;/name&gt; &lt;value&gt;wer&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Configuation类的一般过程 构造Configuration对象 添加需要加载的资源 addResource()方法 然后通过set/get 方法访问/设置配置项 【说明】资源会在第一次使用时自动加载到对象中 Configuration类介绍$类图$ 说明$类图中，Configuration有7个非静态成员 布尔变量quietmode，用来设置加载配置的模式。如果quietmode为true (默认值)，则在加载解析配置文件的过程中，不输出日志信息。quietmode只是-一个方便开发人员调试的变量。 数组resources保存了所有通过addResource()方法添加Configuration对象的资源 Configuration.addResource()有如下4种形式: 1234public void addResource (InputStream in) //已打开的输入流public void addResource (Path file) //Hadoop文件路径public void addResource (String name) //CLASSPATH 资源 String形式public void addResource (URL url) //URL,统一资源定位符，如https://lyhcc.github.io 布尔变量loadDefaults用于确定是否加载默认资源，这些默认资源保存在defaultResources中。注意，defaultResources 是个静态成员变量，通过方法addDefaultResource()可以添加系统的默认资源。在HDFS中，会把hdfs-default.xml和hdfs-site.xml作为默认资源，并通过addDefaultResource()保存在成员变量defaultResources中;在MapReduce中，默认资源是mapred-default.xml和mapred-site.xml。1234567//下面的代码来自hadoop-1.x 的org.apache.hadoop.hdfs.server.datanode.DataNode static{ Configuration . addDe faultResource (\"hdfs-default . xml\") ; Conf igurat ion. addDe faultResource (\"hdfs-site. xml\") ;}//在hadoop2.x以后这代码被移到了Configuration类里面//hadoop-2.8.4中的1809行 properties 存放Hadoop配置文件解析后的键-值对，为java.util.Properties类型 finalParameters 类型是Set， 用来保存所有在配置文件中已经被声明为final的键-值对的键 overlay用于记录通过set()方式改变的配置项。也就是说，出现在overlay中的键-值对是应用设置的，而不是通过对配置资源解析得到的，为java.util.Properties类型 Configuration 是一一个类加载器变量，可以通过它来加载指定类，也可以通过它加载相关的资源。 上面提到addResource()可以通过字符串方式加载CLASSPATH资源，它其实通过Configuration中的getResource()将字符串转换成URL资源 123public URL getResource (String name){ return classLoader.getResource(name) ;} 2.8.4版本的Configuration类的1188行 Configuration类的过程$构造Configuration对象$资源加载$添加资源到Configuration对象的方法有两种 对象的addResource()方法 类的静态addDefaultResource()方法(设置了loadDefaults标志) 添加的资源并不会立即被加载，只是通过reloadConfiguration()方法清空properties和finalParameters。相关代码如下: 123456789101112131415 //以URL资源为例public void addResource(URL url) { this.addResourceObject(new Configuration.Resource(url)); } //添加资源 private synchronized void addResourceObject(Configuration.Resource resource){ this.resources.add(resource); this.restrictSystemProps |= resource.isParserRestricted(); this.reloadConfiguration(); } //资源重新加载触发函数 public synchronized void reloadConfiguration() { this.properties = null; this.finalParameters.clear(); } 以上是类的成员方法addResource()方法的调用。 静态方法dDefaultResource()也可以清空Configuration对象中的数据（非静态成员），只不过是通过需要通过REGISTRY作为媒介进行。能够调用是因为REGISTRY记录了系统所有的Configuration对象REGISTRY的定义以及为其添加参数的过程 12345678private static final WeakHashMap&lt;Configuration, Object&gt; REGISTRY = new WeakHashMap();public Configuration(boolean loadDefaults) { ... synchronized(Configuration.class) { REGISTRY.put(this, (Object)null); }} 成员变量properties中的数据只在被调用的时候才会被加载进来。在getProps方法中，properties为空时，会触发loadResources()执行 123456789101112131415161718192021222324protected synchronized Properties getProps() { if (this.properties == null) { this.properties = new Properties(); Map&lt;String, String[]&gt; backup = new ConcurrentHashMap(this.updatingResource); this.loadResources(this.properties, this.resources, this.quietmode); ... } return this.properties;}//加载默认资源private void loadResources(Properties properties, ArrayList&lt;Configuration.Resource&gt; resources, boolean quiet) { if (this.loadDefaults) { Iterator i$ = defaultResources.iterator(); while(i$.hasNext()) { String resource = (String)i$.next(); this.loadResource(properties, new Configuration.Resource(resource, false), quiet); } if (this.getResource(\"hadoop-site.xml\") != null) { this.loadResource(properties, new Configuration.Resource(\"hadoop-site.xml\", false), quiet); }} hadoop配置文件解析$ hadoop 的配置文件都是XML文件，JAXP(JAVA API for XML processing)是一种稳定的、可靠的XML处理API，支持两种XML处理方法 SAX解析(Simple API for XML) DOM解析(Documnet Object Model) hadoop使用的DOM解析 两种解析的区别 SAX 提供了一种流式的、事件驱动的XML处理方式 缺点：编写处理逻辑比较复杂 优势：适合处理大的XML文件 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档 Configurable接口$ Configurable是一个很简单的接口，位于org.apache.hadoop.conf包中 类图 hadoop 代码中存在大量实现了该接口的类，可以通过setConf方法设置配置参数简化创建和setConf的两个步骤java反射机制实现，利用org.apache.hadoop.util.ReflectionUtils的newInstance方法 1public static &lt;T&gt; T newInstance(Class&lt;T&gt; theClass, Configuration conf) 该方法调用了ReflectionUtils中的setConf方法 12345678910public static void setConf(Object theObject, Configuration conf) { if (conf != null) { if (theObject instanceof Configurable) { ((Configurable)theObject).setConf(conf); } setJobConf(theObject, conf); } }","link":"/post/fa571a7.html"},{"title":"Hadoop IPC 数据分帧和读写","text":"数据通信中定界的方法 ## 数据通信中定界的方法 - 定长消息:通信双方发送的消息长度是固定的，接收者只需要简单地将数据读入对应的缓冲区中，就可以获得消息。 - 基于定界符:消息的结束由唯一标记指出，消息发送者在传输完数据后，添加一-个特殊的字节序列。这个特殊的标记不能在传输的数据中出现，接收者简单地扫描输入信 息并查找定界符，并将定位符前面的数据形成消息交给上层应用。 - 显式长度:在具体消息前面附加一一个固定大小的字段，指示该消息包含多少字节。接 收者首先以定长消息的方式接受长度字段，然后根据这个长度接收消息。 Hadoop IPC通信的定界方法 客户端-&gt;服务器端：显式长度 服务器-&gt;客户端：定长消息，通过Writable序列化","link":"/post/a9423fcb.html"},{"title":"HDFS快照管理","text":"快照管理 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 快照影响 快照创建瞬间完成，所耗时间成本为O(1) 快照修改时才会使用额外的额外的内存空间，内存成本O(M),M表示修改过的文件或目录数 快照记录块和文件大小，不对DataNode中的块进行复制 说明：* 快照可以在HDFS任何目录下设置，一个目录最多容纳65536个并发快照 基本语法12345678（1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能）（2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）（3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照）（4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照）（5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）（6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有已快照目录）（7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处）（8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 案例实操 开启/禁用指定目录的快照功能 指定创建目录的位置为 /tmp/snapshot(即快照的存储目录)，在指定目录之前必须创建目录，不然会报错 12hdfs dfsadmin -allowSnapshot /tmp/snapshot hdfs dfsadmin -disallowSnapshot /tmp/snapshot //禁用时，对应的目录不允许存在快照 对目录创建快照 只有被开启快照功能的目录才能创建快照 123hdfs dfs -createSnapshot /tmp/snapshot // 对目录创建快照hdfs dfs -createSnapshot /tmp/snapshot snapshot //重命名快照（注：快照是只读的，无法修改名）通过web访问hdfs://Master:9000/tmp/snapshot/.snapshot/s…..// 快照和源文件使用相同数据块 查看快照 12hdfs dfs -lsr /tmp/snapshot/.snapshot/ //查看快照目录的详细信息hdfs lsSnapshottableDir //查看所有允许快照的目录 更改快照名字 12345hdfs dfs -renameSnapshot /tmp/snapshot/ snapshot snapshot1 注：路径只是你创建得名字/tmp/snapshot，不要带后边得/tmp/snapshot/.snapshot/，不然会出现hdfs dfs -renameSnapshot /tmp/snapshot/.snapshot/ snapshot1 snapshotrenameSnapshot: Modification on a read-only snapshot is disallowed 比较两个快照目录的不同之处 1234[root@vmaster opt]# hdfs snapshotDiff /tmp/snapshot s1 s2Difference between snapshot s1 and snapshot s2 under directory /tmp/snapshot:M .+ ./p 符号的意义： 符号 含义 + 文件或者目录被创建 - 文件或目录被删除 M 文件或目录被修改 R 文件或目录被重命名 恢复快照 123451.自定义创建一个快照名：hdfs dfs -createSnapshot /HAHA1 snapshot12.展示原文件包含内容：Hadoop fs -ls /HAHA13.里面有五个文件、删除其中1~2个/HAHA1/.snapshot/snapshot14.回复快照：hdfs dfs -cp /HAHA1/.snapshot/snapshot1 /snapshot 删除快照 12dfs dfs -deleteSnapshot 快照目录 快照名称dfs dfs -deleteSnapshot /tmp/snapshot snapshot1","link":"/post/21534.html"},{"title":"Hadoop IPC 连接的建立","text":"相关参数说明 相关方法说明 服务器端 **** Hadoop 2.x **** ## 相关参数说明 - connections 用于保存ConnectionId到Connection的映射，位于org.apache.hadoop.ipc.Client中 1private ConcurrentMap&lt;Client.ConnectionId, Client.Connection&gt; connections; - calls 当前正在处理的远程调用，位于org.apache.hadoop.ipc.Client.Connection中 1private Hashtable&lt;Integer, Client.Call&gt; calls = new Hashtable(); - shouldCloseConnection 连接关闭标志 1private AtomicBoolean shouldCloseConnection = new AtomicBoolean(); 相关方法说明$ getConnection Client需要获取连接的时候，调用getConnection方法，该方法先检查connections中是否存在满足条件的IPC连接。有，则 复用 ，否则，创建新的连接复用是指connection相等（connection里面的三个参数相等则说明两个connection相等），就使用同一个connection 相关代码如下 123456789101112131415161718192021222324252627 private Client.Connection getConnection(Client.ConnectionId remoteId, Client.Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth) throws IOException { //首先，看看客户端是否还在运行 if (!this.running.get()) { throw new IOException(\"The client is stopped\"); } else { while(true) { //2. 查一下是否存在remoteId对应的连接connection Client.Connection connection = (Client.Connection)this.connections.get(remoteId); //connection==null，表明不存在，就需要创建一个新的IPC连接 if (connection == null) { //创建连接 connection = new Client.Connection(remoteId, serviceClass); Client.Connection existing = (Client.Connection)this.connections.putIfAbsent(remoteId, connection); if (existing != null) { connection = existing; } } //将IPC调用放入IPC连接中 if (connection.addCall(call)) { connection.setupIOstreams(fallbackToSimpleAuth); return connection; } this.connections.remove(remoteId, connection); } }} addCall 作用是将一个IPC调用放入IPC连接中&emsp;如果成员变量shouldCloseConnection为true, 返回false，这样可以防止将一个IPC调用放入一个已经关闭的IPC连接中。&emsp;否则，将调用放入IPC连接calls中再说一下，为什么会有这种情况？IPC来呢及可以在多个地方被触发,进入关闭过程，但知道Connection.close方法被调用，对应的connection才会在connections中删除。删除后的连接只有新建后才能将IPC调用传入连接中相关代码 12345678910 private synchronized boolean addCall(Client.Call call) { if (this.shouldCloseConnection.get()) { return false; } else { this.calls.put(call.id, call); //notify是唤醒等待的线程，因为这个方法会有多个地方调用，但进来的只能有一个 this.notify(); return true; }} setupIOstreams 该方法是使客户端和服务器通过Socket连接起来连接失败的话，会重传，最多maxRetries，可以设置${ipc.client.connect.max.retries} 12345678910111213141516171819202122232425262728293031323334353637 private synchronized void setupIOstreams(AtomicBoolean fallbackToSimpleAuth) { if (this.socket == null &amp;&amp; !this.shouldCloseConnection.get()) { ... if (Client.LOG.isDebugEnabled()) { Client.LOG.debug(\"Connecting to \" + this.server); } Span span = Tracer.getCurrentSpan(); if (span != null) { span.addTimelineAnnotation(\"IPC client connecting to \" + this.server); } short numRetries = 0; Random rand = null; while(true) { /** * 建立Socket连接，具体可以查看源代码Ctrl + 鼠标左键查看 * connection使用Socket连接设置了tcpNoDelay标志，禁用Nagle算法,无需等待直接发送 * 配置项${ipc.client.tcpnodelay} */ this.setupConnection(ticket); ... //与IPC服务器进行握手 this.writeConnectionContext(this.remoteId, this.authMethod); //更改最后访问时间lastActivity,该变量也是Client成员变量 this.touch(); ... //启动接受进程 this.start(); ... this.close(); } }} 以上是客户端，下面为IPC连接的另一端 服务器端$ 服务器建立IPC连接的代码分散在Listener和Server.Connection中Listener基于Java NIO开发的，是一个标准的NIO应用，其构造函数中打开服务器端口，创建Selector监听注意参数backlogLength，它由ipc.server.listen.queue.size参数指定，backlogLength是调用ServerSocket.bind()时可以额外提供的一个参数，用于指定在监听端口上排队请求的最大长度，队列满了以后的客户端请求，会被拒绝 run方法123456789101112131415161718192021222324252627282930public void run() { ... while(Server.this.running) { SelectionKey key = null; try { //select调用 this.getSelector().select(); //获取相关键 for(Iterator iter = this.getSelector().selectedKeys().iterator(); iter.hasNext(); key = null) { key = (SelectionKey)iter.next(); iter.remove(); try { //判断是不是可接受事件 if (key.isValid() &amp;&amp; key.isAcceptable()) { //doAccept方法,接受客户端请求、注册Socket到选择器上， //并且创建Reader，Reader里面的run方法处理 OP_READ //该方法中最主要的是ReadAndProcess方法 this.doAccept(key); } } catch (IOException var8) { } ... } }","link":"/post/28a780a5.html"},{"title":"Python/python数据可视化/散点图","text":"","link":"/post/1620326b.html"},{"title":"python可视化","text":"","link":"/post/c0163c11.html"},{"title":"十大经典算法的优缺点","text":"KNN Apriori 决策树 CART EM （Expectation Maximization） 朴素贝叶斯 K-means PageRank KNN$优点： 理论成熟，实现简单 缺点： 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数 计算量较大 Apriori$优点： 适合稀疏数据集。 算法原理简单，易实现。 适合事务数据库的关联规则挖掘 缺点： 多次扫描事务数据库，需要很大的I/O负载 对每次k循环，侯选集Ck中的每个元素都必须通过扫描数据库一次来验证其是否加入Lk。假如有一个最大频繁项目集包含10个项的话，那么就至少需要扫描事务数据库10遍。 可能产生庞大的侯选集 由Lk-1产生k-侯选集Ck是指数增长的，例如104个1-频繁项目集就有可能产生接近107个元素的2-侯选集。如此大的侯选集对时间和主存空间都是一种挑战。 决策树$ID3 优点： 理论清晰，方法简单 学习能力强 ID3算法在搜索的每一步都使用当前的所有训练样例，大大降低了对个别训练样例错误的敏感性 缺点： 只对比较小的数据集有效，且对噪声比较敏感，当训练数据集加大时，决策树可能会随之改变。 ID3算法在搜索过程中不进行回溯。收敛到局部最优而不是全局最优 ID3算法只能处理离散值的属性。 信息增益度量存在一个内在偏置，它偏袒具有较多值的属性。如日期属性。 ID3算法增长树的每一个分支的深度，直到恰好能对训练样例完美地分类。当数据中有噪声或训练样例的数量太少时，产生的树会过渡拟合训练样例。 ID3的改进 C4.5 优点： 通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免出现过拟合的特性，提升决策树的泛化能力。 CART$优点： 可以处理连续值 缺点： EM （Expectation Maximization）$优点： 简单稳定 缺点： 在缺失数据较多的情形,收敛的速度较慢，次数多，容易陷入局部最优 对于某些情况下,要计算算法中的M步,即完成对似然函数的估计是非常困难的 在某些情况下是要获得EM算法中的E步的期望显式是非常困难或者不可能的 朴素贝叶斯$优点： 生成式模型，通过计算概率来进行分类，可以用来处理多分类问题， 对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。 缺点： 对输入数据的表达形式很敏感， 由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。 需要计算先验概率，分类决策存在错误率。 K-means$优点： 可解释性比较强。 调参的参数仅为簇数k。 相对于高斯混合模型而言收敛速度快，因而常用于高斯混合模型的初始值选择。K-means 的时间复杂度为 O(N⋅K⋅I) ，簇数 K 和 迭代次数 I 通常远小于N，所以可优化为 O(N) ，效率较高。 缺点： 对离群点敏感。 K值难以事先选取，交叉验证不大适合 PageRank$优点： PageRank算法通过网页间的链接来评价网页的重要性，在一定程度上避免和减少了人为因素对排序结果的影响； 采用与查询无关的离线计算方式，使其具有较高的响应速度； 一个网页只能通过别的网页对其引用来增加自身的PR值，且算法的均分策略使得一个网页的引用越多，被引用网页所获得的PR值就越少。 因此，算法可以有效避免那些为了提高网站的搜索排名而故意使用链接的行为。 缺点： 主题漂移问题 PageRank 算法仅利用网络的链接结构，无法判断网页内容上的相似性；且算法根据向外链接平均分配权值使得主题不相关的网页获得与主题相关的网页同样的重视度，出现主题漂移。 偏重旧网页问题 决定网页 P R 值的主要因素是指向它的链接个数的多少。一个含有重要价值的新网页，可能因为链接数目的限制很难出现在搜索结果的前面，而不能获得与实际价值相符的排名。 算法并不一定能反映网页的重要性，存在偏重旧网页现象。 忽视用户个性化问题 PageRank算法在设计之初，没有考虑用户的个性化需要。个性化搜索引擎的兴起，对 PageRank排序算法提出新的挑战。 参考： 朴素贝叶斯 https://www.jianshu.com/p/6309e084ce64 K-means https://www.cnblogs.com/massquantity/p/9416109.html","link":"/post/85ed9b6d.html"},{"title":"Kafka初始","text":"kafka的设计主要目标 为什么使用消息系统 Kafka架构 Kafka网络拓扑 Kafka的通信过程详解 kafka的设计主要目标$ 以复杂度O(1)的方式提供消息持久化能力，即使对TB级以上的数据也能保证常数的访问性能 高吞吐率，即使在非常廉价的商用机器也能做到单机支持秒100k条消息的传输 支持Kafka Server间消息分区，及分布式消费，同时保证分区内的消息顺序传输 支持离线的数据处理和实时数据处理 支持在线水平扩展 为什么使用消息系统$ 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可 灵活性和峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证性 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性） 缓冲 消息队列通过一个缓冲层来帮助任务最高效率地执行，写入队列的处理尽可能开始。有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 异步通信 很多时候，用户不想也不需要立即处理信息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但不立即处理，想要队列中放入多少数据就放多少，然后在需要的时候再去处理 Kafka架构$ Kafka集群中将消息以Topic命名的消息队列中，消费者订阅发往某个Topic命名的消息队列queue中的消息其中Kafka集群由若干个Broker组成，Topic中含有多个Partition，每个Partition中通过Offset来获取 Producer：消息生产者，即将消息发布到指定的Topic中，同时Producer也能绝对消息所属的Partition Consumer：消息消费者，即向指定的Topic获取消息，根据指定的Topic的分区索引及其对应分区上的的消息偏移量来获取消息 Consumer Group: 消费者组，每一个Consumer属于一个Consumer Group,每一个Consumer Group包含一个或多个Consumer。如果所有的Consumer都具有相同的Consumer Group，那么消息将会在Consumer之间进行负载均衡，这也是传统的消息系统 “队列”模型。也就是说一个Partition中的消息只会被具有相同groupID的Consumer消费，并且每个Consumer Group之间是相互独立的。如果要实现“发布-订阅”模型，则每个消费 者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。 Broker:一台Kafka服务器即使一个Broker，一个集群由多个Broker组成，一个Broker可以容纳多个Topic，Broker之间的关系基本是平等的，并不像Hadoop集群那样存在主从模式和为防止单点故障Standby节点 Topic:每条发送到Kafka集群的消息都属于某个Topic。物理上Topic是分开存储的，逻辑上，用户读写数据时并不需要关心他们是存储到哪里的 Partition：Kafka集群为了实现可扩展性，一个非常大的Topic可以分成多个Partition，从而分布到多台Broker中。Partition中的每条消息都会分配有一个自增ID(Offset)。Kafka保证一个Partition内的消息的有序，不保证一个Topic的Partition之间有序 Offset：消息Topic的partition中的位置，同一个Partition中，随着消息的写入，对应Offset自增 Replica：副本。Topic的Partition含有N个副本。其中一个是Replica的Leader，其他的都是Follower，Leader处理partition的读写请求，与此同时，Follower会定期的去同步Leader上的数据 Message：消息，是通信的结伴单位，每个Producer可以向一个Topic发布一些Message Zookeeper：存放Kafka集群的元数据组件。在Zookeeper集群中会保存Topic的状态信息，如分区个数，分区组成，分区分布情况等、保存Broker的状态信息、保存消费者的消费信息等。通过这些信息，Kafka很好地将消息生产、消息存储、消息消费的过程结合起来 Kafka网络拓扑$说明 Producer根据指定的路由方法（Round-Robin、Hash等），将消息Push到Topic中的某个Partition中 Kafka集群收到Producer发来的信息后，将其持久化到硬盘，并保留消息的指定时长（可配置），而不关注消息是否被消费 Consumer从Kafka集群中Pull数据，并控制获取消息的Offset Kafka的通信过程详解$ 先说一下KafkaController,它是Broker内部负责管理分区和副本状态以及异常情况下分区重新分配等功能的模块，每个Kafka集群只有一个KafkaController为Leader其他的为Standby，当一个Leader挂掉后，Zookeeper或选举一个新的KafkaController为Leader","link":"/post/df89d65f.html"},{"title":"Kafka通信详情","text":"ProducerRequest:生产者发送消息的请求，生产者将消息发送至Kafka集群中的某个Broker, Broker 接收到此请求后持久化此消息并更新相关元数据信息。（发送消息） TopicMetadataRequest :获取Topic元数据信息的请求，无论是生产者还是消费者都需要通过此请求来获取感兴趣的Topic的元数据。（获取感兴趣Topic元数据） FetchRequest:消费者获取感兴趣Topic的某个分区的消息的请求，除此之外，分区状态为Follower的副本也需要利用此请求去同步分区状态为Leader的对应副本数据。 （获取感兴趣Topic分区消息） OffsetRequest:消费者发送至Kafka集群来获取感兴趣Topic的分区偏移量的请求，通过此请求可以获知当前Topic所有分区在不同时间段的偏移量详情。（获取消费者感兴趣的分区偏移量） OffsetCommitRequest:消费者提交Topic被消费的分区偏移量信息至Broker, Broker接收到此请求后持久化相关偏移量信息。（提交消费了的分区偏移量） OffsetFetchRequest:消费者发送 获取 提交至Kafka集群的相关Topic被消费的 详细信息，和OffsetCommitRequest相互对应。（获取被消费的消息） LeaderAndlsrRequest:当Topic的某个分区状态发生变化时，处于Leader状态的KafkaController发送此请求至相关的Broker,通知其做出相应的处理。 （检测Topic分区变化状态） StopReplicaRequest:当Topic的某个分区被删除或者下线的时候，处于Leader状态的KafkaController发送此请求至相关的Broker,通知其做出相应的处理。（检测分区是否在线） UpdateMetadataRequest:当Topic 的元数据信息发生变化时，处于Leader状态的KafkaContoller发送此请求至相关的Broker,通知其做出相应的处理。（Topic元数据变化） BrokerContolledShutdownRequest:当Broker正常下线时，发生此请求至处于Leader状态的KafnRaCotoller。（Broker下线） ConsumerMetadataRequest: 获取保存特定Consumer Group消费详情的分区信息。（获取CG信息） 交互过程 Producer和Kafka集群: Producer需要利用ProducerRequest和TopicMetadataRequest米完成 Topic元数据的查询、消息的发送。 Consumer和Kafka集群: Consumer需要利用TopicMetadataRequest请求，FetchRequest请求。OffsetRequest 请求、OffsetCommitRequest 请求、fsetFechnRequest 请求和ConsumerMetadataRequest请求来完成Topic元数据的查询、消息的订阅、历史偏移量的查询、偏移量的提交、当前偏移量的查询。 Kafkaontroller状态为Leader的Broker和KafkaController状态为Standby的Broker:KafkaContoller 状态为Leader的Broker需要利用LeaderAndIsrRequest请求、Stop-ReplicaRequest请求，UpdateMetadataRequest 请求来完成对Topic的管理; Kafka-Controller状态为Standby 的Broker需要利用BrokerControlledShutdownRequest请求来通知KafkaContoller状态为Leader的Broker自己的下线动作。 Broker和Broker之间: Broker相互之间需要利用FetchRequest请求来同步Topic分区的副本数据，这样才能使Topic分区各副本数据实时保持一致。","link":"/post/bc0600c8.html"}],"tags":[{"name":"分布式存储管理","slug":"分布式存储管理","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"name":"NoSSQL特点","slug":"NoSSQL特点","link":"/tags/NoSSQL%E7%89%B9%E7%82%B9/"},{"name":"hadoop配置文件解析","slug":"hadoop配置文件解析","link":"/tags/hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/"},{"name":"DOM解析","slug":"DOM解析","link":"/tags/DOM%E8%A7%A3%E6%9E%90/"},{"name":"什么是大数据","slug":"什么是大数据","link":"/tags/%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Java NIO","slug":"Java-NIO","link":"/tags/Java-NIO/"},{"name":"Java代理机制","slug":"Java代理机制","link":"/tags/Java%E4%BB%A3%E7%90%86%E6%9C%BA%E5%88%B6/"},{"name":"Java Proxy","slug":"Java-Proxy","link":"/tags/Java-Proxy/"},{"name":"Java远程调用","slug":"Java远程调用","link":"/tags/Java%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"},{"name":"RMI","slug":"RMI","link":"/tags/RMI/"},{"name":"Java代理","slug":"Java代理","link":"/tags/Java%E4%BB%A3%E7%90%86/"},{"name":"Hadooop介绍","slug":"Hadooop介绍","link":"/tags/Hadooop%E4%BB%8B%E7%BB%8D/"},{"name":"Google三宝","slug":"Google三宝","link":"/tags/Google%E4%B8%89%E5%AE%9D/"},{"name":"Java NIO实例：回显服务器","slug":"Java-NIO实例：回显服务器","link":"/tags/Java-NIO%E5%AE%9E%E4%BE%8B%EF%BC%9A%E5%9B%9E%E6%98%BE%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Hadoop IPC","slug":"Hadoop-IPC","link":"/tags/Hadoop-IPC/"},{"name":"Hadoop RPC","slug":"Hadoop-RPC","link":"/tags/Hadoop-RPC/"},{"name":"Hadoop 通信机制","slug":"Hadoop-通信机制","link":"/tags/Hadoop-%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/"},{"name":"hadoop序列化","slug":"hadoop序列化","link":"/tags/hadoop%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"MapReduce 通信协议","slug":"MapReduce-通信协议","link":"/tags/MapReduce-%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"},{"name":"Hadoop 协议","slug":"Hadoop-协议","link":"/tags/Hadoop-%E5%8D%8F%E8%AE%AE/"},{"name":"Hadoop 压缩","slug":"Hadoop-压缩","link":"/tags/Hadoop-%E5%8E%8B%E7%BC%A9/"},{"name":"Hadoop 文件分片","slug":"Hadoop-文件分片","link":"/tags/Hadoop-%E6%96%87%E4%BB%B6%E5%88%86%E7%89%87/"},{"name":"MapReduce介绍","slug":"MapReduce介绍","link":"/tags/MapReduce%E4%BB%8B%E7%BB%8D/"},{"name":"Windows操作系统配置文件","slug":"Windows操作系统配置文件","link":"/tags/Windows%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Java配置文件","slug":"Java配置文件","link":"/tags/Java%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Hadoop配置文件","slug":"Hadoop配置文件","link":"/tags/Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"HDFS快照","slug":"HDFS快照","link":"/tags/HDFS%E5%BF%AB%E7%85%A7/"},{"name":"Hadoop 如何建立 IPC连接","slug":"Hadoop-如何建立-IPC连接","link":"/tags/Hadoop-%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B-IPC%E8%BF%9E%E6%8E%A5/"},{"name":"Hadoop IPC通信中是如何知道数据长度的","slug":"Hadoop-IPC通信中是如何知道数据长度的","link":"/tags/Hadoop-IPC%E9%80%9A%E4%BF%A1%E4%B8%AD%E6%98%AF%E5%A6%82%E4%BD%95%E7%9F%A5%E9%81%93%E6%95%B0%E6%8D%AE%E9%95%BF%E5%BA%A6%E7%9A%84/"},{"name":"Hadoop IPC 数据分帧和读写","slug":"Hadoop-IPC-数据分帧和读写","link":"/tags/Hadoop-IPC-%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%A7%E5%92%8C%E8%AF%BB%E5%86%99/"},{"name":"十大经典算法优缺点","slug":"十大经典算法优缺点","link":"/tags/%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9/"},{"name":"Kafka初识","slug":"Kafka初识","link":"/tags/Kafka%E5%88%9D%E8%AF%86/"},{"name":"kafka通信","slug":"kafka通信","link":"/tags/kafka%E9%80%9A%E4%BF%A1/"}],"categories":[{"name":"BigData","slug":"BigData","link":"/categories/BigData/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"Hadoop","slug":"BigData/Hadoop","link":"/categories/BigData/Hadoop/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Hadoop_common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"Hadoop common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"MapReduce","slug":"BigData/Hadoop/MapReduce","link":"/categories/BigData/Hadoop/MapReduce/"},{"name":"HDFS","slug":"BigData/Hadoop/HDFS","link":"/categories/BigData/Hadoop/HDFS/"},{"name":"IPC","slug":"BigData/Hadoop/Hadoop-common/IPC","link":"/categories/BigData/Hadoop/Hadoop-common/IPC/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Python可视化","slug":"Python/Python可视化","link":"/categories/Python/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"DataMining","slug":"DataMining","link":"/categories/DataMining/"},{"name":"Kafka","slug":"Kafka","link":"/categories/Kafka/"}]}