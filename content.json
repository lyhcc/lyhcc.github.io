{"pages":[{"title":"分类","text":"","link":"/categories/index.html"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/about/about.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"DOM解析","text":"说明：这里主要分析hadoop的DOM解析 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档处理步骤 获得用于创建DOM解析器的工厂对象1DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); 可以设置一下参数[可选]12345678docBuilderFactory.setIgnoringComments(true);docBuilderFactory.setNamespaceAware(true);boolean useXInclude = !wrapper.isParserRestricted();try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28);} 获得解析XML的DocumentBuilder对象1DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); 获取根节点下的所有节点1NodeList props = root.getChildNodes(); 遍历节点123456789101112for(int i = 0; i &lt; props.getLength(); ++i) { //获取节点 Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; //prop.getTagName()获取节点内的值 if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { ... } hadoop配置文件解析的特别说明 对DocumentBuilderFactory做的处理 忽略XML文档中的注释1docBuilderFactory.setIgnoringComments(true); 支持XML命名空间1docBuilderFactory.setNamespaceAware(true); 支持XML包含机制12345try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \")for parser \" + docBuilderFactory, var28);} XInclude机制允许将XML文档分解为多个可管理的块，然后将-一个或多个较小的文档组装成一个大型文档。 hadoop 配置文件解析完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157private Configuration.Resource loadResource(Properties properties, Configuration.Resource wrapper, boolean quiet) { String name = \"Unknown\"; try { Object resource = wrapper.getResource(); name = wrapper.getName(); DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); docBuilderFactory.setIgnoringComments(true); docBuilderFactory.setNamespaceAware(true); boolean useXInclude = !wrapper.isParserRestricted(); try { docBuilderFactory.setXIncludeAware(useXInclude); } catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28); } if (wrapper.isParserRestricted()) { docBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true); } DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); Document doc = null; Element root = null; boolean returnCachedProperties = false; if (resource instanceof URL) { doc = this.parse(builder, (URL)resource); } else if (resource instanceof String) { URL url = this.getResource((String)resource); doc = this.parse(builder, url); } else if (resource instanceof Path) { File file = (new File(((Path)resource).toUri().getPath())).getAbsoluteFile(); if (file.exists()) { if (!quiet) { LOG.debug(\"parsing File \" + file); } doc = this.parse(builder, new BufferedInputStream(new FileInputStream(file)), ((Path)resource).toString()); } } else if (resource instanceof InputStream) { doc = this.parse(builder, (InputStream)resource, (String)null); returnCachedProperties = true; } else if (resource instanceof Properties) { this.overlay(properties, (Properties)resource); } else if (resource instanceof Element) { root = (Element)resource; } if (root == null) { if (doc == null) { if (quiet) { return null; } throw new RuntimeException(resource + \" not found\"); } root = doc.getDocumentElement(); } Properties toAddTo = properties; if (returnCachedProperties) { toAddTo = new Properties(); } if (!\"configuration\".equals(root.getTagName())) { LOG.fatal(\"bad conf file: top-level element not &lt;configuration&gt;\"); } NodeList props = root.getChildNodes(); Configuration.DeprecationContext deprecations = (Configuration.DeprecationContext)deprecationContext.get(); for(int i = 0; i &lt; props.getLength(); ++i) { Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { if (wrapper.isParserRestricted() &amp;&amp; \"http://www.w3.org/2001/XInclude\".equals(prop.getNamespaceURI())) { throw new RuntimeException(\"Error parsing resource \" + wrapper + \": XInclude is not supported for restricted resources\"); } LOG.warn(\"Unexpected tag in conf file \" + wrapper + \": expected &lt;property&gt; but found &lt;\" + prop.getTagName() + \"&gt;\"); } NodeList fields = prop.getChildNodes(); String attr = null; String value = null; boolean finalParameter = false; LinkedList&lt;String&gt; source = new LinkedList(); //遍历所有节点，并根据情况设置对象的成员变量properties和finalParameters for(int j = 0; j &lt; fields.getLength(); ++j) { Node fieldNode = fields.item(j); if (fieldNode instanceof Element) { Element field = (Element)fieldNode; if (\"name\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { attr = StringInterner.weakIntern(((Text)field.getFirstChild()).getData().trim()); } if (\"value\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { value = StringInterner.weakIntern(((Text)field.getFirstChild()).getData()); } if (\"final\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData()); } if (\"source\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { source.add(StringInterner.weakIntern(((Text)field.getFirstChild()).getData())); } } } source.add(name); if (attr != null) { if (deprecations.getDeprecatedKeyMap().containsKey(attr)) { Configuration.DeprecatedKeyInfo keyInfo = (Configuration.DeprecatedKeyInfo)deprecations.getDeprecatedKeyMap().get(attr); keyInfo.clearAccessed(); String[] arr$ = keyInfo.newKeys; int len$ = arr$.length; for(int i$ = 0; i$ &lt; len$; ++i$) { String key = arr$[i$]; this.loadProperty(toAddTo, name, key, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } else { this.loadProperty(toAddTo, name, attr, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } } } } if (returnCachedProperties) { this.overlay(properties, toAddTo); return new Configuration.Resource(toAddTo, name, wrapper.isParserRestricted()); } else { return null; } } catch (IOException var29) { LOG.fatal(\"error parsing conf \" + name, var29); throw new RuntimeException(var29); } catch (DOMException var30) { LOG.fatal(\"error parsing conf \" + name, var30); throw new RuntimeException(var30); } catch (SAXException var31) { LOG.fatal(\"error parsing conf \" + name, var31); throw new RuntimeException(var31); } catch (ParserConfigurationException var32) { LOG.fatal(\"error parsing conf \" + name, var32); throw new RuntimeException(var32); }}","link":"/post/e9fcf702.html"},{"title":"分布式存储管理","text":"为什么直接采用关系模型的分布式数据库并不能适应大数据时代的？ 规模效应所带来的压力 传统数据库倾向于纵向扩展(Scale-Up)，即增加单台计算机的性能 适应大数据的数据库系统的应该具有良好的横向扩展(Scale-Out)，即为集群增加一台计算机 数据类型的多样化 传统数据类型： 结构化数据大数据时代的数据类型： 结构化数据 半结构化数据 非结构化数据 设计理念所带来的冲突 关系型数据库 One size fits all ,即面对不同问题不需要重新考虑数据管理问题 简单来说就是，单一模式可以适应所有变化 新理念 “One size fits one” 和 “One size fits domain” 数据库的事务特性 传统数据库ACID特性 A(Atom，原子性)，C(Consistency，一致性)，I(Isolation,隔离性)，D(Durability,持久性) 大数据时代的数据库BASE。 Basically Available(基本可用)，Soft State(柔性状态)，Eventually Consistency(最终一致性)根据分布式领域著名的CAP理论来看，ACID追求一致性C,而BASE更加关注A一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）新型数据库Spanner NoSQL 特点 模式自由（Schema-free） 支持简易备份（Easy Replication Support) 简单应用程序接口（Simple API） 最终一致性（或说支持BASE特性，不支持ACID特性） 支持海量数据（Huge Amount of Data）","link":"/post/4faba951.html"},{"title":"others/Java_NIO","text":"","link":"/post/f2d80d11.html"},{"title":"大数据介绍","text":"什么是大数据 百度百科的定义,大数据（BIG DATA），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 在目前的业界尚未对大数据由清晰明确的定义, 它的第一次出现是在麦肯锡公司的报告中出现的, 在维基百科上的较为模糊的定义是很难运用软件的手段获取大量的内容信息, 对其处理后整理得出的数据集合。其他计算机学科的学者给出的定义是数据的尺度极为巨大, 常规的数据处理软件无法对数据识别、存储和应用的海量数据信息 维基百科的定义，大数据是指无法在可承受的时间范围内用常规软件工具进行捕捉、管理和处理的数据集合。 研究机构Gartner定义，“大数据”是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 数据单位1MB = 1024KB、1GB = 1024MB1TB = 1024GB、1PB = 1024TB 大数据的特征 容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息； 种类（Variety）：数据类型的多样性； 速度（Velocity）：指获得数据的速度； 可变性（Variability）：妨碍了处理和有效地管理数据的过程。 真实性（Veracity）：数据的质量 复杂性（Complexity）：数据量巨大，来源多渠道 价值（value）：合理运用大数据，以低成本创造高价值 大数据相关技术 数据采集： OLAP(联机分析处理)和数据挖掘的基础。ETL工具负责将分布的、异构的数据源进行抽取，抽取到中间层，进行清洗、转换、集成，（不过对于负责的逻辑处理不会这么干，用Spark或者其他的进行处理），最后放到数据仓库中存储，如Hive 数据存取： 关系型数据库、NoSQL(Not Only SQL,泛指非关系型数据库)，SQL等 基础架构： 云存储、分布式文件存储等 数据处理： 自然语言处理(Natural Language Processing, NLP) 数据分析： 假设检验、显著性检验、差异检验、差异分析、相关性分析、T检验、方差分析、卡方分析、偏相关性分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、预测和残差分析、岭回归、Logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、判别分析、对应分析、快速聚类和聚类法、对应分析、多元对应分析等 数据挖掘： 分类（Classification）、估计（Estimation）、预测（Prediction)、相关性分析或关联规则（Association）、复杂数据类型挖掘（Text、Web、图形图像、视频、音频等） 模型预测： 预测模型、机器学习、建模仿真 结果呈现： 云计算、标签云、关系图等 大数据带来的变革大数据小故事 &emsp;&emsp;最早关于大数据的故事发生在美国第二大的超市塔吉特百货（Target）。孕妇对于零售商来说是个含金量很高的顾客群体。但是他们一般会去专门的孕妇商店而不是在Target购买孕期用品。人们一提起Target，往往想到的都是清洁用品、袜子和手纸之类的日常生活用品，却忽视了Target有孕妇需要的一切。那么Target有什么办法可以把这部分细分顾客从孕妇产品专卖店的手里截留下来呢？ &emsp;&emsp;为此，Target的市场营销人员求助于Target的顾客数据分析部的高级经理Andrew Pole，要求他建立一个模型，在孕妇第2个妊娠期就把她们给确认出来。在美国出生记录是公开的，等孩子出生了，新生儿母亲就会被铺天盖地的产品优惠广告包围，那时候Target再行动就晚了，因此必须赶在孕妇第2个妊娠期行动起来。如果Target能够赶在所有零售商之前知道哪位顾客怀孕了，市场营销部门就可以早早的给他们发出量身定制的孕妇优惠广告，早早圈定宝贵的顾客资源。 &emsp;&emsp;可是怀孕是很私密的信息，如何能够准确地判断哪位顾客怀孕了呢？Andrew Pole想到了Target有一个迎婴聚会（baby shower）的登记表。Andrew Pole开始对这些登记表里的顾客的消费数据进行建模分析，不久就发现了许多非常有用的数据模式。比如模型发现，许多孕妇在第2个妊娠期的开始会买许多大包装的无香味护手霜；在怀孕的最初20周大量购买补充钙、镁、锌的善存片之类的保健品。最后Andrew Pole选出了25种典型商品的消费数据构建了“怀孕预测指数”，通过这个指数，Target能够在很小的误差范围内预测到顾客的怀孕情况，因此Target就能早早地把孕妇优惠广告寄发给顾客。 &emsp;&emsp;那么，顾客收到这样的广告会不会吓坏了呢？Target很聪明地避免了这种情况，它把孕妇用品的优惠广告夹杂在其他一大堆与怀孕不相关的商品优惠广告当中，这样顾客就不知道Target知道她怀孕了。百密一疏的是，Target的这种优惠广告间接地令一个蒙在鼓里的父亲意外发现他高中生的女儿怀孕了，此事甚至被《纽约时报》报道了，结果Target大数据的巨大威力轰动了全美。 &emsp;&emsp;根据Andrew Pole的大数据模型,Target制订了全新的广告营销方案，结果Target的孕期用品销售呈现了爆炸性的增长。Andrew Pole的大数据分析技术从孕妇这个细分顾客群开始向其他各种细分客户群推广，从Andrew Pole加入Target的2002年到2010年间，Target的销售额从440亿美元增长到了670亿美元。 &emsp;&emsp;我们可以想象的是，许多孕妇在浑然不觉的情况下成了Target常年的忠实拥泵，许多孕妇产品专卖店也在浑然不知的情况下破产。浑然不觉的背景里，大数据正在推动一股强劲的商业革命暗涌，商家们早晚要面对的一个问题就是：究竟是在浑然不觉中崛起，还是在浑然不觉中灭亡 其他故事 Google根据搜索关键字分析流感病毒H1N1 2008年金融危机 大数据初步学习路线 技术 工具 JAVA 面向对象的编程语言 Linux 类Unix操作系统 Hadoop生态圈 1、HDFS 解决存储问题存储极大数目的信息（terabytes or petabytes），将数据保存到大量的节点当中。支持很大单个文件。提供高可靠性，是指一个或多个节点故障，系统仍然可以继续工作提供数据快速访问 2、MapReduce 解决计算问题它有个特点就是不管多大的数据只要给它时间它就能把数据跑完，但是时间可能不是很快所以它叫数据的批处理 3、Yarn 资源调度器 4、ZooKeeper 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 5、Flume 数据采集工具 6、Hive 基于Hadoop的数据仓库工具 7、Hbase 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 8、Sqoop 数据传递工具，如将数据从关系型数据库导入Hive Scala 多范式编程语言、面向对象和函数式编程的特性 Spark 目前企业常用的批处理离线数据/实时计算引擎它是用来弥补基于MapReduce处理数据速度上的缺点，它很是流氓，直接将数据存在内存中 【注意】 MapReduce运行时也是需要将代码数据加载到内存中的，只不过Spark都是基于内存操作 Flink 目前最火的流式处理框架、既支持流处理、也支持批处理 Elasticsearch 大数据分布式弹性搜索引擎 大数据处理流程 大数据处理模型按照数据的三状态定义 水库里一平如镜的水—&gt;静止数据(data at rest) 水处理系统中上下翻滚的水—&gt;正在使用的数据(data in use) 汹涌而来的新水流—&gt;动态的水(data in motion) “快”说的是两层面 “动态数据” 来得快 “正在使用的数据” 处理得快 批处理 MapReduce 流处理 Spark Streaming 科普 根据国际数据公司（IDC）的《数据宇宙》报告显示：2008年全球数量为0.5ZB，2010年为1.2ZB，人类正式进入ZB时代。更为惊人的是，2020年以前全球数据量仍将保持每年40%多的高速增长，大约每两年就翻一倍，这与IT界的摩尔定律极为相似，姑且称之为“大数据爆炸定律”。","link":"/post/2260.html"},{"title":"Java动态代理","text":"概述 Hadoop远程过程调用实现使用Java动态代理和新输入1输出系统(NewInput/Output,NIO) Java动态代理类位于java.lang.reflect包下，主要包括java.lang rlfct.Proxy和java.lang.reflect.InvocationHandler 代理对象两大任务 创建代理接口 实现由java.lang,reflect.Proxy完成 调用转发通过java.lang.reflect.InvocationHandler的实例完成 代理接口的创建 在Java中，代理对象往往实现和目标对象-致的接口，并作为目标对象的代替，接收对象用户(Client) 的调用，并将全部或部分调用转发给目标对象 代理也是拥有和目标对象一样的权利的‘人’， 简单的说， 代理可以越俎代庖。实际上他是调用转发，将这个任务交给有权利实施的人 代理时序图 java.lang.eflct.Proxy提供了用于创建动态代理类和对象的静态方法。也就是说，通过java.lang.reflect.Proxy可以动态地创建某个接口实现 java.lang.eflct.Proxy比较重要的方法 public static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces) 获得代理类的java.lang.Class对象。该代理类将定义在指定的类加载器（参数loader）中，并将实现参数interfaces指定所有接口 注意： 这个类只创建一次，如果再次传入相同的loader和interfaces给newProxyInstance()方法，获得的也只是第一次调用创建的那个java.lang.Class对象 通过Proxy.getProxyClass()获得的代理类都包含一个构造函数,该构造函数需要一个java.lang.reflect.InvocationHandler 的实例 如何获取Proxy对象？，请看以下代码 123456Class clss = Proxy.getProxyClass(loader, interfaces);//获得构造函数Constructor constructor = clss.getConstructor(new Class[]{InvocationHandler.class});//创建代理对象Object proxy = constructor.newInstance(new Object[]{invocationHandler}); public static boolean isProxyClass(Class&lt;?&gt; cl) 判断java.lang.Class对象是否是代理类 public static InvocationHandler getInvocationHandler(Object proxy) throws IllegalArgumentException 获取代理实例对应的调用处理程序（即构建代理传入的InvocationHandler实例） 调用转发 InvocationHandler调用实例也叫调用句柄实例 12345678910111213141516/** * * @param proxy 代理对象本身 * @param method 用户调用的代理对象上的方法 * @param args 传递给该方法的参数 * @return 代理对象方法调用结果 * @throws Throwable * * java.lang.reflect.Method * 它提供了关于类或接口上某个方法以及如何访问该方法的信息 * 其中的invoke方法可以在指定对象上调用对象的方法 */@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return null;} Method类中的invoke方法声明如下 123public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException 假设目标对象为target，实现转发代码如下 123public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return method.invoke(target, args);s} Java动态代理实例实例类图 代码 12345678910111213/**DPStatus.java**/public class DPStatus { String name; public DPStatus(String name) { this.name = name; } @Override public String toString() { return \"Hello, \" + name + \"!\"; }} 12345678/** * PDQueryStatus.java * * 动态代理机制与java远程调用不同，不需要继承什么接口 */public interface PDQueryStatus { public DPStatus getStatus();} 1234567891011/** * DPQueryStatusImpl.java * * PDQueryStatus的简单实现 */public class DPQueryStatusImpl implements PDQueryStatus{ @Override public DPStatus getStatus() { return new DPStatus(\"Bitty\"); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.text.MessageFormat;import java.util.Arrays;/** * DPInvocationHandler.java * * 转发由该类实现 * 该类中最重要的一部分是invoke方法， * * 如果代理对象调用某个方法时，DPInvocationHandler.invoke将会被调用， * 传入invoke的method中，也就是说，PDQueryStatus.getStatus()就是是method对象， * 而getStatus()没有参数 * * 在这里完成代理转发的是 * Object res = method.invoke(dpqs, args); * 当method是PDQueryStatus.getStatus()时，其效果就相当于 * res = pdqs.getStatus() * * * */public class DPInvocationHandler implements InvocationHandler { //目标对象 private DPQueryStatusImpl dpqs; public DPInvocationHandler(DPQueryStatusImpl dpqs) { this.dpqs = dpqs; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //实现附加功能，在控制台输出调用参数的String表示 String msg = MessageFormat.format(\"Calling method\", method.getName(), Arrays.toString(args)); System.out.println(msg); //调用转发 Object res = method.invoke(dpqs, args); //其他附加功能 return res; }} 1234567891011121314151617/** * DPMain.java * * 在这个类中使用create方法创建代理 */public class DPMain { public static PDQueryStatus create(DPQueryStatusImpl dpqs){ //newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) //参数准备 Class&lt;?&gt;[] interfaces = new Class[]{PDQueryStatus.class}; DPInvocationHandler handler = new DPInvocationHandler(dpqs); return (PDQueryStatus) Proxy.newProxyInstance(dpqs.getClass().getClassLoader(), interfaces, handler); }} 123456789public class Demo { public static void main(String[] args) throws Exception { PDQueryStatus pdqs = DPMain.create(new DPQueryStatusImpl()); System.out.println(pdqs.getStatus()); }}","link":"/post/e63a7813.html"},{"title":"Java远程调用","text":"概述 Java远程方法调用(Remote Method Invocation, RMI)是Java的一个核心API和类库,允许一个Java虚拟机上运行的Java程序调用不同虚拟机上运行的对象中的方法，即使这两个虚拟机运行于物理隔离的不同主机上。在某种程度上，RMI可以看成RPC的Java升级版。 和RPC一样,存在服务端和客户端典型服务器端应用程序 创建多个远程对象（Remote Object）,使这些对象能被客户端引用，并等待客户端调用远程对象的方法典型的客户端程序 从服务器获得一个或多个远程对象的引用，然后调用远程对象的方法 Java远程方法调用依赖于Java序列化，调用远程方法传的参数、返回值都是序列化对象 远程方法调用实例123456789/**RMIQueryStatus.java**/import java.rmi.Remote;import java.rmi.RemoteException;public interface RMIQueryStatus extends Remote { String getStatus(String name) throws RemoteException;} RMIQueryStatus的定义要求 远程接口必须声明为public，否则客户端试着装载“实现远程接口”的远端对象时，会收到错误的消息。 远程接口必须继承自java.rmi.Remote。 远程接口中的每-一个方法，除了自定义的异常之外，必须将java.rmi.RemoteException声明于其throws子句中。 在远程方法声明中，作为参数或者返回值的远程对象，或者包含在其他非远程对象中的远程对象，必须声明为其对应的远程接口，而不是实际的实现类。(这点在String类中并没有体现) RMIQueryStatus的实现类 123456789101112/**RMIQueryStatusImp.java**/import java.rmi.RemoteException;import java.rmi.server.UnicastRemoteObject;public class RMIQueryStatusImp extends UnicastRemoteObject implements RMIQueryStatus { protected RMIQueryStatusImp() throws RemoteException { } public String getStatus(String name) throws RemoteException { return \"I'm \" + name + \".\"; }} 客户端和服务端的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/**RMIDemoServer**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.RemoteException;import java.rmi.registry.LocateRegistry;/** * @author lyhcc */public class RMIDemoServer { public static void main(String[] args) throws RemoteException, MalformedURLException { //1. 创建RMIQueryStatus对象 RMIQueryStatusImp queryService = new RMIQueryStatusImp(); //2. 设置服务端口 LocateRegistry.createRegistry(12090); //3. 绑定远端对象名 Naming.rebind(\"rmi://localhost:12090/queryTest\", queryService); System.out.println(\"Server is running!\"); }}/**客户端的代码**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.NotBoundException;import java.rmi.RemoteException;public class RMIDemoClient { public static void main(String[] args) throws RemoteException, NotBoundException, MalformedURLException { //1. 创建RMIQueryStatusImp对象 RMIQueryStatus queryStatus = (RMIQueryStatus) Naming.lookup(\"rmi://localhost:12090/queryTest\"); //2. 调用远程方法 String status = queryStatus.getStatus(\"KiKi\"); System.out.println(status); }} 先运行服务端然后运行客户端查看结果 Java远程调用实例类图 客户端RMIQueryStatusClient的工作依赖于RMI存根(Stub)，这个存根是通过Java的代理机制 java.lang.reflect.Proxy","link":"/post/55cde24e.html"},{"title":"Hadoop介绍","text":"起源 Google 在大数据方面的三大论文 （谷歌三宝）在github大的当前目录下三宝的介绍 Hadoop 三大发行版本 Apache、Cloudera、Hortonworks Apache版本最原始、最基础：适合零基础 大公司在用 Cloudera Cloudera’s DistributionIncluding Apache Hadoop 简称CDH中小型公司用、简单方便、自带可视化 Hortonworks 文档较好 注：Cloudera 和Hortonworks 在2018年10月，国庆期间宣布合并硬件要求内存 最大支持内存查询：win + R输入 wmic memphysical get maxcapacity计算 MaxCapacity/1024/1024GB 硬盘:500G+","link":"/post/8a9ad9ba.html"},{"title":"Hadoop通信机制和内部协议之RPC","text":"Hadoop RPCRPC简介 简要地说，RPC就是允许程序调用位于其他机器上的过程(也可以是同一台机器的不同进程)。RPC调用过程是透明的 传统过程调用：传统的过程调用中，主程序将参数压人栈内并调用过程，这时候主程序停止执行并开始执行相应的过程。被调用的过程从栈中获取参数，然后执行过程函数;执行完毕后，将返回参数入栈(或者保存在寄存器里)，并将控制权交还给调用方。调用方获取返回参数，并继续执行。 而RPC调用是进程间的过程调用 RPC模型 通行模块： 请求-响应 Stub程序： 用于保证RPC的透明性。在客户端，不在本地调用，而是将请求信息通过网络模块发送给法服务器端，服务器接收后进行解码。服务器中，Stub程序依次进行 解码（请求的参数）、调用相应的服务过程、编码返回结果等处理 调度程序： 调度来自通行模块的请求信息，根据其中标识选一个Stub程序运行 客户程序： 请求发出者 服务过程： 请求接收者 一个RPC的旅游： 客户端以本地调用方式产生本地Stub程序 该Stub程序将函数调用信息按照网络通信模块的要求封装成消息包，并交给通信模块发送到远程服务器端。 远程服务器端接收此消息后，将此消息发送给相应的Stub程序 Stub程序拆封消息，形成被调过程要求的形式，并调用对应函数 服务端执行被调用函数，并将结果返回给Stub程序 Stub程序将此结果封装成消息，通过网络通信模块逐级地传送给客户程序。 RPC特性 透明性 调用过程就像本地调用，察觉不到它的经历 高性能 ：Hadoop各个系统（如HDFS、MapReduce、YARN等）均采用了Master/Slave结构，其中，Master实际上是一个RPC server，它负责响应集群中所有Slave发送的服务请求。RPC Server性能要求高，为的是能够让多个客户端并发方位 易用性/可控性 Hadoop系统不采用Java内嵌的RPC（RMI,Remote Method Invocation）框架的主要原因是RPC是Hadoop底层核心模块之一，需要满足易用性、高性能、轻量级等特性 RPC例子执行过程： CalculateClient对象本地调用产生Stub程序 经通信模块上传至服务器CalculateServer对象，在创建Server时设置了协议和业务逻辑（服务过程），处理过后根据上述RPC过程返回 客户端接收后打印到日志中 先定义一些常量 这里不需要太多的在意，直接使用在代码里面也行，在大的项目中为了使程序易于修改而这样设置 1234567891011/** * 静态变量声明类 */public interface Constants { public interface VersionID { public static final long RPC_VERSION = 7788L; } public static final String RPC_HOST = &quot;127.0.0.1&quot;; public static final int RPC_PORT = 8888;} 定义一个Service接口，协议类12345678910111213import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.ipc.ProtocolInfo;@ProtocolInfo(protocolName = &quot;&quot;, protocolVersion = Constants.VersionID.RPC_VERSION)public interface CalculateService { //真实业务逻辑，加减法， public IntWritable add(IntWritable a, IntWritable b); public IntWritable sub(IntWritable a, IntWritable b); public Text echo(Text mt);} @ProtocolInfo(protocolName = “”, protocolVersion = Constants.VersionID.RPC_VERSION) 没有这句就不能将该类设置为协议，不过也可以通过继承VersionProtocol接口 Service接口的实现类1234567891011121314151617181920212223242526272829303132333435import java.io.IOException;public class CalculateServiceImpl implements CalculateService { /** * 该方法没有也行 * */ public ProtocolSignature getProtocolSignature(String arg0, long arg1, int arg2) throws IOException{ return this.getProtocolSignature(arg0, arg1, arg2); } /** * 校验hadoop RFC版本号 * @param arg0 * @param arg1 * @return */ public long getProtocolVersion(String arg0, long arg1) throws IOException { return Constants.VersionID.RPC_VERSION; } @Override public IntWritable add(IntWritable a, IntWritable b) { return new IntWritable(a.get() + b.get()); } @Override public IntWritable sub(IntWritable a, IntWritable b) { return new IntWritable(a.get() - b.get()); } @Override public Text echo(Text mt) { return mt; }} Server和Client类123456789101112131415161718192021222324252627282930import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;public class CalculateServer { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { try { //构造Server,并设置协议接口，主机、端口，真实业务逻辑 RPC.Server server = new RPC.Builder(new Configuration()) .setProtocol(CalculateService.class) .setBindAddress(Constants.RPC_HOST) .setPort(Constants.RPC_PORT) .setInstance(new CalculateServiceImpl()) .build(); //启动Server server.start(); LOG.info(&quot;Server has Started!&quot;); } catch (IOException e) { LOG.error(&quot;Server has Error&quot;); } }} 123456789101112131415161718192021222324252627282930313233343536import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.net.InetSocketAddress;public class CalculateClient { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { //格式化IP和端口 InetSocketAddress addr = new InetSocketAddress(Constants.RPC_HOST, Constants.RPC_PORT); //校验Hadoop RPC版本号 long protocolVersion = RPC.getProtocolVersion(CalculateService.class); try { //获取Server连接 CalculateService proxy = RPC.getProxy(CalculateService.class, protocolVersion, addr, new Configuration()); IntWritable add = proxy.add(new IntWritable(1), new IntWritable(2)); IntWritable sub = proxy.add(new IntWritable(3), new IntWritable(2)); LOG.info(&quot;1+2 = &quot; + add); LOG.info(&quot;3-2 = &quot; + sub); } catch (IOException e) { LOG.error(&quot;Client has error!&quot;); } }} 注意： 查看本程序运行结果需要一个日志文件，如果不想加，把LOG的相关语句换为打印输出就行在resource文件夹下创建 log4j.properties 12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 客户端运行结果12342019-10-31 18:59:28,499 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-10-31 18:59:28,619 WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 1+2 = 3 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 3-2 = 5 其他开源RPC架构 Java RMI Apache Thrift Google Protocol Buffer","link":"/post/56192.html"},{"title":"Hadoop 压缩","text":"压缩 压缩是指将数据转换为比原来的格式占用空间更小的格式来存储，以达到减小存储空间解压是压缩的反过程 Hadoop文件切片 Hadoop MapReduce是通过划分切片来处理得，这样就使得支持分割的压缩格式更适合Hadoop 针对ss.txt文件大小为300M 计算公式computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M（Hadoop 1.x中块大小为64M） 默认情况下，切片大小=blocksize 开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） 注意：切片主要由这几个值来运算决定 12mapreduce.input.fileinputformat.split.minsize=1 默认值为1mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认Long.MAXValue 因此，默认情况下，切片大小=blocksize。 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 Hadoop压缩 Hadoop作为一个叫通用的海量数据处理平台，在压缩方面主要考虑压缩速度和压缩的可分割性 小提示： 使用gzip压缩文件时 -9表示空间优先，也就是先考虑压缩空间的减小 -1表示时间优先，也就是压缩速度要快 Hadoop支持的压缩格式 压缩格式 工具 算法 扩展名 多文件 可分割性 换成压缩格式后，原来的程序是否需要修改 DEFLATE 无 DEFLATE .deflate 不 不 和文本处理一样，不需要修改 GZIP gzip DEFLATE .gzp 不 不 和文本处理一样，不需要修改 ZIP zip DEFLATE .zip 是 是，在文件范围内 BZIP2 bzip2 BZIP2 .bz2 不 是 和文本处理一样，不需要修改 LZO lzop LZO .lzo 不 是 需要建索引，还需要指定输入格式 压缩算法及其编码/解码器 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 性能压缩比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 压缩格式选择Gzip压缩 优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 Bzip2压缩 优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native(java和c互操作的API接口)；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 Lzo压缩 优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。 Snappy压缩 优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。 Hadoop 压缩案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Demo { public static void main(String[] args) throws IOException, ClassNotFoundException { //压缩为bz2格式 compress(\"org.apache.hadoop.io.compress.BZip2Codec\"); } private static void compress(String method) throws ClassNotFoundException, IOException { //1. 获取resources下的资源文件流 InputStream in = Demo.class.getClassLoader().getResourceAsStream(\"properties.xml\"); //2. 通过Java反射机制创建对应得编码名称 Class&lt;?&gt; codeClass = Class.forName(method); //3. 通过编码名称找对应得编码/解码器 Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codeClass, conf); //4. 指定压缩后的文件,codec.getDefaultExtension()获得相应的扩展名 File fileOut = new File(System.currentTimeMillis() + codec.getDefaultExtension()); //如果文件存在，删除，否则什么也不做 fileOut.delete(); //5. 创建输出流 FileOutputStream out = new FileOutputStream(fileOut); //6. 通过编码/解码器创建对应得输出流 CompressionOutputStream cout = codec.createOutputStream(out); //7. 压缩输出 /** * in 输入流 * cout 压缩输出流 * 1024 缓冲大小 * false 不关闭相应流，true则关闭 */ IOUtils.copyBytes(in,cout,4096,false); in.close(); cout.close(); } /** * 解压缩 * @param file * @throws IOException */ private static void decompress(File file) throws IOException { Configuration conf = new Configuration(); CompressionCodecFactory factory = new CompressionCodecFactory(conf); //通过扩展名获得编码/解码器 CompressionCodec codec = factory.getCodec(new Path(file.getName())); //通过编码解码器获得输入流 CompressionInputStream in = codec.createInputStream(new FileInputStream(file)); IOUtils.copyBytes(in, System.out, 4096, true); }} 说明：snappy压缩格式在Windows运行失败，不过打包放到集群里面是可以的，前提是你的hadoop集群支持snappy压缩格式","link":"/post/420bf138.html"},{"title":"Hadoop通信机制和内部协议之协议","text":"概述MapReduce核心协议 名称 描述 ClientProtocol 继承于Version基类，查看作业情况监控当前集群等 RefreshUserMappingProtocol 刷新用户到用户组映射关系到超级用户代理组列表 RefreshAuthorizationPolicyProtocol 刷新HDFS和MapReduce服务几倍访问控制列表 ResourceManagerAdministrationProtocol 继承于GetUserMappingProtocol基类，刷新队列列表，节点列表 ## ClientProtocol通信协议 ClientProtocol协议是JobClient和JobTracker之间进行交流的枢纽。JobClient 可以使用该协议中的函数来提交-一个作业(Job) 并执行，以了解当前系统的状态 提交作业协议中JobClient通过Hadoop RPC的submitjob()函数提交作业(Job)，函数所包含的参数有作业ID (JobID)，然后JobClient通过getNewJoblD0函数为作业(Job) 获得一个唯一的ID。 操作作业当用户提交作业(Job) 后，可以通过调用函数来控制该作业的执行流程，如设置提交作业的优先级(setlobPriority()函数) 、停止一个作业(killJob()函数) 、停止一个任务(illTask()函数)。 查看状态从实现源代码来看，该通信协议还提供了一系列函数来 查看状态，如查看集群当前状态(getClusterMetrics()函数)、查看当前任务状态(getJobTrackerStatus()函数) 、获取所有任务(getllobs()函数)等。 RefreshUserMappingProtocol RefreshU serMappingsProtocol 协议用于更新 HDFS 和 MapReduce 级别的用户到用户组映射关系及超级用户代理组列表 refreshUserToGroupsMappings() 函数和refreshSuperUserGroupsConfiguration()函数来实现，这两个函数均是通过调用Hadoop RPC来完成具体的逻辑。 RefreshAuthorizationPolicyProtocol RefreshAuthorizationPol icyProtocol 协议用于刷新当前使用的授权策略 通过调用 Hadoop RPC 远程调用 refreshServiceAcl（）函数，实现基于 HDFS 和MapReduce 级别的授权策略 ResourceManagerAdministrationProtocol ResourceManagerAdministrationProtocol 协议用于更新队列列表、节点 列表 、节点资源等 该协议继承于 GetUserMappingsProtocol 基类 ，通过 Hadoop RPC 远程调用来实现节点更新、资源更新 、添加标签等操作 说明：在IDE中导入hadoop源码加载进去后，按Ctrl+鼠标左键进入即可查看源码","link":"/post/13550.html"},{"title":"hadoop序列化","text":"序列化介绍 序列化是一种将对象的状态信息转化成可以存储或者传输的过程，与之相反的为反序列化不是某一种编程语言所独有的特性序列化的用途 作为一种持久化格式。对象序列化后存盘 作为一种通信的数据格式。如虚拟机之间通信 作为一种拷贝、克隆机制。放缓存 Java序列化 Java通过实现Serializable接口Java序列化后放入对象，通过对象流进行IO操作，ObjectInputStream/ObjectOutputStream 1234567import java. io.Serializable ;／＊＊定义一个可以序列化的 App 信息类. */public class Appinfo implements Serializable{ ／／序列化标识 private static final long serialVersionUID = 11 ;} Hadoop 不使用Java序列化原因 Java 自带的序列化机制占用内存空间大，额外的开销会导致速度降低，Hadoop对序列化的要求较高，需要保证序列化速度快、体积小、占用带宽低等特性 Hadoop 序列化机制是将对象序列化到流中，而 Java 序列化机制是不断创建新对象，对于MapReduce应用来说，不能重用对象 Java序列化在反序列化时，有可能需要访问前一个数据，这将导致数据无法分割来通过MapReduce来处理 Hadoop 序列化 在 Hadoop 序列化机制中，org.apache.hadoop.io包中定义了大量的可序列化对象 Hadoop 序列化机制通过调用write方法（它带有一个类型为DataOutput的参数），将对象序列化到流中 Hadoop 反序列化通过对象的readFields从流中读取数据 Hadoop序列化机制的特征 对于处理大数据的Hadoop平台，其序列化需要具备以下特征 紧凑。这样可以充分利用Hadoop集群的资源，hadoop集群中最稀缺的是资源 快速。进程通信时会大量使用序列化机制，因此需要减少序列化开销 可扩展性。为适应发展，序列化机制也需要支持这些升级和变化 互操作。支持不同语言开发 Hadoop Writable机制 Hadoop序列化都必须实现该接口 均实现Wriable接口的两个函数， 12(1) write：将对象写入字节流：(2) readFields：从字节流中解析出对象。例子 123456789101112131415161718192021222324252627282930313233343536373839/** * BlockWritable有三个对象， * write方法将三个对象写到流中 * readFields从流中读出三个对象 */public class BlockWritable implements Writable { private long blockId; private long numBytes; private long generationStamp; /** * 输出序列化对象到流中 * @param out * @throws IOException */ @Override public void write(DataOutput out) throws IOException { out.writeLong(this.blockId); out.writeLong(this.numBytes); out.writeLong(this.generationStamp); } /** * 从流中读取序列化对象 * 为了效率，尽可能复用现有对象 * @param in 从该流中读取数据 * @throws IOException */ @Override public void readFields(DataInput in) throws IOException { this.blockId = in.readLong(); this.numBytes = in.readLong(); this.generationStamp = in.readLong(); if (this.numBytes &lt; 0L) { throw new IOException(&quot;Unexpected block size: &quot; + this.numBytes); } }} Hadoop序列化的其它几个接口 WritableComparable RawComparator RawComparator允许执行者 比较 流中读取的未被反序列化为对象的 记录，从而省去创建对象所带来的开销 1234567891011/** * * @param var1 字节数组1 * @param var2 字节数组1的开始位置 * @param var3 字节数组1的记录长度 * @param var4 字节数组2 * @param var5 字节数组2的开始位置 * @param var6 字节数组2的记录长度 * @return */int compare(byte[] var1, int var2, int var3, byte[] var4, int var5, int var6); WritableComparator 在RawComparator中WritableComparator是个辅助类，实现了RawComparator接口 以DoubleWritable为例 1234567891011public static class Comparator extends WritableComparator { public Comparator() { super(DoubleWritable.class); } public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) { double thisValue = readDouble(b1, s1); double thatValue = readDouble(b2, s2); return thisValue &lt; thatValue ? -1 : (thisValue == thatValue ? 0 : 1); } } WritableComparator是RawComparator对WritableComparable类的一一个通用实现。提供两个主要功能。首先，提供了一个RawComparator的compare()默认实现，该实现从数据流中反序列化要进行比较的对象，然后调用对象的compare()方法进行比较(这些对象都是Comparable的)。其次，它充当了RawComparator实例的一个工厂方法,通过DoubleWritable获得RawComparator的代码如下 1RawComparator&lt;DoubleWritable&gt; comparator = WritableComparator.get(DoubleWritable.class); RawComparator和WritableComparable的类图 Hadoop 序列化的类java基本类型的封装 说明： 这些类实现了WritableComparable接口 VIntWritable和VLongWritable是只可变长 可变长的格式更空间 VIntWritable可用VLongWritable读入 变长整型分析 writeVLong ()方法实现了对整型数值的变长编码，它的编码规则如下:&emsp;&emsp;如果输入的整数大于或等于-112同时小于或等于127，那么编码需要1字节:否则，序列化结果的第一个字节，保存了输入整数的符号和后续编码的字节数。符号和后续字节数依据下面的编码规则(又一个规则): 如果是正数，则编码值范围落在-113和-120间(闭区间)，后续字节数可以通过-(v+112)计算。 如果是负数，则编码值范围落在-121和-128间(闭区间)，后续字节数可以通过-(v+120)计算。 后续编码将高位在前，写入输入的整数(除去前面全0字节)。代码如下: 1234567891011121314151617181920212223242526272829public static void writeVInt(DataOutput stream, int i) throws IOException { writeVLong(stream, (long)i); } public static void writeVLong(DataOutput stream, long i) throws IOException { if (i &gt;= -112L &amp;&amp; i &lt;= 127L) { stream.writeByte((byte)((int)i)); } else { int len = -112; if (i &lt; 0L) { i = ~i; len = -120; } for(long tmp = i; tmp != 0L; --len) { tmp &gt;&gt;= 8; } stream.writeByte((byte)len); len = len &lt; -120 ? -(len + 120) : -(len + 112); //后续编码 for(int idx = len; idx != 0; --idx) { int shiftbits = (idx - 1) * 8; long mask = 255L &lt;&lt; shiftbits; stream.writeByte((byte)((int)((i &amp; mask) &gt;&gt; shiftbits))); } } } ObjectWritable 针对Java基本类型、字符串、枚举、Writable、空值、Writable的其 他子类,ObjectWritable提供了一个封装，适用于字段需要使用多种类型。ObjectWritable 可应用于Hadoop远程过程调用中参数的序列化和反序列化; ObjectWritable 的另一个典型应用是在需要序列化不同类型的对象到某-个字段，如在一个SequenceFile 的值中保存不同类型的对象( 如LongWritable值或Text值)时，可以将该值声明为ObjectWritable。 ObjectWritable的实现比较冗长，需要根据可能被封装在ObjectWritable中的各种对象进行不同的处理。ObjectWritable 有三个成员变量，包括被封装的对象实例instance、该对象运行时类的Class对象和Configuration对象。 123private Class declaredClass;private Object instance;private Configuration conf; ObjectWritable的write 方法调用的是静态方法ObjectWritable.writeObject()，该方法可以往DataOutput接口中写入各种Java对象。 writeObject()方法先输出对象的类名(通过对象对应的Class对象的getName()方法获得)， 1UTF8.writeString(out, declaredClass.getName()); 然后根据传入对象的类型，分情况系列化对象到输出流中，也就是说，对象通过该方法输出对象的类名，对象序列化结果对到输出流中。在ObjectWritable. writeObject(的逻辑中，需要分别处理null Java 数组、字符串String、Java 基本类型、枚举和Writable的子类6种情况，由于类的继承，处理Writable时，序列化的结果包含对象类名，对象实际类名和对象序列化结果三部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public void write(DataOutput out) throws IOException { writeObject(out, this.instance, this.declaredClass, this.conf); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf) throws IOException { writeObject(out, instance, declaredClass, conf, false); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf, boolean allowCompactArrays) throws IOException { if (instance == null) { instance = new ObjectWritable.NullInstance(declaredClass, conf); declaredClass = Writable.class; } if (allowCompactArrays &amp;&amp; declaredClass.isArray() &amp;&amp; instance.getClass().getName().equals(declaredClass.getName()) &amp;&amp; instance.getClass().getComponentType().isPrimitive()) { instance = new Internal(instance); declaredClass = Internal.class; } UTF8.writeString(out, declaredClass.getName()); /****************此处****************/ if (declaredClass.isArray()) { int length = Array.getLength(instance); out.writeInt(length); for(int i = 0; i &lt; length; ++i) { writeObject(out, Array.get(instance, i), declaredClass.getComponentType(), conf, allowCompactArrays); } } else if (declaredClass == Internal.class) { ((Internal)instance).write(out); } else if (declaredClass == String.class) { UTF8.writeString(out, (String)instance); } else if (declaredClass.isPrimitive()) { if (declaredClass == Boolean.TYPE) { out.writeBoolean((Boolean)instance); } else if (declaredClass == Character.TYPE) { out.writeChar((Character)instance); } else if (declaredClass == Byte.TYPE) { out.writeByte((Byte)instance); } else if (declaredClass == Short.TYPE) { out.writeShort((Short)instance); } else if (declaredClass == Integer.TYPE) { out.writeInt((Integer)instance); } else if (declaredClass == Long.TYPE) { out.writeLong((Long)instance); } else if (declaredClass == Float.TYPE) { out.writeFloat((Float)instance); } else if (declaredClass == Double.TYPE) { out.writeDouble((Double)instance); } else if (declaredClass != Void.TYPE) { throw new IllegalArgumentException(\"Not a primitive: \" + declaredClass); } } else if (declaredClass.isEnum()) { UTF8.writeString(out, ((Enum)instance).name()); } else if (Writable.class.isAssignableFrom(declaredClass)) { UTF8.writeString(out, instance.getClass().getName()); ((Writable)instance).write(out); } else { if (!Message.class.isAssignableFrom(declaredClass)) { throw new IOException(\"Can't write: \" + instance + \" as \" + declaredClass); } ((Message)instance).writeDelimitedTo(DataOutputOutputStream.constructOutputStream(out)); } } 和输出对应，ObjectWritable 的readFields()方法调用的是静态方法ObjectWritable.readObject()，该方法的实现和writeObject()类似，唯一值得研究的是Writable对象处理部分，readObject ()方法依赖于WritableFactories类。WritableFactories 类允许非公有的Writable子类定义一一个对象工厂，由该工厂创建Writable对象，如在上面的readObject()代码中，通过WritableFactories的静态方法newInstance()，可以创建类型为instanceClass的Writable子对象。具体查看org.apache.hadoop.io.WritableFactories类 注：ObjectWritable它比较浪费资源，可以使用静态数组来记录数据类型以提高效率 Hadoop序列化优势 1231. 减少垃圾回收：从流中反序列化数据到当前对象，重复使用当前对象，减少了垃圾回收GC ;2. 减少网络流量 ： 序列化和反序列化对象类型不变 ，因此可以只保存必要的数据来减少网络流量；3. 提升 I/O 效率 ： 由于序列化和反序列化的数据量减少了，配合Hadoop压缩机制，可以提升I/O效率。","link":"/post/20979.html"},{"title":"配置文件","text":"Windows操作系统配置文件 配置设置文件（INI）文件是windows操作系统中的一种特殊的ASCII文件，以ini为文件扩展名,作为它的主要文件配置文件标准。该文件也被称为初始化文件initialization file和概要文件profile。应用程序可以拥有自己的配置文件，存储应用设置信息，也可以访问windows的基本系统配置文件win.ini中存储的配置信息 INI配置信息分为两部分 节，节标题放在方括号中, [section] 项，一个等式，key=value 1234567;注释;节 [section] ;参数（键=值） name=value INI文件片段 1234[0x0419]1100=Ошибка инициализации программы установки1101=%s1102=%1 Идет подготовка к запуску мастера %2, выполняющего установку программы. Ждите. Windows 提供的API 12345678910111213141516DWORD GetPrivateProfileString( LPCTSTR lpAppName, // If this parameter is NULL, the GetPrivateProfileString function copies all section names in the file to the supplied buffer. LPCTSTR lpKeyName, // If this parameter is NULL, all key names in the section specified by the lpAppNameparameter are copied to the buffer specified by the lpReturnedString parameter. LPCTSTR lpDefault, // If the lpKeyName key cannot be found in the initialization file, GetPrivateProfileString copies the default string to the lpReturnedString buffer. LPTSTR lpReturnedString, // destination buffer DWORD nSize, // size of destination buffer LPCTSTR lpFileName // The name of the initialization file);UINT GetPrivateProfileInt( LPCTSTR lpAppName, //节 LPCTSTR lpKeyName,//项 INT nDefault, //The default value to return if the key name cannot be found in the initialization file. LPCTSTR lpFileName //INI文件名); Java配置文件 JDK提供了java.util.Properties类，用于处理简单的配置文件。Properties继承自Hashtable相对于INI文件，Properties处理得配合文件格式非常简单 Properties的使用 非XML文件格式12345678//通过指定的键搜索属性public String getProperty(String key)//功能同上，参数defaultValue提供了默认值public String getProperty(String key, String defaultValue)//最终调用Hashtable 的方法putpublic synchronized object setProperty (String key, String value) Properties中的属性通过load)方法加载，该方法从输入流中读取键-值对，而store()方法法则将Properties表中的属性列表写入输出流。使用输入流和输出流，Properties对象但可以保存在文件中，而且还可以保存在其他支持流的系统中，如Web服务器。 123456789101112131415161718192021222324/** * log4j.properties内容如下 * log4j.rootLogger=INFO, stdout * log4j.appender.stdout=org.apache.log4j.ConsoleAppender * log4j.appender.stdout.layout=org.apache.log4j.PatternLayout * log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n * log4j.appender.logfile=org.apache.log4j.FileAppender * log4j.appender.logfile.File=target/spring.log * log4j.appender.logfile.layout=org.apache.log4j.PatternLayout * log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n * @param args * @throws IOException */ public static void main(String[] args) throws IOException { Properties properties = new Properties(); //获取resources目录下的文件流 InputStream stream = MyConfiguration.class.getClassLoader().getResourceAsStream(\"log4j.properties\"); //加载文件获取并获取配合信息 properties.load(stream); String property = properties.getProperty(\"log4j.rootLogger\"); System.out.println(property); /*输出： INFO, stdout */ } Java 1.5之后支持XML配置文件,Properties中的数据也可以以XML格式保存，对应的加载和写出方法是loadFromXML()和storeToXML() storeToXML() 12345Properties props = new Properties();props.setProperty(\"Length\", \"100\");props.setProperty(\"Width\", \"50\")FileOutputStream fos = new FileOutputStream(\"properties.xml\");props.storeToXML(fos, null); loadFromXML() 1234567Properties props = new Properties();InputStream in = MyConfiguaration2.class.getClassLoader().getResourceAsStream(\"properties.xml\");props.loadFromXML(in)String length = props.getProperty(\"Length\");System.out.println(length); xml有指定格式 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;entry key=\"Width\"&gt;50&lt;/entry&gt; &lt;entry key=\"Length\"&gt;100&lt;/entry&gt;&lt;/properties&gt; java.util.Properties提供的能力有限，其他配置信息读写方法，如Apache Jakarta Commons工具集提供的Commons Configuration 【说明】：上面的MyConfiguration都是自定义的类,并且这些配置文件都在resources文件夹下 Hadoop Configuration详解 Hadoop没有使用java.util.Properties管理配置文件，也没有使用Apache JakartaCommons Configuration 管理配置文件，而是使用了一套独有的配置文件管理系统，并提供自己的API，即使用org.apache.hadoop.conf.Configuration处理配置信息。 hadoop 配置文件格式12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.9.2/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hadoop 配置文件参数说明配置参数类型说明 参数名 String 参数值 boolean int long float，也可以是其他类型 参数说明 根元素 configuration configuration下的property元素 property下 name 参数名 value 参数值 description 参数描述 final 相当于java的final关键字，在资源合并时可以防止配置项被覆盖 合并资源 合并资源是是指将多个配置合并，产生一个配置文件，如core-site.xml和core-defualt.xml通过addResources()方法合并 1234567Configuration conf = new Configuration();//加载resources文件夹内容ClassLoader classLoader = HadoopConfiguaration.class.getClassLoader//添加合并资源conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-site.xml\")));conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-default.xml\")System.out.println(conf.get(\"fs.defaultFS\")); 【注意】如果第一个配置中存在final，则会以下出现警告,并且值不发生更改 1232019-11-12 17:31:34,548 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@cac736f:an attempt to override final parameter: fs.defaultFS; Ignoring. 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@1d7acb34:an attempt to override final parameter: fs.defaultFS; Ignoring. 测试用的core-default.xml文件 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://vmaster:8200&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.3.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;123&lt;/name&gt; &lt;value&gt;wer&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Configuation类的一般过程 构造Configuration对象 添加需要加载的资源 addResource()方法 然后通过set/get 方法访问/设置配置项 【说明】资源会在第一次使用时自动加载到对象中 Configuration类介绍类图 说明类图中，Configuration有7个非静态成员 布尔变量quietmode，用来设置加载配置的模式。如果quietmode为true (默认值)，则在加载解析配置文件的过程中，不输出日志信息。quietmode只是-一个方便开发人员调试的变量。 数组resources保存了所有通过addResource()方法添加Configuration对象的资源 Configuration.addResource()有如下4种形式: 1234public void addResource (InputStream in) //已打开的输入流public void addResource (Path file) //Hadoop文件路径public void addResource (String name) //CLASSPATH 资源 String形式public void addResource (URL url) //URL,统一资源定位符，如https://lyhcc.github.io 布尔变量loadDefaults用于确定是否加载默认资源，这些默认资源保存在defaultResources中。注意，defaultResources 是个静态成员变量，通过方法addDefaultResource()可以添加系统的默认资源。在HDFS中，会把hdfs-default.xml和hdfs-site.xml作为默认资源，并通过addDefaultResource()保存在成员变量defaultResources中;在MapReduce中，默认资源是mapred-default.xml和mapred-site.xml。1234567//下面的代码来自hadoop-1.x 的org.apache.hadoop.hdfs.server.datanode.DataNode static{ Configuration . addDe faultResource (\"hdfs-default . xml\") ; Conf igurat ion. addDe faultResource (\"hdfs-site. xml\") ;}//在hadoop2.x以后这代码被移到了Configuration类里面//hadoop-2.8.4中的1809行 properties 存放Hadoop配置文件解析后的键-值对，为java.util.Properties类型 finalParameters 类型是Set， 用来保存所有在配置文件中已经被声明为final的键-值对的键 overlay用于记录通过set()方式改变的配置项。也就是说，出现在overlay中的键-值对是应用设置的，而不是通过对配置资源解析得到的，为java.util.Properties类型 Configuration 是一一个类加载器变量，可以通过它来加载指定类，也可以通过它加载相关的资源。 上面提到addResource()可以通过字符串方式加载CLASSPATH资源，它其实通过Configuration中的getResource()将字符串转换成URL资源 123public URL getResource (String name){ return classLoader.getResource(name) ;} 2.8.4版本的Configuration类的1188行 Configuration类的过程构造Configuration对象资源加载添加资源到Configuration对象的方法有两种 对象的addResource()方法 类的静态addDefaultResource()方法(设置了loadDefaults标志) 添加的资源并不会立即被加载，只是通过reloadConfiguration()方法清空properties和finalParameters。相关代码如下: 123456789101112131415 //以URL资源为例public void addResource(URL url) { this.addResourceObject(new Configuration.Resource(url)); } //添加资源 private synchronized void addResourceObject(Configuration.Resource resource){ this.resources.add(resource); this.restrictSystemProps |= resource.isParserRestricted(); this.reloadConfiguration(); } //资源重新加载触发函数 public synchronized void reloadConfiguration() { this.properties = null; this.finalParameters.clear(); } 以上是类的成员方法addResource()方法的调用。 静态方法dDefaultResource()也可以清空Configuration对象中的数据（非静态成员），只不过是通过需要通过REGISTRY作为媒介进行。能够调用是因为REGISTRY记录了系统所有的Configuration对象REGISTRY的定义以及为其添加参数的过程 12345678private static final WeakHashMap&lt;Configuration, Object&gt; REGISTRY = new WeakHashMap();public Configuration(boolean loadDefaults) { ... synchronized(Configuration.class) { REGISTRY.put(this, (Object)null); }} 成员变量properties中的数据只在被调用的时候才会被加载进来。在getProps方法中，properties为空时，会触发loadResources()执行 123456789101112131415161718192021222324protected synchronized Properties getProps() { if (this.properties == null) { this.properties = new Properties(); Map&lt;String, String[]&gt; backup = new ConcurrentHashMap(this.updatingResource); this.loadResources(this.properties, this.resources, this.quietmode); ... } return this.properties;}//加载默认资源private void loadResources(Properties properties, ArrayList&lt;Configuration.Resource&gt; resources, boolean quiet) { if (this.loadDefaults) { Iterator i$ = defaultResources.iterator(); while(i$.hasNext()) { String resource = (String)i$.next(); this.loadResource(properties, new Configuration.Resource(resource, false), quiet); } if (this.getResource(\"hadoop-site.xml\") != null) { this.loadResource(properties, new Configuration.Resource(\"hadoop-site.xml\", false), quiet); }} hadoop配置文件解析 hadoop 的配置文件都是XML文件，JAXP(JAVA API for XML processing)是一种稳定的、可靠的XML处理API，支持两种XML处理方法 SAX解析(Simple API for XML) DOM解析(Documnet Object Model) hadoop使用的DOM解析 两种解析的区别 SAX 提供了一种流式的、事件驱动的XML处理方式 缺点：编写处理逻辑比较复杂 优势：适合处理大的XML文件 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档 Configurable接口 Configurable是一个很简单的接口，位于org.apache.hadoop.conf包中 类图 hadoop 代码中存在大量实现了该接口的类，可以通过setConf方法设置配置参数简化创建和setConf的两个步骤java反射机制实现，利用org.apache.hadoop.util.ReflectionUtils的newInstance方法 1public static &lt;T&gt; T newInstance(Class&lt;T&gt; theClass, Configuration conf) 该方法调用了ReflectionUtils中的setConf方法 12345678910public static void setConf(Object theObject, Configuration conf) { if (conf != null) { if (theObject instanceof Configurable) { ((Configurable)theObject).setConf(conf); } setJobConf(theObject, conf); } }","link":"/post/fa571a7.html"},{"title":"MapReduce介绍","text":"MapReduce的定义 &emsp;&emsp;Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。&emsp;&emsp;Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 MapReduce优缺点优点 MapReduce 易于编程。它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 良好的扩展性。当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 高容错性。MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 适合PB级以上海量数据的离线处理。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。缺点 MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 MapReduce核心思想 分布式的运算程序往往需要分成至少2个阶段。第一个阶段的maptask并发实例，完全并行运行，互不相干。 第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 MapReduce进程 一个完整的mapreduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调。 MapTask：负责map阶段的整个数据处理流程。 ReduceTask：负责reduce阶段的整个数据处理流程。 MapReduce编程规范 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper阶段 12345（1）用户自定义的Mapper要继承自己的父类 （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） （3）Mapper中的业务逻辑写在map()方法中 （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） （5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 Reduce阶段 1234（1）用户自定义的Reducer要继承自己的父类 （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV （3）Reducer的业务逻辑写在reduce()方法中 （4）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 第一代和第二代MapReduce的区别","link":"/post/5429.html"},{"title":"HDFS快照管理","text":"快照管理 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 快照影响 快照创建瞬间完成，所耗时间成本为O(1) 快照修改时才会使用额外的额外的内存空间，内存成本O(M),M表示修改过的文件或目录数 快照记录块和文件大小，不对DataNode中的块进行复制 说明：* 快照可以在HDFS任何目录下设置，一个目录最多容纳65536个并发快照 基本语法12345678（1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能）（2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）（3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照）（4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照）（5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）（6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有已快照目录）（7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处）（8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 案例实操 开启/禁用指定目录的快照功能 指定创建目录的位置为 /tmp/snapshot(即快照的存储目录)，在指定目录之前必须创建目录，不然会报错 12hdfs dfsadmin -allowSnapshot /tmp/snapshot hdfs dfsadmin -disallowSnapshot /tmp/snapshot //禁用时，对应的目录不允许存在快照 对目录创建快照 只有被开启快照功能的目录才能创建快照 123hdfs dfs -createSnapshot /tmp/snapshot // 对目录创建快照hdfs dfs -createSnapshot /tmp/snapshot snapshot //重命名快照（注：快照是只读的，无法修改名）通过web访问hdfs://Master:9000/tmp/snapshot/.snapshot/s…..// 快照和源文件使用相同数据块 查看快照 12hdfs dfs -lsr /tmp/snapshot/.snapshot/ //查看快照目录的详细信息hdfs lsSnapshottableDir //查看所有允许快照的目录 更改快照名字 12345hdfs dfs -renameSnapshot /tmp/snapshot/ snapshot snapshot1 注：路径只是你创建得名字/tmp/snapshot，不要带后边得/tmp/snapshot/.snapshot/，不然会出现hdfs dfs -renameSnapshot /tmp/snapshot/.snapshot/ snapshot1 snapshotrenameSnapshot: Modification on a read-only snapshot is disallowed 比较两个快照目录的不同之处 1234[root@vmaster opt]# hdfs snapshotDiff /tmp/snapshot s1 s2Difference between snapshot s1 and snapshot s2 under directory /tmp/snapshot:M .+ ./p 符号的意义： 符号 含义 + 文件或者目录被创建 - 文件或目录被删除 M 文件或目录被修改 R 文件或目录被重命名 恢复快照 123451.自定义创建一个快照名：hdfs dfs -createSnapshot /HAHA1 snapshot12.展示原文件包含内容：Hadoop fs -ls /HAHA13.里面有五个文件、删除其中1~2个/HAHA1/.snapshot/snapshot14.回复快照：hdfs dfs -cp /HAHA1/.snapshot/snapshot1 /snapshot 删除快照 12dfs dfs -deleteSnapshot 快照目录 快照名称dfs dfs -deleteSnapshot /tmp/snapshot snapshot1","link":"/post/21534.html"}],"tags":[{"name":"hadoop配置文件解析","slug":"hadoop配置文件解析","link":"/tags/hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/"},{"name":"DOM解析","slug":"DOM解析","link":"/tags/DOM%E8%A7%A3%E6%9E%90/"},{"name":"分布式存储管理","slug":"分布式存储管理","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"name":"NoSSQL特点","slug":"NoSSQL特点","link":"/tags/NoSSQL%E7%89%B9%E7%82%B9/"},{"name":"什么是大数据","slug":"什么是大数据","link":"/tags/%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Java代理机制","slug":"Java代理机制","link":"/tags/Java%E4%BB%A3%E7%90%86%E6%9C%BA%E5%88%B6/"},{"name":"Java Proxy","slug":"Java-Proxy","link":"/tags/Java-Proxy/"},{"name":"Java远程调用","slug":"Java远程调用","link":"/tags/Java%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"},{"name":"RMI","slug":"RMI","link":"/tags/RMI/"},{"name":"Java代理","slug":"Java代理","link":"/tags/Java%E4%BB%A3%E7%90%86/"},{"name":"Hadooop介绍","slug":"Hadooop介绍","link":"/tags/Hadooop%E4%BB%8B%E7%BB%8D/"},{"name":"Google三宝","slug":"Google三宝","link":"/tags/Google%E4%B8%89%E5%AE%9D/"},{"name":"Hadoop RPC","slug":"Hadoop-RPC","link":"/tags/Hadoop-RPC/"},{"name":"Hadoop 通信机制","slug":"Hadoop-通信机制","link":"/tags/Hadoop-%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/"},{"name":"Hadoop 压缩","slug":"Hadoop-压缩","link":"/tags/Hadoop-%E5%8E%8B%E7%BC%A9/"},{"name":"Hadoop 文件分片","slug":"Hadoop-文件分片","link":"/tags/Hadoop-%E6%96%87%E4%BB%B6%E5%88%86%E7%89%87/"},{"name":"MapReduce 通信协议","slug":"MapReduce-通信协议","link":"/tags/MapReduce-%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"},{"name":"Hadoop 协议","slug":"Hadoop-协议","link":"/tags/Hadoop-%E5%8D%8F%E8%AE%AE/"},{"name":"hadoop序列化","slug":"hadoop序列化","link":"/tags/hadoop%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"Windows操作系统配置文件","slug":"Windows操作系统配置文件","link":"/tags/Windows%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Java配置文件","slug":"Java配置文件","link":"/tags/Java%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Hadoop配置文件","slug":"Hadoop配置文件","link":"/tags/Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"MapReduce介绍","slug":"MapReduce介绍","link":"/tags/MapReduce%E4%BB%8B%E7%BB%8D/"},{"name":"HDFS快照","slug":"HDFS快照","link":"/tags/HDFS%E5%BF%AB%E7%85%A7/"}],"categories":[{"name":"others","slug":"others","link":"/categories/others/"},{"name":"BigData","slug":"BigData","link":"/categories/BigData/"},{"name":"Hadoop","slug":"BigData/Hadoop","link":"/categories/BigData/Hadoop/"},{"name":"Hadoop_common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"Hadoop common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"MapReduce","slug":"BigData/Hadoop/MapReduce","link":"/categories/BigData/Hadoop/MapReduce/"},{"name":"HDFS","slug":"BigData/Hadoop/HDFS","link":"/categories/BigData/Hadoop/HDFS/"}]}