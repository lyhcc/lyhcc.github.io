{"pages":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/about/about.html"},{"title":"标签","text":"","link":"/tags/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"","text":"","link":"/album/index.html"},{"title":"album","text":"","link":"/album/index-1.html"}],"posts":[{"title":"分布式存储管理","text":"为什么直接采用关系模型的分布式数据库并不能适应大数据时代的？ 规模效应所带来的压力 传统数据库倾向于纵向扩展(Scale-Up)，即增加单台计算机的性能 适应大数据的数据库系统的应该具有良好的横向扩展(Scale-Out)，即为集群增加一台计算机 数据类型的多样化 传统数据类型： 结构化数据大数据时代的数据类型： 结构化数据 半结构化数据 非结构化数据 设计理念所带来的冲突 关系型数据库 One size fits all ,即面对不同问题不需要重新考虑数据管理问题 简单来说就是，单一模式可以适应所有变化 新理念 “One size fits one” 和 “One size fits domain” 数据库的事务特性 传统数据库ACID特性 A(Atom，原子性)，C(Consistency，一致性)，I(Isolation,隔离性)，D(Durability,持久性) 大数据时代的数据库BASE。 Basically Available(基本可用)，Soft State(柔性状态)，Eventually Consistency(最终一致性)根据分布式领域著名的CAP理论来看，ACID追求一致性C,而BASE更加关注A一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）新型数据库Spanner NoSQL 特点 模式自由（Schema-free） 支持简易备份（Easy Replication Support) 简单应用程序接口（Simple API） 最终一致性（或说支持BASE特性，不支持ACID特性） 支持海量数据（Huge Amount of Data）","link":"/post/4faba951.html"},{"title":"大数据介绍","text":"什么是大数据 百度百科的定义,大数据（BIG DATA），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 在目前的业界尚未对大数据由清晰明确的定义, 它的第一次出现是在麦肯锡公司的报告中出现的, 在维基百科上的较为模糊的定义是很难运用软件的手段获取大量的内容信息, 对其处理后整理得出的数据集合。其他计算机学科的学者给出的定义是数据的尺度极为巨大, 常规的数据处理软件无法对数据识别、存储和应用的海量数据信息 维基百科的定义，大数据是指无法在可承受的时间范围内用常规软件工具进行捕捉、管理和处理的数据集合。 研究机构Gartner定义，“大数据”是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 数据单位1MB = 1024KB、1GB = 1024MB1TB = 1024GB、1PB = 1024TB 大数据的特征 容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息； 种类（Variety）：数据类型的多样性； 速度（Velocity）：指获得数据的速度； 可变性（Variability）：妨碍了处理和有效地管理数据的过程。 真实性（Veracity）：数据的质量 复杂性（Complexity）：数据量巨大，来源多渠道 价值（value）：合理运用大数据，以低成本创造高价值 大数据相关技术 数据采集： OLAP(联机分析处理)和数据挖掘的基础。ETL工具负责将分布的、异构的数据源进行抽取，抽取到中间层，进行清洗、转换、集成，（不过对于负责的逻辑处理不会这么干，用Spark或者其他的进行处理），最后放到数据仓库中存储，如Hive 数据存取： 关系型数据库、NoSQL(Not Only SQL,泛指非关系型数据库)，SQL等 基础架构： 云存储、分布式文件存储等 数据处理： 自然语言处理(Natural Language Processing, NLP) 数据分析： 假设检验、显著性检验、差异检验、差异分析、相关性分析、T检验、方差分析、卡方分析、偏相关性分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、预测和残差分析、岭回归、Logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、判别分析、对应分析、快速聚类和聚类法、对应分析、多元对应分析等 数据挖掘： 分类（Classification）、估计（Estimation）、预测（Prediction)、相关性分析或关联规则（Association）、复杂数据类型挖掘（Text、Web、图形图像、视频、音频等） 模型预测： 预测模型、机器学习、建模仿真 结果呈现： 云计算、标签云、关系图等 大数据带来的变革大数据小故事 &emsp;&emsp;最早关于大数据的故事发生在美国第二大的超市塔吉特百货（Target）。孕妇对于零售商来说是个含金量很高的顾客群体。但是他们一般会去专门的孕妇商店而不是在Target购买孕期用品。人们一提起Target，往往想到的都是清洁用品、袜子和手纸之类的日常生活用品，却忽视了Target有孕妇需要的一切。那么Target有什么办法可以把这部分细分顾客从孕妇产品专卖店的手里截留下来呢？ &emsp;&emsp;为此，Target的市场营销人员求助于Target的顾客数据分析部的高级经理Andrew Pole，要求他建立一个模型，在孕妇第2个妊娠期就把她们给确认出来。在美国出生记录是公开的，等孩子出生了，新生儿母亲就会被铺天盖地的产品优惠广告包围，那时候Target再行动就晚了，因此必须赶在孕妇第2个妊娠期行动起来。如果Target能够赶在所有零售商之前知道哪位顾客怀孕了，市场营销部门就可以早早的给他们发出量身定制的孕妇优惠广告，早早圈定宝贵的顾客资源。 &emsp;&emsp;可是怀孕是很私密的信息，如何能够准确地判断哪位顾客怀孕了呢？Andrew Pole想到了Target有一个迎婴聚会（baby shower）的登记表。Andrew Pole开始对这些登记表里的顾客的消费数据进行建模分析，不久就发现了许多非常有用的数据模式。比如模型发现，许多孕妇在第2个妊娠期的开始会买许多大包装的无香味护手霜；在怀孕的最初20周大量购买补充钙、镁、锌的善存片之类的保健品。最后Andrew Pole选出了25种典型商品的消费数据构建了“怀孕预测指数”，通过这个指数，Target能够在很小的误差范围内预测到顾客的怀孕情况，因此Target就能早早地把孕妇优惠广告寄发给顾客。 &emsp;&emsp;那么，顾客收到这样的广告会不会吓坏了呢？Target很聪明地避免了这种情况，它把孕妇用品的优惠广告夹杂在其他一大堆与怀孕不相关的商品优惠广告当中，这样顾客就不知道Target知道她怀孕了。百密一疏的是，Target的这种优惠广告间接地令一个蒙在鼓里的父亲意外发现他高中生的女儿怀孕了，此事甚至被《纽约时报》报道了，结果Target大数据的巨大威力轰动了全美。 &emsp;&emsp;根据Andrew Pole的大数据模型,Target制订了全新的广告营销方案，结果Target的孕期用品销售呈现了爆炸性的增长。Andrew Pole的大数据分析技术从孕妇这个细分顾客群开始向其他各种细分客户群推广，从Andrew Pole加入Target的2002年到2010年间，Target的销售额从440亿美元增长到了670亿美元。 &emsp;&emsp;我们可以想象的是，许多孕妇在浑然不觉的情况下成了Target常年的忠实拥泵，许多孕妇产品专卖店也在浑然不知的情况下破产。浑然不觉的背景里，大数据正在推动一股强劲的商业革命暗涌，商家们早晚要面对的一个问题就是：究竟是在浑然不觉中崛起，还是在浑然不觉中灭亡 其他故事 Google根据搜索关键字分析流感病毒H1N1 2008年金融危机 大数据初步学习路线 技术 工具 JAVA 面向对象的编程语言 Linux 类Unix操作系统 Hadoop生态圈 1、HDFS 解决存储问题存储极大数目的信息（terabytes or petabytes），将数据保存到大量的节点当中。支持很大单个文件。提供高可靠性，是指一个或多个节点故障，系统仍然可以继续工作提供数据快速访问 2、MapReduce 解决计算问题它有个特点就是不管多大的数据只要给它时间它就能把数据跑完，但是时间可能不是很快所以它叫数据的批处理 3、Yarn 资源调度器 4、ZooKeeper 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 5、Flume 数据采集工具 6、Hive 基于Hadoop的数据仓库工具 7、Hbase 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 8、Sqoop 数据传递工具，如将数据从关系型数据库导入Hive Scala 多范式编程语言、面向对象和函数式编程的特性 Spark 目前企业常用的批处理离线数据/实时计算引擎它是用来弥补基于MapReduce处理数据速度上的缺点，它很是流氓，直接将数据存在内存中 【注意】 MapReduce运行时也是需要将代码数据加载到内存中的，只不过Spark都是基于内存操作 Flink 目前最火的流式处理框架、既支持流处理、也支持批处理 Elasticsearch 大数据分布式弹性搜索引擎 大数据处理流程 大数据处理模型按照数据的三状态定义 水库里一平如镜的水—&gt;静止数据(data at rest) 水处理系统中上下翻滚的水—&gt;正在使用的数据(data in use) 汹涌而来的新水流—&gt;动态的水(data in motion) “快”说的是两层面 “动态数据” 来得快 “正在使用的数据” 处理得快 批处理 MapReduce 流处理 Spark Streaming 科普 根据国际数据公司（IDC）的《数据宇宙》报告显示：2008年全球数量为0.5ZB，2010年为1.2ZB，人类正式进入ZB时代。更为惊人的是，2020年以前全球数据量仍将保持每年40%多的高速增长，大约每两年就翻一倍，这与IT界的摩尔定律极为相似，姑且称之为“大数据爆炸定律”。","link":"/post/2260.html"},{"title":"CentOS配置","text":"连网配置防火墙 关闭防火墙：systemctl stop firewalld.service 禁用防火墙：systemctl disable firewalld.service 查看防火墙：systemctl status firewalld.service 12关闭Selinux：vi /etc/selinux /config将SELINUX=enforcing改为SELINUX=disabled 自定义IPvi /etc/sysconfig/network-scripts/ifcfg-ens33 12345678BOOTPROTO=&quot;static&quot; # 默认DHCP自动获取IP地址ONBOOT=&quot;yes&quot;IPADDR=192.168.37.129 # 自定义的IP地址GATEWAY=192.168.37.2 # 默认网关DNS1=8.8.8.8 # DNS服务器,googleDNS2=8.8.4.4 # DNS服务器,baiduNETMASK=255.255.255.0 # 子网掩码 vi /etc/resolv.conf 12nameserver 8.8.8.8nameserver 8.8.4.4 重启网卡：service network restart修改主机名（可选）：hostnamectl set-hostname aaa配置好后就可以使用Xshell连接了，这样也就可以避免了IP地址的动态分配，造成IP地址每次启动都不一样，下面来看一下DHCP的过程 主机广播服务器广播DHCP报文(询问：有没有DHCP服务器) DHCP服务器广播DHCP报文（回答：有，有，有） 主机广播DHCP请求报文（我现在要使用你的IP地址了） DHCP服务器广播DHCP确认报文（用吧） 更详细的过程 文件上传下载命令yum install lrzsz JDK安装 查询是否安装java软件：1rpm -qa|grep java 如果安装的版本低于1.7，卸载该jdk：1rpm -e 软件包名字 在/opt目录下创建两个子文件（可选）1mkdir /opt/module /opt/software 解压jdk到/opt/module目录下1tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/ 配置jdk环境变量vi /etc/profile12export JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin source /etc/profile 测试jdk安装成功12java -versionjava version &quot;1.8.0_144&quot;","link":"/post/f245a8ad.html"},{"title":"十大经典算法的优缺点","text":"KNN优点： 理论成熟，实现简单 缺点： 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数 计算量较大 Apriori优点： 适合稀疏数据集。 算法原理简单，易实现。 适合事务数据库的关联规则挖掘 缺点： 多次扫描事务数据库，需要很大的I/O负载 对每次k循环，侯选集Ck中的每个元素都必须通过扫描数据库一次来验证其是否加入Lk。假如有一个最大频繁项目集包含10个项的话，那么就至少需要扫描事务数据库10遍。 可能产生庞大的侯选集 由Lk-1产生k-侯选集Ck是指数增长的，例如104个1-频繁项目集就有可能产生接近107个元素的2-侯选集。如此大的侯选集对时间和主存空间都是一种挑战。 决策树ID3 优点： 理论清晰，方法简单 学习能力强 ID3算法在搜索的每一步都使用当前的所有训练样例，大大降低了对个别训练样例错误的敏感性 缺点： 只对比较小的数据集有效，且对噪声比较敏感，当训练数据集加大时，决策树可能会随之改变。 ID3算法在搜索过程中不进行回溯。收敛到局部最优而不是全局最优 ID3算法只能处理离散值的属性。 信息增益度量存在一个内在偏置，它偏袒具有较多值的属性。如日期属性。 ID3算法增长树的每一个分支的深度，直到恰好能对训练样例完美地分类。当数据中有噪声或训练样例的数量太少时，产生的树会过渡拟合训练样例。 ID3的改进 C4.5 优点： 通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免出现过拟合的特性，提升决策树的泛化能力。 CART优点： 可以处理连续值 缺点： EM （Expectation Maximization）优点： 简单稳定 缺点： 在缺失数据较多的情形,收敛的速度较慢，次数多，容易陷入局部最优 对于某些情况下,要计算算法中的M步,即完成对似然函数的估计是非常困难的 在某些情况下是要获得EM算法中的E步的期望显式是非常困难或者不可能的 朴素贝叶斯优点： 生成式模型，通过计算概率来进行分类，可以用来处理多分类问题， 对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。 缺点： 对输入数据的表达形式很敏感， 由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。 需要计算先验概率，分类决策存在错误率。 K-means优点： 可解释性比较强。 调参的参数仅为簇数k。 相对于高斯混合模型而言收敛速度快，因而常用于高斯混合模型的初始值选择。K-means 的时间复杂度为 O(N⋅K⋅I) ，簇数 K 和 迭代次数 I 通常远小于N，所以可优化为 O(N) ，效率较高。 缺点： 对离群点敏感。 K值难以事先选取，交叉验证不大适合 PageRank优点： PageRank算法通过网页间的链接来评价网页的重要性，在一定程度上避免和减少了人为因素对排序结果的影响； 采用与查询无关的离线计算方式，使其具有较高的响应速度； 一个网页只能通过别的网页对其引用来增加自身的PR值，且算法的均分策略使得一个网页的引用越多，被引用网页所获得的PR值就越少。 因此，算法可以有效避免那些为了提高网站的搜索排名而故意使用链接的行为。 缺点： 主题漂移问题 PageRank 算法仅利用网络的链接结构，无法判断网页内容上的相似性；且算法根据向外链接平均分配权值使得主题不相关的网页获得与主题相关的网页同样的重视度，出现主题漂移。 偏重旧网页问题 决定网页 P R 值的主要因素是指向它的链接个数的多少。一个含有重要价值的新网页，可能因为链接数目的限制很难出现在搜索结果的前面，而不能获得与实际价值相符的排名。 算法并不一定能反映网页的重要性，存在偏重旧网页现象。 忽视用户个性化问题 PageRank算法在设计之初，没有考虑用户的个性化需要。个性化搜索引擎的兴起，对 PageRank排序算法提出新的挑战。 参考： 朴素贝叶斯 https://www.jianshu.com/p/6309e084ce64 K-means https://www.cnblogs.com/massquantity/p/9416109.html","link":"/post/85ed9b6d.html"},{"title":"Tomcat 的安装配置","text":"Tomcat下载继续下一步安装即可 在浏览器输入localhost:8080查看是否安装成功，当然8080端口是默认端口，更改后的使用更改后的端口。 环境变量配置123456789新建两个变量名：CATALINA_HOME变量值：D:\\ApacheTomcat\\bin变量名：CATALINA_BASE变量值：D:\\ApacheTomcat\\bin在PATH中添加%CATALINA_HOME%\\bin 详细参考 新建JavaWeb工程遇到的问题 新建jsp文件后报错 原因是没有导入Tomcat包 在buildpath-&gt;add Libarary-&gt;server runtime找不到Tomcat 在preference中找Server 再选runtime environment选择Tomcat安装路径后再回到第一步，就可以解决这个问题","link":"/post/9f166c45.html"},{"title":"Java Web过滤器和监听器","text":"Java Web 三大基本组件（Servlet、Filter、Listener）Servlet： 处理请求Filter: 过滤拦截请求Listener：监听器 三大组件中基本都需要在web.xml中进行注册：除了Listener的两个（活化钝化监听器、绑定监听器）需要JavaBean实现不需要注册外 过滤器过滤器的使用步骤 实现Filter接口 去web.xml注册 Filter配置12345678&lt;filter&gt; &lt;filter-name&gt;MyFilter&lt;/filter-name&gt; &lt;filter-class&gt;&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;&lt;/filter-name&gt; &lt;url-pattern&gt;&lt;/url-pattern&gt;&lt;/filter-mapping&gt; url-pattern的三种写法 精确匹配 /pics/hello.jsp /hello/login: 直接拦截指定路径 路径匹配(模糊匹配) /pics/* ： 拦截pics下的所有请求 后缀匹配(模糊匹配) *.jsp:拦截所有以jsp结尾的请求 【注】 不能使用/pics/*.jsp Filter原理1234doFilter(){ //放行请求 chain.doFilter(request, response);} 多个Filter的访问顺序 监听器 八个：ServletRequest(2)、HttpSession(4)、ServletContext(2)2:生命周期监听器、属性变化监听器4：HttpSession额外监听器，还有两个(活化钝化监听器、绑定监听器)掌握监听器：ServletContextListener(生命周期监听器)：监听Servlet的创建和销毁(监听服务器的启动停止): 服务器启动为当前项目创建ServletContext对象，服务器停止销毁创建的ServletServletContext 一个Web项目对应一个ServletContext，它代表当前web项目的信息 还可以作为最大的域对象在整个项目运行期间共享数据 使用 实现对应的监听接口 去web.xml中进行配置 注意：有两个Listener是JavaBean需要实现的接口（HttpSessionActivitionListener, HttpSeesionBindingListener）","link":"/post/4585d600.html"},{"title":"AJAX和JSON使用","text":"JSON什么是JSON JSON (JavaScript Object Notation)是(与xml对比)一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。它 基于JavaScript Programing Language, Standard ECMA -262 3rd Edition - December 1999 的一个子集。JSON 采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C, C++，C#,Java,JavaScript, Perl, Python 等)。这些特性 使JSON成为理想的数据交换语言。 JSON格式{key:value,key:value}value的类型 基本类型（字符串、数字、布尔值） 数组 {lastName:”李四”,books:[“西游记”,”红楼梦”,{}]} 对象 {} 123456{ \"lastname\":\"张三\", \"age\":19, \"car\":{\"pp\":\"宝马\",\"price\":\"10000$\"}, \"books\":[\"西游记\",\"红楼梦\"]} 浏览器处理方便，js解析方便JSON：js进行传输，(HTTP只能传输文本)js定义对象加不加双引号都可以JSON是利于传输的数据js中将对象转为JSON ： 1JSON.stringify(student) JSON转js对象： 1JSON.parse(str) AJAX AJAX ( Asynchronous Javascript And XML (景步JavaScrietXML) ) : AJAX：是一种无刚新页面与服务器的交互技术: (页面不剧新顿可以收到服务器响应的数据) 原来的交互 发送请求 服务器收到请求。调用对应的Servlet进行处理; servlet处理完感会有响应信息生成; 浏览器收到了服务器响应的数据，把之前的数据页面的清除，展示新的数据（效果是页面刷新） 现在的交互: ( XmlHttpRequest对象) XMLHttpRequest对象帮我们发送请求 服务器收到请求。调用对应的servlet进行处理; servlet处理完成会有响应信息成; XMLHttpRequest对象收数据;(浏览器就感受不到这个数据；xmlhttprequest对象收到这个数据) 上面两种交互图解 XHR原生编程 XMLHttpRequest对象 所有现代浏览器均支持XMLHttpRequest对象 xhr原生编程: 12345678910var xhr=new XMLHttpRequest0;//创建xhr对象;xhr.open(\"GET\",\"test1.txt\",true);//建立连接xhr.send() //发送数据//监听xhr的状态xhr.onreadystatechange=function()if (xhr.readyState==4 &amp;&amp; xhr.status= =200){ //获取数据 document.getElementByld(\"myDiv\").innerHTML =xhr.responseText;} 改变了我们传统的交互方式; 发请求; 服务器收到请求,处理请求经常要给页面携带数据。reguestsettribute(“map”map);转发到页面 浏览器收到页面数据,在页面使用e!表达式获取数据;导致页面整个刷新;造成很大的服务器负担; 只让服务器返回我们需要的部分数据即可;不用返回整个页面; xhr营代浏览器来接受响应发送请求;利用dom增删改的方式来改变页面效果; 异步：不会堵塞服务器同步：浏览器在接收请求后继续执行，也就是会堵塞到数据返回 什么是ajax :xhr对象向服务器发送请求,并收到响应数据,利用dom增删改的方式改变页面效果 JQuery-AJAX $.get()123456789101112131415 // $get(url,[data],[callback],[type])函数说明// data 传递的数据，可以是k=b&amp;k=v 也可以是js对象// callback ，定义一个回调函数，随便定义一个参数，这个参数就封装了服务器的返回数据// type:返回内容格式，xml, html, script, json, text, _default。 //type指定为json，jquery自动转换为json$.get(\"${ctp}/getinfo\",\"key=value\",function(d){ alert(d);}) //点击事件如不想更新整个页面，就直接在点击后function中返回false $(\"#btn\").click(function(){ return false }) 服务器可以使用gson工具生成json字符串给浏览器123456789 //使用前需要导包，包下载路径 //https://lyhcc.github.io/resourses/files/gson-2.2.4.jarMap&lt;String, Object&gt;map = new HashMap&lt;&gt;();map.put(\"lastname\", \"admin\");map.put(\"age\", 10);//解析Gson gson = new Gson();response.getWriter().write(gson.toJson(map)); $.post() $.ajax(默认异步请求) 123456789101112$.ajax({type: \"POST\",async: false,url: \"getinfo\",data: \"name=John&amp;location=Boston\",success: function(msg){ alert( \"Data Saved: \" + msg );},error: function(xhr, textStatus){ //xhr XMLHttpRequest}}); 国际化、文件上传与下载国际化 国际化：根据Locale代表的区域信息可以进行国际化 得到需要的国际化区域信息：locale cn = Locale.CHINA 需要使用ResourceBundle绑定写好的国际化资源文件(基础名_语言代码_国家代码.properties) 1ResourceBundle bundle = ResoourceBundle.getBundle(&quot;bookstore&quot;, cn) 从bundle中获取配置文件中的值 1String ussername = bundle.getString(&quot;username&quot;); 更多的国际化功能 12345678java.lang.Objectjava.text.FormatAll Implemented Interfaces: Serializable, CloneableDirect Known Subclasses: DateFormat MessageFormat, NumberFormat Q 两种方式进行国际化 根据浏览器的请求头带来的信息国际化页面 12345 Locale locale = request.getLocale(); ``2. 点击超链接切换国际化 - 超链接上戴上区域信息；Locale就根据带上的区域信息来new - 推荐国际化取值，格式化日期 &lt;fmt:message key=&quot;k&quot;&gt; 1234### 文件上传下载#### 上传1. 上传准备： &lt;form method=&quot;post&quot; entype=&quot;multipart/from-data&quot;&gt; 123452. 文件上传请求体，多部件形式3. 需要导包处理#### 下载&gt;把文件交给浏览器，一定告诉浏览器，这个流不要打开请下载 response.setHeader(“Content-Disposition”, “attachment;filename=img.png”) 要求 Jquery(精通) XML了解 Tomcat(掌握) Servlet(掌握思想) JSP/JSTL/EL(熟悉) HTML/CSS/JS(掌握) Session、Cookie(掌握) Filter(掌握) Listener(掌握ServletContext) AJAX和JSON(精通) 国际化和文件上传(了解)","link":"/post/3450b53b.html"},{"title":"Java设计模式面试题","text":"有请使用UML类图画出原型模式核心角色 原型设计模式的深拷贝和浅拷贝是什么，并写出深拷贝的两种方式的源码(重写clone方法实现深拷贝、 使用序列化来实现深拷贝) 在Spring框架中哪里使用到原型模式，并对源码进行分析 beans.xml 1&lt;bean id=\"id01\" class=\"com.atguigu.spring.bean.Monster\" scope=\"prototype\"/&gt; Spring中原型bean的创建，就是原型模式的应用 代码分析+Debug源码 设计模式的七大原则 : 要求： 1) 七大设计原则核心思想 2) 能够以类图的说明设计原则 3) 在项目实际开发中，你在哪里使用到了ocp原则 设计模式常用的七大原则有: 单一职责原则 接口隔离原则 依赖倒转原则 里氏替换原则 开闭原则ocp 迪米特法则 合成复用原则 给定案例让你解释对应的设计模式 金融借贷平台项目： 借贷平台的订单，有审核-发布-抢单 等等 步骤，随着操作的不同，会改变订单的状态, 项目中的这个模块实现就会使用到状态模式，请你使用状态模式进行设计，并完成实际代码问题分析 ：这类代码难以应对变化，在添加一种状态时，我们需要手动添加if/else，在添加一种功能时，要对所有的状态进行判断。因此代码会变得越来越臃肿，并且一旦没有处理某个状态，便会发生极其严重的BUG，难以维护 解释器设计模式 1) 介绍解释器设计模式是什么?2) 画出解释器设计模式的UML类图,分析设计模式中的各个角色是什么?3) 请说明Spring的框架中，哪里使用到了解释器设计模式，并做源码级别的分析 解释器模式在Spring框架应用的源码剖析 单例设计模式一共有几种实现方式？请分别用代码实现，并说明各个实现方式的优点和缺点? 单例设计模式一共有8种写法，1) 饿汉式 两种2) 懒汉式 三种3) 双重检查4) 静态内部类5) 枚举 设计模式的重要性 软件工程中， 设计模式（design pattern）是对软件设计中普遍存在（反复出现）的各种问题，所提出的解决方案。这个术语是由埃里希·伽玛（Erich Gamma）等人在1990年代从建筑设计领域引入到计算机科学的 大厦 VS 简易房 拿实际工作经历来说, 当一个项目开发完后，如果客户提出增新功能，怎么办?。可扩展性，使用设计模式，使软件具有更好的扩展性 如果项目开发完后，原来程序员离职，你接手维护该项目怎么办? (维护性[可读性、规范性]) 目前程序员门槛越来越高，一线IT公司(大厂)，都会问你在实际项目中使用过什么设计模式，怎样使用的，解决了什么问题。 设计模式在软件中哪里？面向对象(oo)=&gt;功能模块[设计模式+算法(数据结构)]=&gt;框架[使用到多种设计模式]=&gt;架构 [服务器集群] 如果想成为合格软件 工程师 ， 那就花时间来研究下设计模式是非常必要的","link":"/post/9174d842.html"},{"title":"Java设计模式之设计模式七大原则","text":"单一职责原则基本介绍 对类来说的，即一个类应该只负责一项职责。如类A负责两个不同职责：职责1，职责2。当职责1需求变更而改变A时，可能造成职责2执行错误， 所以需要将类A的粒度分解为A1， A2 应用实例 以水果类为例 方案一 在方式1中的eat方法中，违反了单一职责原则 解决方案非常简单，根据水果的方法不同，分解成不同的类 1234567891011121314public class SingleResposibility01 { public static void main(String[] args) { Fruit fruit = new Fruit(); fruit.eat(\"苹果\"); fruit.eat(\"葡萄\"); fruit.eat(\"菠萝\"); }}class Fruit{ public void eat(String fruit) { System.out.println(\"正在红色的\" + fruit); }} 方案二 遵守单一职责原则 但是这样做的改动很多大，即将类的分解同时更改客户端 1234567891011121314151617181920212223242526public class SingleResponsibility02 { public static void main(String[] args) { RedFriut redFriut = new RedFriut(); redFriut.eat(\"苹果\"); GreenFruit greenFruit = new GreenFruit(); greenFruit.eat(\"葡萄\"); YellowFruit yellowFruit = new YellowFruit(); yellowFruit.eat(\"菠萝\"); }}class RedFriut{ public void eat(String fruit) { System.out.println(\"正在红色的\" + fruit); }}class GreenFruit{ public void eat(String fruit) { System.out.println(\"正在绿色的\" + fruit); }}class YellowFruit{ public void eat(String fruit) { System.out.println(\"正在黄色的\" + fruit); }} 方案三 这种修改方法没有对原来的类做大的修改，只是增加方法 这里虽然没有在类这个级别上遵守单一职责原则，但在方法级别上仍然是遵守单一职责原则 1234567891011121314151617181920public class SingleResposibility03 { public static void main(String[] args) { MyFruit fruit = new MyFruit(); fruit.eatRedFruit(\"苹果\"); fruit.eatGreenFruit(\"葡萄\"); fruit.eatYellowFruit(\"菠萝\"); }}class MyFruit { public void eatRedFruit(String Fruit) { System.out.println(\"正在吃红色的\" + Fruit); } public void eatGreenFruit(String Fruit) { System.out.println(\"正在吃绿色的\" + Fruit); } public void eatYellowFruit(String Fruit) { System.out.println(\"正在吃黄色的\" + Fruit); }} 单一职责原则注意事项和细节 降低类的复杂度，一个类只负责一项职责。 提高类的可读性，可维护性 降低变更引起的风险 通常情况下， 我们应当遵守单一职责原则，只有逻辑足够简单，才可以在代码级违反单一职责原则；只有类中方法数量足够少，可以在方法级别保持单一职责原则 接口隔离原则应用实例 类A通过接口Interface1依赖类C，类B通过接口Interface1依赖类D，请编写代码完成此应用实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485interface interface1{ public void method1(); public void method2(); public void method3(); public void method4(); public void method5();}class C implements interface1{ @Override public void method1() { System.out.println(\"C 实现了接口1的方法1\"); } @Override public void method2() { System.out.println(\"C 实现了接口1的方法2\"); } @Override public void method3() { System.out.println(\"C 实现了接口1的方法3\"); } @Override public void method4() { System.out.println(\"C 实现了接口1的方法4\"); } @Override public void method5() { System.out.println(\"C 实现了接口1的方法5\"); }}class D implements interface1{ @Override public void method1() { System.out.println(\"D 实现了接口1的方法1\"); } @Override public void method2() { System.out.println(\"D 实现了接口1的方法2\"); } @Override public void method3() { System.out.println(\"D 实现了接口1的方法3\"); } @Override public void method4() { System.out.println(\"D 实现了接口1的方法4\"); } @Override public void method5() { System.out.println(\"D 实现了接口1的方法5\"); }}class A{ public void depend1(interface1 i) { i.method1(); } public void depend2(interface1 i) { i.method2(); } public void depend3(interface1 i) { i.method3(); }}class B{ public void depend1(interface1 i) { i.method1(); } public void depend4(interface1 i) { i.method4(); } public void depend5(interface1 i) { i.method5(); }} 应传统方法的问题和使用接口隔离原则改进 类A通过接口Interface1依赖类B，类C通过接口Interface1依赖类D，如果接口Interface1对于类A和类B来说不是最小接口，那么类C和类D必须去实现他们不需要的方法 将接口Interface1拆分为独立的几个接口，类A和类B分别与他们需要的接口建立依赖关系。也就是采用接口隔离原则 接口Interface1中出现的方法，根据实际情况拆分为三个接口 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class Segregation { public static void main(String[] args) { A a = new A(); C c = new C(); a.depend1(c); a.depend2(c); a.depend3(c); B b = new B(); D d = new D(); b.depend1(d); b.depend4(d); b.depend5(d); } }interface interface1{ public void method1();}interface interface2{ public void method2(); public void method3();}interface interface3{ public void method4(); public void method5();}class C implements interface1,interface2{ @Override public void method1() { System.out.println(\"C 实现了接口1的方法1\"); } @Override public void method2() { System.out.println(\"C 实现了接口2的方法2\"); } @Override public void method3() { System.out.println(\"C 实现了接口2的方法3\"); }}class D implements interface1,interface3{ @Override public void method1() { System.out.println(\"D 实现了接口1的方法1\"); } @Override public void method4() { System.out.println(\"D 实现了接口3的方法4\"); } @Override public void method5() { System.out.println(\"D 实现了接口3的方法5\"); }}class A{ public void depend1(interface1 i) { i.method1(); } public void depend2(interface2 i) { i.method2(); } public void depend3(interface2 i) { i.method3(); }}class B{ public void depend1(interface1 i) { i.method1(); } public void depend4(interface3 i) { i.method4(); } public void depend5(interface3 i) { i.method5(); }} 依赖倒转原则基本介绍 依赖倒转原则(Dependence Inversion Principle)是指：1) 高层模块不应该依赖低层模块，二者都应该依赖其抽象2) 抽象不应该依赖细节，细节应该依赖抽象3) 依赖倒转(倒置)的中心思想是面向接口编程4) 依赖倒转原则是基于这样的设计理念：相对于细节的多变性，抽象的东西要稳定的多。以抽象为基础搭建的架构比以细节为基础的架构要稳定的多。在java中，抽象指的是接口或抽象类，细节就是具体的实现类5) 使用接口或抽象类的目的是制定好规范，而不涉及任何具体的操作，把展现细节的任务交给他们的实现类去完成 应用实例 接口传递 构造方法传递 setter方法传递 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class DInversion { Food food; public static void main(String[] args) { //1. 接口传递 Fruit fruit = new Fruit(); Operation operation = new Operation(); operation.operate(fruit); //2. 构造方法传递 Operation operation2 = new Operation(new Vegetable()); operation2.operate(); //3. setter方法传递 Operation operation3 = new Operation(); operation3.setFood(fruit); operation3.operate(); } }class Operation{ private Food food; public Operation() { } public Operation(Food food) { this.food = food; } public void setFood(Food food) { this.food = food; } public void operate(Food food) { food.eat(); } public void operate() { food.eat(); }}interface Food{ public void eat();}class Vegetable implements Food{ @Override public void eat() { System.out.println(\"吃蔬菜...\"); } }class Fruit implements Food{ @Override public void eat() { System.out.println(\"吃水果..\"); } } 依赖转换原则的注意事项和细节 1) 低层模块尽量都要有抽象类或接口，或者两者都有，程序稳定性更好.2) 变量的声明类型尽量是抽象类或接口, 这样我们的变量引用和实际对象间，就存在一个缓冲层，利于程序扩展和优化3) 继承时遵循里氏替换原则 里氏替换原则OO中的继承性的思考和说明 1) 继承包含这样一层含义：父类中凡是已经实现好的方法， 实际上是在设定规范和契约，虽然它不强制要求所有的子类必须遵循这些契约，但是如果子类对这些已经实现的方法任意修改，就会对整个继承体系造成破坏。2) 继承在给程序设计带来便利的同时，也带来了弊端。比如使用继承会给程序带来侵入性，程序的可移植性降低，增加对象间的耦合性，如果一个类被其他的类所继承，则当这个类需要修改时，必须考虑到所有的子类，并且父类修改后，所有涉及到子类的功能都有可能产生故障3) 问题提出：在编程中，如何正确的使用继承? =&gt; 里氏替换原则 基本介绍 1) 里氏替换原则 (Liskov Substitution Principle) 在1988年，由麻省理工学院的以为姓里的女士提出的。2) 如果对每个类型为T1的对象o1，都有类型为T2的对象o2，使得以T1定义的所有程序P在所有的对象o1都代换成o2时，程序P的行为没有发生变化，那么类型T2类型T1的子类型。换句话说，所有引用基类的地方必须能透明地使用其子类的对象。3) 在使用继承时，遵循里氏替换原则，在子类中尽量不要重写父类的方法4) 里氏替换原则告诉我们，继承实际上让两个类耦合性增强了， 在适当的情况下，可以通过 聚合，组合，依赖 来解决问题。 一个程序引出问题思考解决方法 1) 我们发现原来运行正常的相减功能发生了错误。原因就是类B无意中重写了父类的方法，造成原有功能出现错误。在实际编程中，我们常常会通过重写父类的方法完成新的功能，这样写起来虽然简单，但整个继承体系的复用性会比较差。特别是运行多态比较频繁的时候2) 通用的做法是：原来的父类和子类都继承一个更通俗的基类，原有的继承关系去掉，采用依赖，聚合，组合等关系代替 开闭原则基本介绍 1) 开闭原则（Open Closed Principle） 是编程中最基础、最重要的设计原则2) 一个软件实体如类，模块和函数应该对扩展开放(对提供方)， 对修改关闭(对使用方)。 用抽象构建框架，用实现扩展细节。 ( 这里的使用放是指调用 )3) 当软件需要变化时，尽量通过扩展软件实体的行为来实现变化，而不是通过修改已有的代码来实现变化。4) 编程中遵循其它原则，以及使用设计模式的目的就是遵循开闭原则 看下面一段代码 123456789101112131415161718192021222324252627282930313233343536373839404142public class Test { public static void main(String[] args) { GraphicEditor graphicEditor = new GraphicEditor(); graphicEditor.drawCircle(new Circle()); graphicEditor.drawRectangle(new Rectangle()); }}class GraphicEditor { public void drawShape(Shape s) { if (s.m_type == 1) drawRectangle(s); else if (s.m_type == 2) drawCircle(s); } public void drawRectangle(Shape r) { System.out.println(\"矩形\"); } public void drawCircle(Shape r) { System.out.println(\"圆形\"); }}class Shape { int m_type;}class Rectangle extends Shape { Rectangle() { super.m_type = 1; }}class Circle extends Shape { Circle() { super.m_type = 2; }} 优缺点 1) 优点是比较好理解，简单易操作。2) 缺点是违反了设计模式的ocp原则，即对扩展开放(提供方)，对修改关闭(使用方)。即当我们给类增加新功能的时候，尽量不修改代码，或者尽可能少修改代码.3) 比如我们这时要新增加一个图形种类 三角形，我们需要做如下修改， 修改的地方较多 代码改进改进改进的思路分析思路： 把创建Shape类做成抽象类，并提供一个抽象的draw方法，让子类去实现即可，这样我们有新的图形种类时，只需要让新的图形类继承Shape，并实现draw方法即可，使用方的代码就不需要修 -&gt; 满足了开闭原则 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class OCP { public static void main(String[] args) { GraphicEditor graphicEditor = new GraphicEditor(); graphicEditor.drawShape(new Rectangle()); //增加图形而不更改调用的方法，（使用方） //正在运行的方法位置（提供方） graphicEditor.drawShape(new Triangle()); } }/** * 使用方 */class GraphicEditor { public void drawShape(Shape s) { s.draw(); }}abstract class Shape { int m_type; public abstract void draw();}class Rectangle extends Shape { Rectangle() { super.m_type = 1; } @Override public void draw() { System.out.println(\"画矩形\"); } }class Circle extends Shape { Circle() { super.m_type = 2; } @Override public void draw() { System.out.println(\"画圆\"); } }class Triangle extends Shape{ public Triangle() { super.m_type = 3; } @Override public void draw() { System.out.println(\"画三角形\"); } } 迪米特法则基本介绍 1) 一个对象应该对其他对象保持最少的了解2) 类与类关系越密切，耦合度越大3) 迪米特法则(Demeter Principle)又叫最少知道原则，即一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类不管多么复杂，都尽量将逻辑封装在类的内部。对外除了提供的public 方法，不对外泄露任何信息4) 迪米特法则还有个更简单的定义：*只与直接的朋友通信 *5) 直接的朋友：每个对象都会与其他对象有耦合关系，只要两个对象之间有耦合关系，我们就说这两个对象之间是朋友关系。耦合的方式很多，依赖，关联，组合，聚合等。其中，我们称出现成员变量，方法参数，方法返回值中的类为直接的朋友，而出现在局部变量中的类不是直接的朋友。也就是说，陌生的类最好不要以局部变量的形式出现在类的内部 应用实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class Demeter { public static void main(String[] args) { new PoisonousAirMaking().make(new GeneralAirMaking()); }}class GeneralAirMaking{ private ArrayList&lt;GeneralAir&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;GeneralAir&gt; retrieve() { list.add(new GeneralAir(\"氧气\")); list.add(new GeneralAir(\"二氧化碳\")); list.add(new GeneralAir(\"氮气\")); return list; }}class PoisonousAirMaking{ private ArrayList&lt;PoisonousAir&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;PoisonousAir&gt; retrieve() { list.add(new PoisonousAir(\"一氧化碳\")); list.add(new PoisonousAir(\"氨气\")); list.add(new PoisonousAir(\"二氧化硫\")); return list; } public void make(GeneralAirMaking sub) { retrieve(); for(PoisonousAir paAir:list) { System.out.println(\"有毒气体--\"+paAir.getType()); } System.out.println(\"---\"); ArrayList&lt;GeneralAir&gt; list2 = sub.retrieve(); for(GeneralAir gAir: list2) { System.out.println(\"普通气体--\" + gAir.getType()); } }}class GeneralAir{ private String type; public GeneralAir(String type) { super(); this.type = type; } public String getType() { return type; } public void setType(String type) { this.type = type; }}class PoisonousAir{ private String type; public PoisonousAir(String type) { super(); this.type = type; } public String getType() { return type; } public void setType(String type) { this.type = type; }} 应用实例改进12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class Demeter { public static void main(String[] args) { new PoisonousAirMaking().make(new GeneralAirMaking()); }}class GeneralAirMaking{ private ArrayList&lt;GeneralAir&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;GeneralAir&gt; retrieve() { list.add(new GeneralAir(\"氧气\")); list.add(new GeneralAir(\"二氧化碳\")); list.add(new GeneralAir(\"氮气\")); return list; } public void make() { retrieve(); for(GeneralAir gAir: list) { System.out.println(\"普通气体--\" + gAir.getType()); } }}class PoisonousAirMaking{ private ArrayList&lt;PoisonousAir&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;PoisonousAir&gt; retrieve() { list.add(new PoisonousAir(\"一氧化碳\")); list.add(new PoisonousAir(\"氨气\")); list.add(new PoisonousAir(\"二氧化硫\")); return list; } public void make(GeneralAirMaking sub) { retrieve(); for(PoisonousAir paAir:list) { System.out.println(\"有毒气体--\"+paAir.getType()); } System.out.println(\"---\"); sub.make(); }}class GeneralAir{ private String type; public GeneralAir(String type) { super(); this.type = type; } public String getType() { return type; } public void setType(String type) { this.type = type; }}class PoisonousAir{ private String type; public PoisonousAir(String type) { super(); this.type = type; } public String getType() { return type; } public void setType(String type) { this.type = type; }} 迪米特法则注意事项和细节 1) 迪米特法则的核心是*降低类之间的耦合 *2) 但是注意：由于每个类都减少了不必要的依赖，因此迪米特法则只是要求降低类间(对象间)耦合关系， 并不是要求完全没有依赖关系 合成复用原则（Composite Reuse Principle）基本介绍 原则是尽量使用合成/聚合的方式，而不是使用继承","link":"/post/7e3cde45.html"},{"title":"创建型设计模式","text":"单例模式单例设计模式介绍 所谓类的单例设计模式，就是采取一定的方法保证在整个的软件系统中，对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法(静态方法)。比如Hibernate的SessionFactory，它充当数据存储源的代理，并负责创建Session对象。 SessionFactory并不是轻量级的，一般情况下，一个项目通常只需要一个SessionFactory就够，这是就会使用到单例模式。 123456789单例模式有八种方式： 1) 饿汉式(静态常量) 2) 饿汉式（静态代码块） 3) 懒汉式(线程不安全) 4) 懒汉式(线程安全，同步方法) 5) 懒汉式(线程安全，同步代码块) 6) 双重检查 7) 静态内部类 8) 枚举 饿汉式(静态常量) 构造器私有化 类的内部创建对象 向外暴露一个静态的公共方法12345678class Singleton { private Singleton() { } private final static Singleton instance = new Singleton(); public static Singleton getInstance() { return instance; }} 优缺点说明： 优点：这种写法比较简单，就是在类装载的时候就完成实例化。避免了线程同步问题。 缺点：在类装载的时候就完成实例化，没有达到Lazy Loading的效果。如果从始至终从未使用过这个实例，则会造成内存的浪费 这种方式基于classloder机制避免了多线程的同步问题，不过， instance在类装载时就实例化，在单例模式中大多数都是调用getInstance方法， 但是导致类装载的原因有很多种， 因此不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化instance就没有达到lazy loading的效果 结论：这种单例模式可用， 可能造成内存浪费 饿汉式（静态代码块）应用实例123456789101112class Singleton { private Singleton() { } private static Singleton instance; static { instance = new Singleton(); } public static Singleton getInstance() { return instance; }} 优缺点说明： 1) 这种方式和上面的方式其实类似，只不过将类实例化的过程放在了静态代码块中，也是在类装载的时候，就执行静态代码块中的代码，初始化类的实例。优缺点和上面是一样的。2) 结论： 这种单例模式可用，但是可能造成内存浪费 懒汉式(线程不安全)123456789101112class Singleton { private static Singleton instance; private Singleton() {} public static Singleton getInstance() { if(instance == null) { instance = new Singleton(); } return instance; }} 优缺点说明： 1) 起到了Lazy Loading的效果，但是只能在单线程下使用。2) 如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以在多线程环境下不可使用这种方式3) 结论：在实际开发中，不要使用这种方式 懒汉式(线程安全，同步方法)12345678910class Singleton { private static Singleton instance; private Singleton() {} public static synchronized Singleton getInstance() { if(instance == null) { instance = new Singleton(); } return instance; }} 优缺点说明： 1) 解决了线程不安全问题2) 效率太低了，每个线程在想获得类的实例时候，执行getInstance()方法都要进行同步。而其实这个方法只执行一次实例化代码就够了，后面的想获得该类实例，直接return就行了。方法进行同步效率太低3) 结论： 在实际开发中， 不推荐使用这种方式 懒汉式(线程安全，同步代码块)123456789101112class Singleton { private static Singleton instance; private Singleton() {} public static synchronized Singleton getInstance() { if(instance == null) { synchronized (Singleton.class) { instance = new Singleton(); } } return instance; }} 优缺点说明： 1) 这种方式，本意是想对第四种实现方式的改进，因为前面同步方法效率太低，改为同步产生实例化的的代码块2) 但是这种同步并不能起到线程同步的作用。跟第3种实现方式遇到的情形一致，假如一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例3) 结论：在实际开发中， 不能使用这种方式 双重检查1234567891011121314class Singleton { private static volatile Singleton instance; private Singleton() {} public static Singleton getInstance() { if(instance == null) { synchronized (Singleton.class) { if(instance == null) { instance = new Singleton(); } } } return instance; }} volatile关键字在Java中，实现变量共享，当一个变量被多个线程使用时，且有一个线程修改，会立刻刷新到主存中，已达到数据同步的效果 优缺点说明： 1) Double-Check概念是多线程开发中常使用到的， 如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。2) 这样，实例化代码只用执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象，也避免的反复进行方法同步.3) 线程安全；延迟加载；效率较高4) 结论：在实际开发中，推荐使用这种单例设计模式 静态内部类12345678910class Singleton { private static Singleton instance; private Singleton() {} private static class SingletonInstance { private static final Singleton INSTANCE = new Singleton(); } public static Singleton getInstance() { return SingletonInstance.INSTANCE; }} 优缺点说明： 1) 这种方式采用了类装载的机制来保证初始化实例时只有一个线程。2) 静态内部类方式在Singleton类被装载时并不会立即实例化，而是在需要实例化时，调用getInstance方法，才会装载SingletonInstance类，从而完成Singleton的实例化。3) 类的静态属性只会在第一次加载类的时候初始化，所以在这里， JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。4) 优点：避免了线程不安全，利用静态内部类特点实现延迟加载，效率高5) 结论：推荐使用 枚举12345enum Singleton { public void sayHello() { System.out.println(\"Hello\"); }} 优缺点说明： 1) 这借助JDK1.5中添加的枚举来实现单例模式。不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象。2) 这种方式是Effective Java作者Josh Bloch 提倡的方式3) 结论：推荐使用 单例模式在JDK中的应用 我们JDK中， java.lang.Runtime就是经典的单例模式(饿汉式) 单例模式注意事项和细节说明1) 单例模式保证了 系统内存中该类只存在一个对象，节省了系统资源，对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能2) 当想实例化一个单例类的时候，必须要记住使用相应的获取对象的方法，而不使用new3) 单例模式使用的场景：需要频繁的进行创建和销毁的对象、创建对象时耗时过或耗费资源过多(即：重量级对象)， 但又经常用到的对象、工具类对象、频繁访问数据库或文件的对象(比如数据源、 session工厂等)","link":"/post/fcd38b5a.html"},{"title":"54. Spiral Matrix","text":"54. Spiral Matrix题目 解题思路 直接模拟 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution { public List&lt;Integer&gt; spiralOrder(int[][] matrix) { ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int i=0,j=0; int n=matrix.length; if(n==0)return list; int m = matrix[0].length; int cnt = 0,loop=0; while(cnt&lt;m*n) { while(cnt&lt;n*m&amp;&amp;j&lt;m-loop) { list.add(matrix[i][j]); cnt++; j++; } j--; i++; while(cnt&lt;m*n&amp;&amp;i&lt;n-loop) { list.add(matrix[i][j]); i++; cnt++; } i--; j--; while(cnt&lt;m*n&amp;&amp;j&gt;=loop) { list.add(matrix[i][j]); cnt++; j--; } j++; i--; while(cnt&lt;m*n&amp;&amp;i&gt;loop) { list.add(matrix[i][j]); i--; cnt++; } i++; j++; loop++; //System.out.println(list); } return list; }} 整理后 1234567891011121314151617181920212223242526272829303132333435 public List&lt;Integer&gt; spiralOrder(int[][] matrix) { ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int i=0,j=0; int m=matrix.length; if(m==0)return list; int n = matrix[0].length; int cnt = 0,loop=0; while(cnt&lt;m*n) { i=loop; j=loop; for(int k=j; k&lt;n-loop;k++,cnt++) { list.add(matrix[i][k]); } j += n-loop-1; for(int k=i+1;k&lt;m-loop;k++,cnt++) { list.add(matrix[k][j]); } i += m-loop-1; for(int k=j-1; k&gt;=loop;k--,cnt++) { list.add(matrix[i][k]); } j = loop; for(int k=i-1;k&gt;loop;k--,cnt++) { list.add(matrix[k][j]); } loop++; //System.out.println(list); } return list; } 直接使用一个循环 12345678910111213141516171819202122232425262728 public List&lt;Integer&gt; spiralOrder(int[][] matrix) { ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int i=0,j=0; int m=matrix.length; if(m==0)return list; int n = matrix[0].length; int[][] dirs = {{0,1},{1,0},{0,-1},{-1,0}}; int idx = 0; for(int k=0;k&lt;m*n;k++) { list.add(matrix[i][j]); matrix[i][j] = 1000; int x = i + dirs[idx][0]; int y = j + dirs[idx][1]; if(x&lt;0 || y&lt;0 || x&gt;=m || y&gt;=n || (matrix[x][y]^1000)==0) { idx = (idx + 1)%4; x = i + dirs[idx][0]; y = j + dirs[idx][1]; } i=x; j=y; } return list; }","link":"/post/f8e18060.html"},{"title":"Leetcode/55. Jump Game","text":"55. Jump Game题目 解题思路 判断是否到达或者不能到达 继续遍历 如果当前位置到当前位置可到达最远位置之间，所有点都满足这种情况，就跳到可以跳到的最远位置 不满足，则找最后一个满足的点，这点就是下一步的点 12345678910111213141516171819202122class Solution { public boolean canJump(int[] nums) { return jump(nums, 0); } private boolean jump(int[] nums, int current) { if(current+nums[current]&gt;=nums.length-1) { return true; } if(nums[current]==0)return false; //找最大 int maxi = current+1; while(maxi+nums[maxi]&lt;current+nums[current])maxi++; if(maxi&lt;current+nums[current]) { return jump(nums, maxi); }else { return jump(nums, current+nums[current]); } }}","link":"/post/630c2358.html"},{"title":"51. N-Queens","text":"51. N-Queens题目 解题思路 根据上图的方式是否位于同一对角线 开始按行遍历，在此过程性需要定义一个数组标明是否在对角线，列上已有皇后 这也就是回朔法，先试着走，不合适回退 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution { boolean[][] visited; private List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;List&lt;String&gt;&gt;(); public List&lt;List&lt;String&gt;&gt; solveNQueens(int n) { List&lt;StringBuffer&gt; list = new ArrayList&lt;StringBuffer&gt;(); visited = new boolean[3][n*2]; for(int i=0; i&lt;n;i++) { StringBuffer sbuf = new StringBuffer(); for(int j=0;j&lt;n;j++){ sbuf.append('.'); } list.add(sbuf); } search(list, 0, n); return res; } public void search(List&lt;StringBuffer&gt; list, int cur, int n) { if(cur==n) { ArrayList&lt;String&gt; list2 = new ArrayList&lt;&gt;(); for(StringBuffer sb: list) { list2.add(sb.toString()); } res.add(list2); return ; }else { for(int i=0;i&lt;n;i++) { //System.out.println(cur-i+n+\";;\"+visited[0].length); if(!visited[0][i]&amp;&amp;!visited[1][cur+i] &amp;&amp;!visited[2][cur-i+n]) { list.get(cur).setCharAt(i, 'Q'); visited[0][i] = visited[1][cur+i] = visited[2][cur-i+n]=true; search(list, cur+1, n); list.get(cur).setCharAt(i, '.'); visited[0][i] = visited[1][cur+i] = visited[2][cur-i+n] = false; } } } }} 更快速的方法参考链接","link":"/post/7040787d.html"},{"title":"49. Group Anagrams","text":"49. Group Anagrams题目 解题思路 把相应的字符串转为字符数组，然后排序，并将该数组转为字符串和当前数据种类的索引存入map 没遍历一个查看map是否已存在索引，不存在创建一个List并存进去 存在则获得，并将对应的字符串存入获得的List中 12345678910111213141516171819202122232425262728293031323334353637public class Solution { public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) { List&lt;List&lt;String&gt;&gt; list = new ArrayList&lt;List&lt;String&gt;&gt;(); Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); for(int i=0;i&lt;strs.length;i++) { String current = sortString(strs[i]); Integer index; if((index = map.get(current))==null) { ArrayList&lt;String&gt; list2 = new ArrayList&lt;&gt;(); map.put(current, list.size()); list2.add(strs[i]); list.add(list2); }else { //System.out.println(strs[i]+\",index=\"+index); List&lt;String&gt; list2 = list.get(index); list2.add(strs[i]); } } return list; } public String sortString(String str) { char[] cs = str.toCharArray(); Arrays.sort(cs); return Arrays.toString(cs); } public static void main(String[] args) { String[] input = {\"eat\",\"tea\", \"tan\", \"ate\", \"nat\", \"bat\"}; System.out.println(new Solution().groupAnagrams(input)); }}","link":"/post/7ed42f60.html"},{"title":"56. Merge Intervals","text":"56. Merge Intervals题目 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 public int[][] merge(int[][] a) { //1. 先去除a为空的情况 if(a.length==0)return new int[][]{}; Integer[][] intervals = new Integer[a.length][a[0].length]; //2. 为了方便排序，将其转换为Integer数组 cpy(a, intervals); //3. 排序 Arrays.sort(intervals, new Comparator&lt;Integer[]&gt;() { @Override public int compare(Integer[] x, Integer[] y) { if(x[0] &lt; y[0]){ return -1; } else if(x[0] &gt; y[0]){ return 1; } else { return 0; } }});//4. 遍历查找是否相交 ArrayList&lt;Integer[]&gt; list = new ArrayList&lt;&gt;(); for(int i=0;i&lt;intervals.length;i++) { int j = i; Integer[] tIntegers = new Integer[2]; tIntegers[0] = intervals[i][0]; tIntegers[1] = intervals[i][1]; //有相交的情况 while(i+1&lt;=intervals.length-1&amp;&amp;intervals[i+1][0]&lt;=tIntegers[1]) { if(intervals[i+1][1]&gt;tIntegers[1]) { tIntegers[1] = intervals[i+1][1]; } i++; } list.add(tIntegers); } //5. 转换为int[][]返回 int[][] res = new int[list.size()][2]; for(int i=0;i&lt;list.size();i++) { Integer[] integers = list.get(i); res[i][0] = integers[0]; res[i][1] = integers[1]; }return res; }","link":"/post/15fe401e.html"},{"title":"43. Multiply Strings","text":"43. Multiply Strings（大数乘法）思路 模拟乘法运算即可 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556StringBuffer res = new StringBuffer();public String multiply(String num1, String num2) { if(num1.charAt(0)=='0'||num2.charAt(0)=='0') { return \"0\"; } //为了减少计算，把长度比较小的作为乘数 String tmp; if(num1.length()&lt;num2.length()) { tmp = num1; num1 = num2; num2 = tmp; } for(int i = num2.length()-1; i &gt;= 0; i--) { int j = num2.charAt(i)-'0'; while(j&gt;0) { plus(num1, num2.length()-i-1); j--; } } return res.toString();}public void plus(String num1, int k) { //System.out.println(\"k=\"+k+\"res=\"+res.toString()); if(res.length()==0) { res.append(num1); for(int i=0;i&lt;k;i++)res.append('0'); return; } int quotient = 0; int i,j; //遇到的错误② for(i=res.length()-k-1, j=num1.length()-1; i&gt;=0||j&gt;=0; i--,j--) { int sum = (i&lt;0?0:(res.charAt(i)-'0')) + (j&lt;0?0:(num1.charAt(j)-'0'))+quotient; quotient = sum/10; if(i&gt;=0) { res.setCharAt(i, (char) (sum%10 + '0')); }else { res.insert(0, sum%10); } //System.out.println(\"quotient=\"+quotient+\"i=\"+i+\"sum=\"+sum+\"res=\"+res.toString()); //System.out.println((char) (sum%10 + '0')); } if(quotient!=0) { res.insert(0, quotient); } //System.out.println(\"k=\"+k+\"res:\"+res.toString());} 遇到的问题 忽略num1=”0”或num2=”0”的情况 进位错误，原因是在标注2的位置的一部分放到外面导致进位错误 还有一个错误是字符与数字之间的转换，在StringBuffer中可以直接插入整数，无须转换为字符","link":"/post/2c9200eb.html"},{"title":"42. Trapping Rain Water","text":"42. Trapping Rain Water 题目地址 图解解题思路 首先往左边寻找比当前位置高度的的，往左边寻找时在未遇到比当前高时，遇到相等的表示之前已经找不，就不需要在去找了，因为之前已经找过，以下代码是从左开始遍历的 在往右边寻找比当前位置大的 在寻找过程中，计算宽度，高度使用找到的两边的最小的高度减去当前的高度，结果加上该宽度和高度的乘积 在寻找过程中，需要注意越界问题 12345678910111213141516171819202122232425262728293031323334class Solution { public int trap(int[] height) { int res = 0; for(int i=1; i&lt;height.length-1; i++){ int tmp=1; int lft=i-1; int rht=i+1; boolean isRepeated = false; while(lft&gt;=0&amp;&amp;height[lft]&lt;=height[i]){ if(height[lft]==height[i]){ isRepeated = true; break; } tmp++; lft--; } if(isRepeated)continue; while(rht&lt;height.length&amp;&amp;height[rht]&lt;=height[i]){ tmp++; rht++; } if(lft&gt;=0&amp;&amp;rht&lt;height.length){ //System.out.println(i+\" \"+ tmp +\" \" + height[lft]+\" \" + height[rht]); res += tmp*(Math.min(height[lft],height[rht])-height[i]); } } return res; }}","link":"/post/5b95307d.html"},{"title":"Leetcode/leetcode45","text":"45. Jump Game II参考题目 12345678910111213141516class Solution { public int jump(int[] nums) { int dept=0; int cursor = 0,i=0; while(cursor&lt;nums.length-1) { dept++; int limit = cursor; for(;i&lt;=limit;i++) { cursor = Math.max(cursor, i+nums[i]); } } return dept; }}","link":"/post/c5f1a5de.html"},{"title":"41. First Missing Positive","text":"https://leetcode.com/problems/first-missing-positive/ 41. First Missing Positive1234567891011121314151617181920212223242526272829303132public int firstMissingPositive(int[] nums) { if(nums.length==0)return 1; else if(nums.length==1){ if(nums[0]&lt;=0)return 1; return nums[0]&gt;1?1:2; } Arrays.sort(nums); //没有1的情况 boolean flag = false; for(int i=0;i&lt;nums.length;i++) { if(nums[i]==1) { flag = true; } } if(!flag)return 1; //有1的情况 for(int i =0;i&lt;nums.length;i++) { if(nums[i]&lt;=0)continue; //去重 while(i+1&lt;nums.length&amp;&amp;nums[i]==nums[i+1])i++; //中间有不连续的 if(i+1&lt;nums.length&amp;&amp;nums[i]+1!=nums[i+1]) { return nums[i]+1; } } return nums[nums.length-1]+1; }","link":"/post/c29c61c7.html"},{"title":"40. Combination Sum II","text":"40. Combination Sum II 题目地址 1234567891011121314151617181920212223242526272829303132333435363738394041List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] candidates, int target) { int[] tmp = new int[candidates.length]; //对candidates进行排序 Arrays.sort(candidates); dfs(candidates, target, 0, 0, tmp, 0); return list; } /** * 该实现使用暴力遍历，但遍历过程剔除一层中一样的数据，这样就满足了不重复 * * @param candidates 排序后的原始数组 * @param target 目标值 * @param k 当前的开始的第一个位置 * @param sum 遍历过的数据的和 * @param tmp 保存遍历过的数据 * @param dpth 记录遍历的深度 */ private void dfs(int[] candidates, int target, int k, int sum, int[] tmp, int dpth) { //System.out.println(dpth + \"--\"+sum+\"====\"+k); if(sum==target) { List&lt;Integer&gt; list2 = new ArrayList&lt;&gt;(); for(int i=0; i&lt;dpth;i++) { list2.add(tmp[i]); } //System.out.println(list2); list.add(list2); return ; } if(sum&gt;target) { return ; } for(int i = k; i&lt;candidates.length; i++) { tmp[dpth] = candidates[i]; dfs(candidates, target, i+1, sum+candidates[i], tmp, dpth+1); while(i+1&lt;candidates.length&amp;&amp;candidates[i]==candidates[i+1])i++; } }","link":"/post/b59b5151.html"},{"title":"44. Wildcard Matching","text":"44. Wildcard Matching(通配符匹配)参考资料 1234567891011121314151617181920 public boolean isMatch(String s, String p) { int i=0,j=0,iStar=-1,jStar=-1,pLen=p.length(),sLen=s.length(); while(i&lt;sLen) { if(j&lt;pLen&amp;&amp;(s.charAt(i)==p.charAt(j)||p.charAt(j)=='?')) { i++; j++;}else if(j&lt;pLen&amp;&amp;p.charAt(j)=='*'){ iStar = i; jStar = j++;}else if(jStar&gt;=0){ i = ++iStar; j = jStar+1;}else { return false;} } while(j&lt;pLen&amp;&amp;p.charAt(j)=='*')j++; return j==pLen; } [8, 2,4,4,4,9,5,2,5, 8,8,0,8,6,9, 1,1,6,3,5,1,2,6,6, 0,4,8,6,0,3 ,2,8,7,6,5 ,1,7,0,3,4, 8,3,5,9 ,0,4,0,1,0,5,9,2,0,7,0,2,1,0,8,2,5,1,2,3,9,7,4,7,0,0,1,8,5,6,7,5,1,9,9,3,5,0,7,5]","link":"/post/b2f69548.html"},{"title":"48. Rotate Image","text":"leetcode48题目 解题思路 新建一个等大小的数组，然后换一种形式为原先的数组赋值 1234567891011121314public void rotate(int[][] matrix) { int[][] tmp = new int[matrix.length][matrix.length]; for(int i=0;i&lt;matrix.length;i++) { for(int j=0;j&lt;matrix.length;j++) { tmp[i][j] = matrix[i][j]; } } for(int i=0;i&lt;matrix.length;i++) { for(int j=0;j&lt;matrix.length;j++) { matrix[j][matrix.length-i-1] = tmp[i][j]; } }}","link":"/post/bb40d963.html"},{"title":"47. Permutations II","text":"leetcode47题目 解题思路 分析：要求获得可重集全排列该题通过递归解决，实现时需要注意几点 同一层的重复元素不进行递归 查看是否已经使用完当前元素，如果全部使用完了，也不进行递归 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class Solution { private List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;List&lt;Integer&gt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) { Arrays.sort(nums); permute(nums,new int[nums.length], 0); return list; } private void permute(int[] nums,int[] A, int cur) { if(cur==nums.length) { //System.out.println(\"nums[cur]=\"+nums[cur]); list.add(toList(A)); }else { for(int i=0;i&lt;nums.length;i++) { if(i==0||nums[i]!=nums[i-1]) { int c1 = 0; int c2 = 0; for(int j = 0; j&lt;cur;j++) { System.out.println(\"cur=\"+cur+\",\"+A[j]); if(nums[i]==A[j])c1++; } for(int j=0;j&lt;nums.length;j++) { if(nums[i]==nums[j])c2++; } if(c1&lt;c2) { A[cur] = nums[i]; permute(nums, A, cur+1); //tmp.remove(i); } } } } } public List toList(int[] nums) { ArrayList&lt;Object&gt; arrayList = new ArrayList&lt;&gt;(); for(Integer e: nums) { arrayList.add(e); } return arrayList; } public static void main(String[] args) { int[] nums = {1,1,2}; System.out.println(new Solution().permuteUnique(nums).toString()); }}","link":"/post/2bffc4f2.html"},{"title":"软件测试方法","text":"等价类划分 属于黑盒测试，它将不能穷举的测试过程进行分类，从而保证完整性和代表性；思考步骤： 确定有效等价类和无效等价类 有效等价类划分（题目条件，还要注意边界值（极值），中间甲堕意找个值) 无效等价类划分（跟有效等价类相反，其他特殊的情况【中文、英文、特殊符号、空格、空】） 注意: 两个框要一个正确，一个错误，这样才能准确的判断；一定要根据需求来判断预期结 http://qukuailine.com/的注册页面 等价类细节考虑 1、考虑输入长度2、考库输入类型3、组成规则4、是否为空5、是否区分大小写…6、是否重复7、是否去除空格 边界值划分 如果输入条件规定了值得范围，则应取刚到到这个范围的边界值，以及刚刚超越这个范围边界的值作为输入数据。 两位整数加法器数的范围为99-99，则应测试99，-100和099,100 输入条件规定了值得个数 姓名要求1-20个字符，需要测试0、1、2个字符和19、20、21个字符 某商品信息查询系统，每页最多呈示10条商品信息，我们就应该准备商品信息，使能够查询出10条、11条、1条、0条商品记录 边界值和等价类区别：边界值分析不是从某等价类中随便挑一个作为代表，而是这个等价类的每个边界都要作为测试条件 银行密码案例 需求：密码必须是数字密码组合且长度在8-24 常见的边界值 文本框接收字符个数，比如用户名长度，密码长度等 报表的第1行和最后1行； 数值元素的第1个和最后1个； 循环的第1次、2次和倒数第1次、2次。 因果图法 因果图法是一种利用图解法分析输入的各种组合情况，从而设计测试用例的方法，它适合于检查程序输入条件的各种组合情况因果图法比较适合输入条件比较多的情况，测试所有的输入条件的排列组合。所谓的原因就是输入，所谓的结果就是输出。 12因果图的“因”—输入条件 因果图的“果”—输出结果因果图法要注意考虑： 所有输入/输出条件的相互制约关系以及组合关系输出结果对输入条件的依赖关系，也就是什么样的输入组合会产生怎样的输出结果，即“因果关系” 特点： 考虑输入条件的相互制约及组合关系 考虑输条件对输入条件的依赖关系因果图中的符号 恒等一有因就有果，没有因就没有果 非一有因没有果，没有因有果 或一条件有一个是真，结果就是真，条件都是假，结果才是假 且（与）-条件都为真，结果才为真因果图中的约束条件 E(互斥、排他)。a、b两个原因不会同时出现，最多只有一个出现。 I(包含、或)。a、b、c三个原因至少有一个出现。 O(唯一)。a、b两个原因必须有一个出现，且仅有一个出现。 R(需求)。a出现时b必定出现。 从结果方面考虑主要有1种约束条件： M(屏蔽)。a出现时，b必定不出现；a不出现时，b则不确定。 因果图导出测试用例利用因果图导出测试用例需要经过以下几个步骤 找出所有的原因，原因即输入条件或输入条件的等价类。 找出所有的结果，结果即输出条件。 明确所有输入条件之间的制约关系以及组合关系。 哪些条件不能组合到一起，哪些条件可以组合到一起 明确所有输出条件之间的制约关系以及组合关系。 哪些输出结果不能同时输出，哪些输出结果可以同时输出 找出什么样的输入条件组合会产生哪种输出结果 把因果图转换成判定表/决策表。 为判定表/决策表中的每一列表示的情况设计测试用例。 案例参考 交通一卡通案例 系统只接收50或100元纸币，一次只能使用一张纸币，一次充值金额只能为50元或100元 若输入50元纸币，并选择充值50元，完成充值后退卡，提示充值成功； 若输入50元纸币，并选择充值100元，提示输入金额不足并退回50元； 若输入100元纸币，并选择充值50元，完成充值后退卡，提示充值成功，找零50元； 若输入100元纸币，并选择充值100元，完成充值后退卡，提示充值成功； 若输入纸币后在规定时间内不选择充值按钮，退回输入的纸币，并提示错误； 若选择充值按钮后不输入纸币，提示错误 判定表判定表的组成 条件桩:问题的所有条件 动作桩:问题的所有输出 条件项:针对条件桩的取值 动作项:条件项的各种取值情况下的输出结果 相当于将输入改为条件装桩，输出改为动作桩 判定表法的步骤 列出所有条件桩和条件桩 填入条件项 填入动作项 简化判定表（合并相同或相似规则） 案例 怎样称为一个好学生？遵纪守法的前提下，学习成绩好是一个好学生、品德高尚也是一个好学生；（只要违法乱纪就绝对不是一个好学生；成绩和品德有一项，再加遵纪守法也是好学生）-代表任意值 场景法概述 场景法就是模拟用户操作软件时的场景，主要用于测试系统的业务流程。 当拿到一个测试任务时，我们不是先关注某个控件的边界值、等价类是否满足要求而是先要关注它的主要功能和业务流程是否正确实现，这就需要使用场景法来完成测试 当业务淘程测试没有问题，也就是该软件的主要功能没有问题时，我重点从边界值、等价 在冒烟测试时也主要采用场景法进行测试案例 用例场景的定义 场景法中两个重要的概念 基本流：按照正确的业务流程来实现的一条操作路径（模拟正确的操作流程 备选流：导致程序出现错误的操作流程（模拟错误的操作流程） 还有补充一些异常情况 用例场景是用来描述流经用例路径的过程，这个过程从开始到结束遍方用例中所有基本流和备选流。 场景法设计测试用例时，需要覆盖系统用例中的主成功场景和扩展场景 事件的触发时的情景便形成场景 同一事件的不同触发顺序和处理结果形成了事件流 案例 123456789使用场景法测试QQ登录功能。- 输入正确的账号和密码后点击“登录”按钮，程序能正常登录- 输入正确的账号，错误的密码后点击“登录”按钮，程序应给出错误提示- 输入正确的账号，不输入密码，点击“登录”按钮，程序应给出错误提示- 不输入账号和密码，直接点击“登录”按钮，程序给出错误提示“请您输入账号后登陆”- 不输入账号，输入正确的密码，点击“登录”按钮，程序应给出错误提示- 输入错误的账号，正确的密码，点击“登录”按钮，程序应给出错误提示（更多……）*********列出测试用例矩阵************* 流程分析法 流程分析法主要是针对测试场景类型属于流程测试场景的测试项下的测试子项进行设计，是从白盒测试设计方法中的路径覆盖分析法借鉴过来的一种方法。 在白盒测试中，路径就是指函数代码的某个分支组合，路径覆盖法需要构造足够的用例覆盖函数的所有代码路径。(就是测试所有分支) 在黑盒测试中，若将软件系统的某个流程看成路径的话，则可以针对该路径使用路径分析的方法设计测试用例 优点 降低了测试用例设计难度，只要搞清楚各种流程，就可以设计出高质量的测试用例来，而不需要太多测试方面的经验 在测试时间较迫的情况下，可以有的放矢的选择测试用例，而不用完全根据经来取舍流程分析法步骤 第一步：详细了解需求； 第二步：根据需求说明或界面原型，找出业务流程的各个页面以及各页面之间的流转关系； 第三步：画出业务流程（产品经理使用 Axure软件制作）； 第四步：写用例，覆盖所有的路径分支案例 流程分析法的总结 流程分析法适用于有先后顺序的测试:常用于业务流程、安装流程等流程测试没有问题并不能说明系统功能没有问题还需要针对每步功能进行测试。对于包含复杂流程的系统，只有功能点和处理流程都进行测试覆盖，才算是比较充分的测试。 错误推测法 错误推测法是指利用直觉和经验猜测出出错的可能类型，有针对性列举出程序中所有可能的错误和容易发生错误的情况，它是测试经验丰富的测试人员喜欢使用的一种测试用例设计方法 正交表 正交试验设计是研究多因素多水平的一种设计方法，它是根据正交性从全面试验中挑选出部分有代表性的点进行试验，这些有代表性的点具备了“均匀分散，齐整可比”的特点，正交试验设计是一种基于正交表的、高效率、快速、经济的试验设计方法 正交表的概述正交表：一种特制的表，一般的正交表记为：Ln（mk） n是表的行数，也就是需要测试组合的次数+ K是表的列数，表示控件的个数（因素的个数，或因子个数） m是每个控件包含的取值个数（各因素的水平数，即各因素的状态数）如：L9(34） 有4个控件 每个控件有3个取值 9为需要测试的组合个数 叫4因素3水平常用正交表 正交表使用步骤 根据所测程序千控件的个数（因素）以每个控件的取值个数（水平），选取适的正交排 把控件及其取值列举出来，并对其进行编号 把控件及其取值映射到正交排列表中 把正交排列表中的ABCD（因子）分别替换成4个控件 把每列中的1.2.3（状态）分别换成这个控件的3个取值（水平），排列顺序要按照表中给出的顺序 根据映射好的正交排列表编写测试用例 使用正交排列法的局限性 目前常见的正交排列表只有前面附录文件中给出的几种即使是已有的正交排列表，基本都要求每个控件中取值的个数要相等，这在实际软件中很少遇到。 没有现成的正交排列表怎么办？ 通过正交排列法的学习，我们更多的应该学习到一种测试思想，也就是在从所有组合集合中选取测试数据时，应该均匀的选取其中的组合作为测试用例，而不要只在某个局部选取数据。 混合正交表正交表生成工具Allpairs很对多情况下无法找到合适的正交表，就要使用正交表生成工具使用步骤 1234567891、制作取值表（只列出数据即可，不用编号）2、复制取值表的数据，放到文本文档中保存 （注意不要更改任何格式，例如文件叫Test2.txt）3、把文本文档放在 allpairs文件夹中4、 win+r后输入cnd进入控制台5、进入 allpairs文件夹6、在控制台中输入 allpairs. exe Test2.txt&gt;Test21.txt （Test21是自己起的名字，用来存放生成的组合用例， 可以自动生成，不必提前建好） 测试方法的选择在确定测试方法时，应遵循以下原则 根据程序的重要性和一旦发生故障将造成的损失来确定测试等级和测试重点。 认真选择测试策略，以便能尽可能少的使用测试用例，发现尽可能多的程序错误。因为一次完整的软件测试过后，如果程序中遗留的错误过多并且严重，则表明该次测试是不足的，而测试不足则意味着让用户承担隐藏错误带来的危险，但测试过度又会带来资源的浪费。因此测试需要找到一个平衡点。 （1）拿到一个测试任务时，先关注它的主要功能和业务流程、业务逻辑是否正确实现，考虑使用场景法。 （2）需要输入数据的地方，考虑采用等价类划分法，包括输入条件和输出条件的等价划分，将无限测试变成有限测试。 （3）在任何情况下都必须采用边界值分析法。这种方法设计出的测试用例发现程序错误的能力最强。 （4）如果程序的功能说明中含有输入条件的组合情况，则一开始就应考虑选用因果图和判定表法。 （5）对于参数配置类的软件，需要考虑参数之间的组合情况，考虑使用正交排列法选择较少的组合方式（最少的测试用例获得最大的的测试覆盖率）。（6）对照程序逻辑，检查已设计出的测试用例的逻辑覆盖程度。如果没有达到要求的覆盖标准，则应当再补充更多的测试用例。（7）采用错误推断法再追加测试用例—依靠测试工程师的经验和智慧。 如何选择？ 如果测试功能和流程，要使用场景法 需要输入数据的地方，我们使用等价类划分法，要注意配置边界值划分来测试 如果有条件组合的情况，我们需要使用因果图法制作判定表 配置类软件，组合比较多的。我们使用正交表来科学的选择测试用例 如果没有达到覆盖标准，就增加一些测试用例 依靠经验追加一些测试用例（错误推断法）","link":"/post/bd735dfe.html"},{"title":"Spring AOP","text":"AOP(Aspect Oriented Programming) 面向切面编程：基于OOP(Object Oriented Programing基础之上的编程思想)指在程序运行期间，将某段代码动态的切入到指定的位置进行运行的编程方式 场景：计算器运行计算方法时进行日志记录 直接写在方法内部，不推荐，修改维护麻烦 日志记录：辅助功能 业务逻辑：核心功能 耦合 我们希望的是： 业务逻辑（核心功能）；日志功能在运行期间，自己动态加上 运行的时候日志加上 可以使用 动态代理 动态代理的问题 当没有实现任何接口时无法使用动态代理，代理对象和被代理对象唯一能产生的关联就是实现了同一接口 实现起来困难 Spring AOP功能实现，底层使用动态代理 利用Spring一句代码都不写的去创建代理 实现简单，而且额没有强制目标对象必须实现接口将某段代码 动态的切入(不把日志代码写死在业务逻辑方法中) 到 指定方法(加减乘除) 的 指定的位置(方法开始、结束…) 进行运行的编程方式 Spring AOP的专业术语 如何将LoggerUtils这个类（切面类）中的通知方法动态的在目标方法运行的各个位置切入 将动态代理的装换为Spring AOP 导包 1234567891011121314核心包 commons-logging-1.1.3.jar spring-aop-4.0.0.RELEASE.jar spring-beans-4.0.0.RELEASE.jar spring-context-4.0.0.RELEASE.jar spring-core-4.0.0.RELEASE.jar spring-expression-4.0.0.RELEASE.jar切面编程 spring-aspects-4.0.0.RELEASE.jar增强版切面编程（即使目标对象没有实现任何接口也能创建代理）下载地址https://github.com/lyhcc/resourses/tree/master/Spring/extra com.springsource.net.sf.cglib-2.2.0.jar com.springsource.org.aopalliance-1.0.0.jar com.springsource.org.aspectj.weaver-1.6.8.RELEASE.jar 写配置 将目标类和切面类（封装了通知方法（在目标方法这行前后直线的方法））加入IOC容器中 告诉Spring哪个是切面类@Aspect 告诉Spring，里面的通知方法什么时候运行 123456789101112131415161718try{ @Before meethod.invoke @AfterReturning}catch(e){ @AfterThrowing}finally{ @After}@Before 目标方法运行之前 前置通知@After 目标方法运行之后 后置通知@AfterReturning 在目标方法正常返回之后 返回通知@AfterThrowing 目标方法跑出异常时 异常通知@Around 环绕 环绕通知 指定在哪个方法运行execution(访问权限 返回值 方法签名)@Before(&quot;execution(方法签名)&quot;) 开启基于注解的AOP12&lt;context:component-scan base-package=&quot;xzy.lyhcc&quot;&gt;&lt;/context:component-scan&gt;&lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt; AOP细节细节一 IOC容器保存的是代理对象 AOP的底层就是动态代理，容器保存的组件就是代理对象$Proxy16，并不是本类的类型 12345678Calculator calculator = ioc.getBean(Calculator.class);calculator.add(10, 2);System.out.println(calculator);System.out.println(calculator.getClass());/* xyz.lyhcc.calculate.MathCalculator@7f485fda class com.sun.proxy.$Proxy16*/ 在Spring中如果结果加上组件注解，不会创建对象只是告诉ioc容器中可能有这种类型的组件 细节二 cglib为没有接口的类创建代理对象 没有接口，cglib帮我们创建代理对象， 1class xyz.lyhcc.calculate.MathCalculator$$EnhancerByCGLIB$$59ae67da 细节三 切入点表达式（通配符）固定格式 execution(访问权限符 返回值类型 方法全类名(参数表)) 通配符 * 1) 匹配一个或者多个字符 execution(public int xyz.lyhcc.ab.MyClass.(int, int)) 2) 匹配任意一个参数 execution(public int xyz.lyhcc.ab.MyClass.(int, )) 3) 只能匹配一层路径 4) 权限位置不能表示， 表示任意时不写即可 .. 1) 匹配任意多个参数，任意类型参数execution(public int xyz.lyhcc.ab.MyClass.(..)) 2) 匹配任意多层路径： execution(public int xyz.lyhcc..MyClass.(int, int)) &amp;&amp;、||、！ 1) &amp;&amp; 切入的位置满足这两边的表达式 2) || 切入点满足两边的一个 3) ！ 只要不是当前的就行 123记住两种： 最精确的：execution(public int xyz.lyhcc.ab.MyClass.*(int, int)) 最模糊的：execution(* *(..)), 别写 细节三 通知方法的执顺序 正常执行：@Before –&gt;@After –&gt; @AfterReturning 异常执行：@Before –&gt;@After –&gt; @AfterThrowing 细节四 jointPoint获取通知方法的信息（参数方法名获取等）1) 只需为通知方法参数列表上写一个参数 12345org.aspectj.lang.JoinPointJointPoint 封装了目标方法的详细信息 joinPoint.getSignature().getName()joinPoint.getArgs() 细节五 通知方法返回值获取、异常获取 在通知方法的参数列表中添加一个参数， 需要告诉Spring这个参数是用来接收返回值的 12345@AfterReturning(value=\"execution(public int xyz.lyhcc.calculate.MathCalculator.*(int, int))\",returning=\"result\") public static void logFinished(JoinPoint joinPoint, Object result)@AfterThrowing(value=\"execution(public int xyz.lyhcc.calculate.MathCalculator.*(int, int))\",throwing=\"exception\") public static void logError(JoinPoint joinPoint, Exception exception) 细节六 Spring对通知方法的要求 Spring 对通知方法要求不严格 就算是有返回值，私有的方法也能正常执行唯一的要求是参数不能乱写&emsp;通知方法是Spring利用反射调用的，每次方法调用得确定这个方法的参数表的值，参数表的参数，Spring都得知道是什么JointPoint认识不知道的参数要告诉SpringException e ：指定通知方法可以接收哪些异常 细节七 抽取切入点表达式 在编写AspectJ切面时，可以直接在通知注解中书写切入点表达式。但同一个切点表达式可能会在多个通知中重复出现。 在AspectJ切面中，可以通过 @Pointcut注解 将一个切入点声明成简单的方法。切入点的 方法体通常是空 的，因为将切入点定义与应用程序逻辑混在一起是不合理的。 切入点方法的访问控制符同时也控制着这个切入点的可见性。如果切入点要在多个切面中共用，最好将它们集中在一个公共的类中。在这种情况下，它们必须被声明为public。在引入这个切入点时，必须将类名也包括在内。如果类没有与这个切面放在同一个包中，还必须包含包名。 其他通知可以通过 方法名称引入 该切入点 12345@Pointcut(value=\"execution(public int xyz.lyhcc.calculate.MathCalculator.*(int, int))\") public void myPointcut() {} @Before(\"myPointcut()\") public static void logStart(JoinPoint joinPoint) 细节八 环绕通知 环绕通知 （本身就是一个动态代理，四合一） 是所有通知类型中 功能最为强大 的，能够全面地控制连接点，甚至可以控制是否执行连接点。 对于环绕通知来说，连接点的 参数类型必须是ProceedingJoinPoint 。它是 JoinPoint的子接口，允许控制何时执行，是否执行连接点。 在环绕通知中需要明确调用ProceedingJoinPoint的 proceed()方法 （就相当于method.invoke()）来执行被代理的方法。如果忘记这样做就会导致通知被执行了，但目标方法没有被执行。 注意：环绕通知的方法 需要返回目标方法执行之后的结果 ，即调用 joinPoint.proceed();的返回值，否则会出现空指针异常。1234567891011121314151617 @Around(value=\"myPointcut()\") public static Object aroundMethod(ProceedingJoinPoint point) throws Throwable { System.out.println(\"环绕通知正在执行，当前运行的方法是\"+point.getSignature().getName()+\",参数是\"+Arrays.asList(point.getArgs())); try { proceed = point.proceed(); System.out.println(\"环绕通知：运行结果是\"+proceed); } catch (Exception e) { System.out.println(\"环绕通知：异常\"+e); throw new RuntimeException(e); }finally { System.out.println(\"环绕通知：方法运行完成\"); } return proceed; }} 细节九 环绕通知和普通通知同时存在，异常需要抛出普通通知才能接收到环绕通知优先于普通通知执行执行顺序： （环绕前置–&gt;普通前置）–&gt;目标方法执行–&gt;环绕返回或异常–&gt;环绕后置–&gt;普通后置–&gt;普通返回或异常（出现异常想要普通通知知道需要 抛异常 ） 细节十 多个切面的运行顺序 在同一个连接点上应用不止一个切面时，除非明确指定，否则它们的优先级是不确定的。 切面的优先级可以通过实现Ordered接口或利用@Order注解指定。 实现Ordered接口，getOrder()方法的返回值越小，优先级越高。 若使用@Order注解，序号出现在注解中 先进去的后出来环绕在哪个切面就在哪个切面的普通切面之前运行 AOP的应用场景1) AOP加日志到数据库2) AOP做权限验证3) AOP做安全检查4) AOP做事务控制 基于注解的AOP 基于注解的AOP","link":"/post/b8393840.html"},{"title":"Spring AOP引入之Java动态代理打印日志","text":"概述 使用动态代理使得在不影响打印日志的类的情况下进行日志打印，也就解耦业务逻辑和日志打印 代码 代码执行流程：Proxy-&gt;获取代理ProxyCalculator.getProxy，代理执行对应的方法，并打印日志 测试类ProxyTest123456789101112import xyz.lyhcc.calculate.Calculator;import xyz.lyhcc.calculate.MathCalculator;import xyz.lyhcc.proxy.ProxyCalculator;public class ProxyTest { @Test public void test() { Calculator proxy = (Calculator) ProxyCalculator.getProxy(new MathCalculator()); proxy.add(1,2); proxy.div(10, 2); }} 代理获取类ProxyCalculator1234567891011121314151617181920212223242526272829303132333435import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import xyz.lyhcc.calculate.Calculator;import xyz.lyhcc.logger.LoggerUtils;public class ProxyCalculator { public static Calculator getProxy(Calculator calculator) { ClassLoader loader = calculator.getClass().getClassLoader(); Class&lt;?&gt;[] interfaces = calculator.getClass().getInterfaces(); InvocationHandler h = new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object result = null; try { LoggerUtils.logStart(method, args); result = method.invoke(calculator, args); LoggerUtils.logFinished(method, result); } catch (Exception e) { LoggerUtils.logError(method, e); } return result; } }; Object proxy = Proxy.newProxyInstance(loader, interfaces, h); return (Calculator) proxy; }} 日志类 LoggerUtils123456789101112131415import java.lang.reflect.Method;import java.util.Arrays;public class LoggerUtils { public static void logStart(Method method, Object... args) { System.out.println(\"[\" + method.getName() + \" is running..] args=\" + Arrays.asList(args)); } public static void logFinished(Method method, Object result) { System.out.println(\"[\" + method.getName() + \" is Finished...] result=\" +result); } public static void logError(Method method, Exception e) { System.out.println(\"[\" + method.getName() + \" has errors] error is \" + e.getCause()); } } 计算器类和接口123456789101112131415161718192021222324252627282930public interface Calculator { public int add(int x, int y); public int sub(int x, int y); public int multi(int x, int y); public int div(int x, int y);}public class MathCalculator implements Calculator{ @Override public int add(int x, int y) { return x + y; } @Override public int sub(int x, int y) { return x - y; } @Override public int multi(int x, int y) { return x * y; } @Override public int div(int x, int y) { return x / y; }} 动态代理的问题 当没有实现任何接口时无法使用动态代理，代理对象和被代理对唯一能产生的关联就是实现了同一接口 实现起来困难","link":"/post/68c47b7a.html"},{"title":"Spring初始之HelloWorld","text":"概述 通过各种方式给容器中注册对象以前是自己new对象，现在交给容器创建；给容器中注册主键组件 框架编写流程 导包 下载https://blog.csdn.net/qq_34738667/article/details/79612958或使用maven的pom文件获取 方法https://github.com/spring-projects/spring-framework/wiki/Spring-Framework-Artifacts 12spring-beans-4.0.0.RELEASE.jar、spring-context-4.0.0.RELEASE.jar、spring-core-4.0.0.RELEASE.jar、spring-expression-4.0.0.RELEASE.jarSpring 运行时依赖的日志包 commons-logging-1.1.3.jar 写配置文件 Spring 配置文件中，集合了spring对的ioc容器管理的所有组件（会员清单） 创建一个Spring Bean Configuration File(Spring bean配置文件) 1234567891011121314151617181920212223 创建一个Persion类，属性有LastName Name gender email，生成getter/setter方法 在配置文件中创建一个Person对象，Spring会自动创建Person对象 一个bean标签创建一个组件 &lt;!-- 一个bean标签可以注册一个组件（对象、类） class 写要注册的组件的全类名 id 这个对象的唯一标识 --&gt;&lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"李四\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"王五\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"男\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abc@123.com\"&gt;&lt;/property&gt;&lt;/bean&gt; //在JUnit中测试 //ApplicationContext,代表ioc容器 //当前应用的xml配置文件在ClassPath下，ClassPathXmlApplicationContext ApplicationContext ioc = new ClassPathXmlApplicationContext(\"ioc.xml\"); Person bean = (Person) ioc.getBean(\"person01\"); System.out.println(bean); 测试 总结 注意： src，源码包开始的路径，称为类路径的开始 所有源码包里面的东西都会合并放到类路径里面Java Wen类开始 /WEB-INF/classes/Java /bin/ 导包commons-logging-1.1.3.jar 先导包在创建配置文件 Spring的容器接管标志了s的类 几个细节 ApplicationContext（IOC容器接口） new ClassPathXmlApplicationContext(“ioc.xml”); ioc配置文件在类路径下 new FileSystemXmlApplicationContext(“D://ioc.xml”); ioc容器配置文件在磁盘路径下 给容器中注册一个组件，我们也从容器中按照id拿到了这个组件的对象 组件的创建工作，是容器完成的 Person对象是在容器创建完成时就已经完成 同一个组件在ioc容器中是单例的，容器启动时创建完成 容器中如果没有这个组件，报异常NoSuchBeanDefinitionException ioc容器正在创建这个对象的时候，会利用setter方法为javaBean的属性进行赋值 JavaBean的属性名由什么决定？getter/setter方法决定的","link":"/post/f431a4c4.html"},{"title":"Spring JDBCTemplate","text":"概述 为了使JDBC更加易于使用，Spring在JDBC API上定义了一个抽象层，以此建立一个JDBC存取框架。作为Spring JDBC框架的核心，JDBC模板的设计目的是为不同类型的JDBC操作提供模板方法，通过这种方式，可以在尽可能保留灵活性的情况下，将数据库存取的工作量降到最低。可以将Spring的JdbcTemplate看作是一个小型的轻量级持久化层框架，和我们之前使用过的DBUtils风格非常接近。 环境准备导包1) IOC容器所需jar包 12345commons-logging-1.1.1.jarspring-beans-4.0.0.RELEASE.jarspring-context-4.0.0.RELEASE.jarspring-core-4.0.0.RELEASE.jarspring-expression-4.0.0.RELEASE.jar 2) jdbcTemplate所需的jar包 123spring-jdbc-4.0.0.RELEASE.jarspring-orm-4.0.0.RELEASE.jarspring-tx-4.0.0.RELEASE.jar 3) 数据库驱动和数据源 12c3p0-0.9.1.2.jarmysql-connector-java-5.1.7-bin.jar 3) 数据库基本信息文件db.properties 1234567891011jdbc.user=rootjdbc.password=rootjdbc.jdbcUrl=jdbc:mysql://192.168.37.101:3306/jdbc_templatejdbc.driverClass=com.mysql.jdbc.Driverjdbc.initialPoolSize=30jdbc.minPoolSize=10jdbc.maxPoolSize=100jdbc.vacquireIncrement=5jdbc.maxStatements=1000jdbc.maxStatementsPerConnection=10 4) Spring配置文件 1234567891011&lt;context:property-placeholder location=\"classpath:db.properties\"/&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"${jdbc.user}\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"${jdbc.password}\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"${jdbc.jdbcUrl}\"&gt;&lt;/property&gt; &lt;property name=\"driverClass\" value=\"${jdbc.driverClass}\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;constructor-arg name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 持久化操作 增删改JdbcTemplate.update(String, Object…)1String sql = &quot;UPDATE employee set salary=? WHERE emp_id=?;&quot;;int a = template.update(sql, 1345.00, 4); 批量增删改 Object[]封装了SQL语句每一次执行时所需要的参数 List集合封装了SQL语句多次执行时的所有参数JdbcTemplate.batchUpdate(String, List&lt;Object[]&gt;)1234567 JdbcTemplate bean = ioc.getBean(JdbcTemplate.class);String sql = \"INSERT INTO employee(emp_name,salary) VALUES(?,?)\";List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;();batchArgs.add(new Object[] {\"boby\",11120.00});batchArgs.add(new Object[] {\"GGBound\",10202.00});int[] res = bean.batchUpdate(sql, batchArgs);System.out.println(Arrays.asList(res)); 查询单行 封装成java对象返回JdbcTemplate.queryForObject(String, RowMapper, Object…)12String sql = \"SELECT emp_id, emp_name, salary from employee WHERE emp_id=?\";Object forObject = bean.queryForObject(sql, new BeanPropertyRowMapper(Employee.class), 5); 查询多行 RowMapper对象依然可以使用BeanPropertyRowMapperJdbcTemplate.query(String, RowMapper, Object…)12String sql = \"SELECT emp_id, emp_name, salary from employee WHERE salary&gt;?\";List query = bean.query(sql, new BeanPropertyRowMapper(Employee.class), 4000); 查询单一值 查询没结果时会报错JdbcTemplate.queryForObject(String, Class, Object…)12 String sql = \"SELECT MAX(salary) FROM employee\";Object forObject = bean.queryForObject(sql, Double.class); 使用具名参数的JdbcTemplate 关于具名参数 在Hibernate的HQL查询中我们体验过具名参数的使用，相对于基于位置的参数，具名参数具有更好的可维护性，在SQL语句中参数较多时可以考虑使用具名参数。在Spring中可以通过NamedParameterJdbcTemplate类的对象使用带有具名参数的SQL语句。 容器创建具名参数的JDBCTemplate对象 123&lt;bean id=\"namedParameterJdbcTemplate\" class=\"org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate\"&gt; &lt;constructor-arg name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 具名参数传入①通过Map对象传入 Map的键是参数名，值是参数值NamedParameterJdbcTemplate.update(String, Map&lt;String, ?&gt;) 12345 String sql = \"INSERT INTO employee(emp_name,salary) VALUES(:empName, :salary)\";Map&lt;String,Object&gt; paramMap = new HashMap&lt;&gt;();paramMap.put(\"empName\", \"cmp\");paramMap.put(\"salary\", 12345.0);bean.update(sql, paramMap); ②通过SqlParameterSource对象传入 123456 NamedParameterJdbcTemplate bean = ioc.getBean(NamedParameterJdbcTemplate.class);String sql = \"INSERT INTO employee(emp_name,salary) VALUES(:empName, :salary)\";Employee employee = new Employee();employee.setEmpName(\"asdasda\");employee.setSalary(\"123.123\");bean.update(sql, new BeanPropertySqlParameterSource(employee)); 数据库 1234567891011121314151617181920212223242526272829303132333435363738/*SQLyog Ultimate v9.20 MySQL - 5.1.37-community : Database - jdbc_tmeplate**********************************************************************//*!40101 SET NAMES utf8 */;/*!40101 SET SQL_MODE=''*/;/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;CREATE DATABASE /*!32312 IF NOT EXISTS*/`jdbc_template` /*!40100 DEFAULT CHARACTER SET gb2312 */;USE `jdbc_template`;/*Table structure for table `employee` */DROP TABLE IF EXISTS `employee`;CREATE TABLE `employee` ( `emp_id` int(11) NOT NULL AUTO_INCREMENT, `emp_name` char(100) DEFAULT NULL, `salary` double DEFAULT NULL, PRIMARY KEY (`emp_id`)) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=gb2312;/*Data for the table `employee` */insert into `employee`(`emp_id`,`emp_name`,`salary`) values (1,'Susan',5000.23),(2,'Julian',4234.77),(3,'Papu',9034.51),(4,'Babala',8054.33),(5,'Kasier',6039.11),(6,'Owen',7714.11);/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;","link":"/post/3812c119.html"},{"title":"Spring 初识","text":"框架介绍 框架是一种可重用代码的一种设计，具有高度可重用性（ 半成品 ）比如说，在设计一个书城项目：涉及到一些类，将它们打包为jar,再将其中一些jar抽取成高度可重用的；事务控制的，强大的Servlet，项目的一些工具框架：多个可重用模块的集合，形成能一个领域的整体解决方案 Spring 容器 可以用来管理所有 组件（类） 的框架核心:IOC AOP Spring是一个开源框架 Spring为简化企业级开发而生，使用Spring，JavaBean就可以实现很多以前要靠EJB（Enterprise JavaBean）才能实现的功能。同样的功能，在EJB中要通过繁琐的配置和复杂的代码才能够实现，而在Spring中却非常的优雅和简洁 Spring是一个IOC(DI)和AOP容器框架。 Spring的优良特性 非侵入式：基于Spring开发的应用中的对象可以不依赖于Spring的API 依赖注入：DI——Dependency Injection，反转控制(IOC)最经典的实现。 面向切面编程：Aspect Oriented Programming——AOP 容器：Spring是一个容器，因为它包含并且管理应用对象的生命周期 组件化：Spring实现了使用简单的组件配置组合成一个复杂的应用。在 Spring 中可以使用XML和Java注解组合这些对象。（可以导入部分包） 一站式：在IOC和AOP的基础上可以整合各种企业应用的开源框架和优秀的第三方类库（实际上Spring 自身也提供了表述层的SpringMVC和持久层的Spring JDBC）。 Spring插件安装 Spring 模块 Test: Spring 的单元测试模块 Core Container: 核心容器（IOC）；黑色的那部分代表这部分的功能有哪些jar包组成 1spring-beans-4.0.0.RELEASE.jar、spring-context-4.0.0.RELEASE.jar、spring-core-4.0.0.RELEASE.jar、spring-expression-4.0.0.RELEASE.jar AOP+Aspects（面向切面编程模块） 1spring-aop-4.0.0.RELEASE、spring-aspects-4.0.0.RELEASE.jar 数据访问模块 123spring-jdbc-4.0.0.RELEASE、spring-orm(Object Relation Mapping)-4.0.O)RELEASEspring-ox(xml)m-4.0.0.RELEASE、spring-jms-4.0.0.RELEASE(integration)spring-tX-4.0.0.RELEASE(事务) Web: Spring 开发web应用模块 1234spring- websocket(新的技术)-4.0.日. RELEASE、spring-web-4. e.0. RELEASE、和原生的web相关(servlet )spring- webmvc-4.0.0. RELEASE、开发web项目的(web)spring- webmvc-portlet-4. e.日. RELEASE (开发web应用的组件集成) 使用时只需要导入对应的包即可 开发Spring框架的应用，经常要写框架的配置文件，写起来复杂，我们需要提示eclipse安装需要安装插件 IOC (Iversion Of Control):控制反转 在应用程序中的组件需要获取资源时，传统的方式是组件主动的从容器中获取所需要的资源，在这样的模式下开发人员往往需要知道在具体容器中特定资源的获取方式，增加了学习成本，同时降低了开发效率。反转控制的思想完全颠覆了应用程序组件获取资源的传统方式：反转了资源的获取方向—-改由容器主动的将资源推送给需要的组件，开发人员不需要知道容器是如何创建资源对象的，只需要提供接收资源的方式即可，极大的降低了学习成本，提高了开发的效率。这种行为也称为查找的被动形式。 控制：资源获取方式 主动式：(要什么资源自给new) 1234BookServlet{ BookService bs = new BookService() AirPlane ap = new AirPlane(); //复杂对象的创建是一个比较庞大的工程} 被动式：资源的获取不是我们自己创建，而是交给组件创建 123456BookServlet{ BookService bs; public void test01(){ bs.checkout(); }} 容器： 管理所有组件（有功能的类），假设，BookSevlet受容器管理，BookService也受容器管理；容器可以自动的探查除那些组件（类）需要用到另一些组件（类），容器帮我们创建BookService对象，并把BookService对象赋值过去 主动的new资源变为被动的接受资源 IOC是一种思想，DI是一种实现DI:（Dependency injection） 依赖注入 IOC的另一种表述方式：即组件以一些预先定义好的方式(例如：setter 方法)接受来自于容器的资源注入。相对于IOC而言，这种表述更直接 容器能知道那个组件（类）运行的时候，需要另外一个类（组件）；容器通过反射的形式，容器通过反射的形式，将容器中准备的BookSevice对象注入（利用反射给属性赋值）到BookServlet中 只要是容器管理的组件，都能使用容器提供的强大功能 HelloWorld 实现HelloWorld 实现 IOC在Spring中的实现 在通过IOC容器读取Bean的实例之前，需要先将IOC容器本身实例化。 Spring提供了IOC容器的两种实现方式 (1)BeanFactory：IOC容器的基本实现，是Spring内部的基础设施，是面向Spring本身的，不是提供给开发人员使用的。(2)ApplicationContext：BeanFactory的子接口，提供了更多高级特性。面向Spring的使用者，几乎所有场合都使用ApplicationContext而不是底层的BeanFactory。 ApplicationContext的主要实现类 ClassPathXmlApplicationContext：对应类路径下的XML格式的配置文件 FileSystemXmlApplicationContext：对应文件系统中的XML格式的配置文件 在初始化时就创建单例的bean，也可以通过配置的方式指定创建的Bean是多实例的。 其他ConfigurableApplicationContext 是ApplicationContext的子接口，包含一些扩展方法 refresh()和close()让ApplicationContext具有启动、关闭和刷新上下文的能力。 WebApplicationContext &gt;专门为WEB应用而准备的，它允许从相对于WEB根目录的路径中完成初始化工作","link":"/post/e866659f.html"},{"title":"Spring 声明式事务","text":"事务概述 在JavaEE企业级开发的应用领域，为了保证数据的 完整性和一致性 ，必须引入数据库事务的概念，所以事务管理是企业级应用程序开发中必不可少的技术。 事务就是一组由于逻辑上紧密关联而合并成一个整体(工作单元)的多个数据库操作，这些操作 要么都执行，要么都不执行 。 事务的四个关键属性(ACID) 原子性 (atomicity)：“原子”的本意是“不可再分”，事务的原子性表现为一个事务中涉及到的多个操作在逻辑上缺一不可。事务的原子性要求事务中的所有操作要么都执行，要么都不执行。 一致性 (consistency)：“一致”指的是数据的一致，具体是指：所有数据都处于满足业务规则的一致性状态。一致性原则要求：一个事务中不管涉及到多少个操作，都必须保证事务执行之前数据是正确的，事务执行之后数据仍然是正确的。如果一个事务在执行的过程中，其中某一个或某几个操作失败了，则必须将其他所有操作撤销，将数据恢复到事务执行之前的状态，这就是回滚。 隔离性 (isolation)：在应用程序实际运行过程中，事务往往是并发执行的，所以很有可能有许多事务同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏。隔离性原则要求多个事务在并发执行过程中不会互相干扰。 持久性(durability)：持久性原则要求事务执行完成后，对数据的修改永久的保存下来，不会因各种系统错误或其他意外情况而受到影响。通常情况下，事务对数据的修改应该被写入到持久化存储器中。 拓展： 在大数据中的新型数据库中存在严格一致性 Spring 中的数据库管理 编程式事务管理 使用原生的JDBC API进行事务管理 [1]获取数据库连接Connection对象[2]取消事务的自动提交[3]执行操作[4]正常完成操作时手动提交事务[5]执行失败时回滚事务[6]关闭相关资源 评价 使用原生的JDBC API实现事务管理是所有事务管理方式的基石，同时也是最典型的编程式事务管理。编程式事务管理需要将事务管理代码嵌入到业务方法中来控制事务的提交和回滚。在使用编程的方式管理事务时，必须在每个事务操作中包含额外的事务管理代码。相对于核心业务而言，事务管理的代码显然属于非核心业务，如果多个模块都使用同样模式的代码进行事务管理，显然会造成较大程度的代码冗余。 声明式事务 大多数情况下声明式事务比编程式事务管理更好：它将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。事务管理代码的固定模式作为一种横切关注点，可以通过AOP方法模块化，进而借助Spring AOP框架实现声明式事务管理。Spring在不同的事务管理API之上定义了一个抽象层，通过配置的方式使其生效，从而让应用程序开发人员不必了解事务管理API的底层实现细节，就可以使用Spring的事务管理机制。Spring 既支持编程式事务管理，也支持声明式的事务管理 。 12345678910111213Spring 中的Around可以实现编程式事务获取数据库连接Connection对象 try{ 取消事务的自动提交 执行操作 正常完成操作时手动提交事务 } catch(){ 执行失败时回滚事务 }finally{ 关闭相关资源 } Spring 提供的事务管理器 Spring从不同的事务管理API中抽象出了一整套事务管理机制，让事务管理代码从特定的事务技术中独立出来。开发人员通过配置的方式进行事务管理，而不必了解其底层是如何实现的。Spring的核心事务管理抽象是PlatformTransactionManager。它为事务管理封装了一组独立于技术的方法。无论使用Spring的哪种事务管理策略(编程式或声明式)，事务管理器都是必须的。事务管理器可以以普通的bean的形式声明在Spring IOC容器中。事务管理器的主要实现 DataSourceTransactionManager：在应用程序中只需要处理一个数据源，而且通过JDBC存取。 JtaTransactionManager：在JavaEE应用服务器上用JTA(Java Transaction API)进行事务管理 HibernateTransactionManager：用Hibernate框架存取数据库 测试 需求测试数据库12345678910111213141516171819202122232425262728293031CREATE TABLE book ( isbn VARCHAR (50) PRIMARY KEY, book_name VARCHAR (100), price INT) ;CREATE TABLE book_stock ( isbn VARCHAR (50) PRIMARY KEY, stock INT, CHECK (stock &gt; 0)) ;CREATE TABLE account ( username VARCHAR (50) PRIMARY KEY, balance INT, CHECK (balance &gt; 0)) ;INSERT INTO account (`username`,`balance`) VALUES ('Tom',100000);INSERT INTO account (`username`,`balance`) VALUES ('Jerry',150000);INSERT INTO book (`isbn`,`book_name`,`price`) VALUES ('ISBN-001','book01',100);INSERT INTO book (`isbn`,`book_name`,`price`) VALUES ('ISBN-002','book02',200);INSERT INTO book (`isbn`,`book_name`,`price`) VALUES ('ISBN-003','book03',300);INSERT INTO book (`isbn`,`book_name`,`price`) VALUES ('ISBN-004','book04',400);INSERT INTO book (`isbn`,`book_name`,`price`) VALUES ('ISBN-005','book05',500);INSERT INTO book_stock (`isbn`,`stock`) VALUES ('ISBN-001',1000);INSERT INTO book_stock (`isbn`,`stock`) VALUES ('ISBN-002',2000);INSERT INTO book_stock (`isbn`,`stock`) VALUES ('ISBN-003',3000);INSERT INTO book_stock (`isbn`,`stock`) VALUES ('ISBN-004',4000);INSERT INTO book_stock (`isbn`,`stock`) VALUES ('ISBN-005',5000); 配置文件123456789101112131415161718192021&lt;context:component-scan base-package=\"xyz.lyhcc\"&gt;&lt;/context:component-scan&gt;&lt;context:property-placeholder location=\"classpath:db.properties\"/&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"${jdbc.user}\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"${jdbc.password}\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"${jdbc.jdbcUrl}\"&gt;&lt;/property&gt; &lt;property name=\"driverClass\" value=\"${jdbc.driverClass}\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;constructor-arg name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- 配置事务管理器 --&gt;&lt;bean id=\"tm\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;constructor-arg name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- 启动事务注解 --&gt;&lt;tx:annotation-driven transaction-manager=\"tm\"/&gt; &lt;!-- 在事务方法中添加@Transactional注解 --&gt; Spring事务参数 isolation Isolation 事务的隔离级别 propagation Propagation 事务传播行为 noRollbackFor Class[] 哪些异常事务不回滚 noRollbackForClassName String[] String全类名 rollbackFor Class[] 哪些异常事务需要回滚 rollbackForClassName String[] readOnly boolean 设置事务为只读事务 可以进行事务优化readOnly=true 加快查询速度，不管事务那一推操作， 是不对事务进行加锁，只有查询是被允许的 1org.springframework.dao.TransientDataAccessResourceException: PreparedStatementCallback; SQL [UPDATE book_stock SET stock=stock-1 WHERE isbn=?]; Connection is read-only. Queries leading to data modification are not allowed; nested exception is java.sql.SQLException: Connection is read-only. Queries leading to data modification are not allowed timeout int 超时，指定时长后终止事务，并回滚,跑出异常 1org.springframework.transaction.TransactionTimedOutException: Transaction timed out 异常分类： 运行时（非检查异常）：可以不用处理，默认会回滚 编译时异常（检查异常）：必须处理 默认不回滚 数据库事务并发问题 假设现在有两个事务：Transaction01和Transaction02并发执行。 脏读[1]Transaction01将某条记录的AGE值从20修改为30。 [2]Transaction02读取了Transaction01更新后的值：30。 [3]Transaction01回滚，AGE值恢复到了20。 [4]Transaction02读取到的30就是一个无效的值。不可重复读[1]Transaction01读取了AGE值为20。 [2]Transaction02将AGE值修改为30。 [3]Transaction01再次读取AGE值为30，和第一次读取不一致。幻读[1]Transaction01读取了STUDENT表中的一部分数据。 [2]Transaction02向STUDENT表中插入了新的行。 [3]Transaction01读取了STUDENT表时，多出了一些行。事务隔离级别 数据库系统必须具有隔离并发运行各个事务的能力，使它们不会相互影响，避免各种并发问题。 一个事务与其他事务隔离的程度称为隔离级别。 SQL标准中规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性就越好，但并发性越弱。 读未提交：READ UNCOMMITTED允许Transaction01读取Transaction02未提交的修改。 读已提交：READ COMMITTED要求Transaction01只能读取Transaction02已提交的修改。 可重复读：REPEATABLE READ确保Transaction01可以多次从一个字段中读取到相同的值， 即Transaction01执行期间禁止其它事务对这个字段进行更新。 串行化：SERIALIZABLE 一般不再使用 各个隔离级别解决并发问题的能力见下表 / 脏读 不可重复读 幻读 READ UNCOMMITTED 有 有 有 READ COMMITTED 无 有 有 REPEATABLE READ 无 无 有 SERIALIZABLE 无 无 无 MySql 在REPEATABLE READ隔离级别下任何问题都没有 各种数据库产品对事务隔离级别的支持程度 / Oracle MySQL READ UNCOMMITTED × √ READ COMMITTED √ √ REPEATABLE READ × √(默认) SERIALIZABLE √ √ Spring 基于XML实现事务隔离级别设置 在Spring 2.x事务通知中，可以在tx:method元素中指定隔离级别 注意： Spring 事务使用代理机制 隔离级别根据自己的业务进行调整 Spring事务的传播行为简介 当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。事务的传播行为可以由传播属性指定。Spring定义了7种类传播行为 测试 说明 REQUIRED传播行为 当bookService的purchase()方法被另一个事务方法checkout()调用时，它默认会在现有的事务内运行。这个默认的传播行为就是REQUIRED。因此在checkout()方法的开始和终止边界内只有一个事务。这个事务只在checkout()方法结束的时候被提交，结果用户一本书都买不了 REQUIRES_NEW传播行为 表示该方法必须启动一个新事务，并在自己的事务内运行。如果有事务在运行，就应该先挂起它。 补充 在Spring 2.x事务通知中，可以像下面这样在tx:method元素中设定传播事务属性。 总结： REQUIRED 搭顺风车 事务的属性继承于大事务 将之前的connection传递给这个方法使用 REQUIRES_NEW 开自己的车 事务属性可以任意调整 这个方法直接使用新的connection 事务控制（XML实现） 基于xml配置的事务，依赖tx和aop命名空间 Spring 中提供事务管理器（事务切面）配置这个事务管理器 配置事务方法 告诉Spring哪些方法是事务方法，（事务切面按照我们的切入点表达式去切入方法） 指明哪些方法是事务方法，切入点表达式只是说，事务管理器要切入这些方法，哪些方法需要加事务需要tx:method指定","link":"/post/e8c12e4b.html"},{"title":"Spring IOC实验","text":"实验1：通过IOC容器创建对象，并为属性赋值★HelloWorld 实现 实验2：根据bean的类型从IOC容器中获取bean的实例★ 如果IOC容器这个类型的bean有多个，查找会报错报错org.springframework.beans.factory.NoUniqueBeanDefinitionException 123456 /*Person bean = ioc.getBean(Person.class);System.out.println(bean);*/// 指定类之后不需要强转Person bean = ioc.getBean(\"person02\", Person.class);System.out.println(bean); 实验3： 通过构造器为bean的属性赋值（index,type属性介绍） 1234567891011121314151617 &lt;!-- 必须为Person创建全参构造 --&gt; &lt;!-- 方法一 --&gt;&lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;constructor-arg name=\"lastName\" value=\"KL\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=\"name\" value=\"KJJJL\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=\"gender\" value=\"F\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=\"email\" value=\"KL@123.com\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; &lt;!-- 方法二 --&gt; &lt;!-- 注意：下面的顺序必须按照构造函数的参数顺序，但可以使用index指定 --&gt;&lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;constructor-arg value=\"KL1\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg value=\"KJJ1JL\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg value=\"M\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg value=\"KLw@123.com\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; &lt;!-- 构造函数重载时，如果需要指定调用，可以使用type=\"java.lang.Integer\" --&gt; 通过p名称空间为bean赋值，导入p命名空间 1234567 &lt;!-- 名称控件是用来防止标签重复的 --&gt; &lt;!-- 1）导入命名空间 2）写带前缀的标签 --&gt; xmlns:p=\"http://www.springframework.org/schema/p\" &lt;!-- 遇到的问题：在对应的spring configuration xml中没有namespace选项，将文件关掉，点击explore上面找到对应的配置文件，右键-&gt;open with -&gt;spring config editor --&gt; &lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"p:gender=\"F\" p:email=\"abc@123.com\" p:lastName=\"AC\" p:name=\"CB\"&gt;&lt;/bean&gt; 实验4：正确的为各种属性赋值 测试使用null值 1234567 &lt;!-- 默认不赋值也为null --&gt;&lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\"&gt; &lt;!-- 复杂标签赋值 --&gt; &lt;null&gt;&lt;/null&gt; &lt;/property&gt;&lt;/bean&gt; 引用类型赋值（引用其他bean、引用内部bean）123456789101112131415161718192021222324252627282930&lt;!-- 为Person类增加一个Person属性，并生成getter/setteer方法 --&gt;&lt;!-- 引用外部 --&gt; &lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"李四\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"王五\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"男\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abc@123.com\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"person02\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"AS\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"QW\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"F\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abcds@123.com\"&gt;&lt;/property&gt; &lt;property name=\"person\" ref=\"person01\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;!-- 内部 说明：内部bean不能直接获取 --&gt; &lt;bean id=\"person02\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"AS\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"QW\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"F\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abcds@123.com\"&gt;&lt;/property&gt; &lt;property name=\"person\"&gt; &lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"李四\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"王五\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"男\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abc@123.com\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; 集合类型赋值（List、Map、Properties）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;!-- List --&gt; &lt;!-- Person添加friends属性 --&gt; &lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"李四\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"王五\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"男\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abc@123.com\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"person02\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"AS\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"QW\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"F\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abcds@123.com\"&gt;&lt;/property&gt; &lt;property name=\"friends\"&gt; &lt;list&gt; &lt;bean class=\"xyz.lyhcc.bean.Person\" p:lastName=\"AC\" p:email=\"as@123.com\"&gt;&lt;/bean&gt; &lt;ref bean=\"person01\"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt;&lt;!-- Map --&gt; &lt;!-- Person中添加map属性 --&gt; &lt;bean id=\"person02\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"AS\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"QW\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"F\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abcds@123.com\"&gt;&lt;/property&gt; &lt;property name=\"map\"&gt; &lt;!-- new LinkedHashMap --&gt; &lt;map&gt; &lt;entry key=\"A\" value=\"1\"&gt;&lt;/entry&gt; &lt;entry key=\"B\" value=\"2\"&gt;&lt;/entry&gt; &lt;entry key=\"D\" value=\"3\"&gt;&lt;/entry&gt; &lt;entry key=\"C\" value=\"4\"&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt;&lt;!-- Properties --&gt; &lt;!-- 在Person中添加Properties属性 --&gt; &lt;bean id=\"person02\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"AS\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"QW\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"F\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abcds@123.com\"&gt;&lt;/property&gt; &lt;property name=\"properties\"&gt; &lt;!-- new Properties() --&gt; &lt;props&gt; &lt;prop key=\"username\"&gt;WSX&lt;/prop&gt; &lt;prop key=\"passwd\"&gt;AS&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; util名称空间创建集合类型的bean 12345678&lt;!-- util名称空间创建集合的bean，方便引用 --&gt;&lt;util:map id=\"myMap\"&gt; &lt;entry key=\"A\" value=\"1\"&gt;&lt;/entry&gt; &lt;entry key=\"B\" value=\"2\"&gt;&lt;/entry&gt; &lt;entry key=\"C\" value=\"3\"&gt;&lt;/entry&gt; &lt;entry key=\"D\" value=\"4\"&gt;&lt;/entry&gt;&lt;/util:map&gt;&lt;util:list id=\"mylist\"&gt;&lt;/util:list&gt; 级联属性赋值(级联属性表示属性的属性) ====================工厂方式创建bean========== 实验5：配置通过静态工厂方法创建的bean、实例工厂方法创建的bean、FactoryBean★1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;!-- 使用Book类，里面有name,author,price三个属性，并生成getter/setter方法 --&gt;&lt;!-- bean的创建默认就是框架通过利用反射new出来的 --&gt;&lt;!-- 工厂模式：工厂帮我们去创建对象 AirPlane ap = AirPlaneFactory.getAirPlane(String airPlane) 静态工厂：工厂本身不要创建对象，通过静态调用 对象 = 工厂类.工厂方法（） 实例工厂：工厂本省不需要创建对象 对象 = （new 工厂类）工厂方法（） --&gt; &lt;!-- 静态工厂通过 class: 指定静态工厂的全类名 factory-method=“getAirPlane” 指定工厂方法 constructor-arg 进行传参 --&gt; &lt;!-- 工厂方法定义： public class BookStaticFactory { public static Book getBookInstance(String name) { System.out.println(\"静态工厂被调用...\"); return new Book(name); }} --&gt;&lt;bean id=\"book1\" class=\"xyz.lyhcc.factory.BookStaticFactory\" factory-method=\"getBookInstance\"&gt; &lt;constructor-arg name=\"name\" value=\"计算机网络\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; &lt;!-- 实例工厂 factory-method 指定这个实例工厂的工厂方法 factory-bean 指定当前对象黄建使用哪个工厂 1）先配置出实例化工厂对象 2）配置我们要创建的类使用哪个工厂 --&gt; &lt;!-- 工厂方法 public class BookFactory { public Book getInstance(String name) { System.out.println(\"实例工厂被调用\"); return new Book(name); }} --&gt;&lt;bean id=\"factory\" class=\"xyz.lyhcc.factory.BookFactory\"&gt;&lt;/bean&gt; &lt;bean id=\"book02\" class=\"xyz.lyhcc.bean.Book\" factory-bean=\"factory\" factory-method=\"getInstance\"&gt; &lt;constructor-arg value=\"操作系统\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt; 123456789101112131415161718192021222324252627282930313233343536&lt;!-- FactoryBean 是Spring规定的一个接口，只要这个接口的实现类， Spring都认为是一个工厂，Spring会自动调用工厂方法 1. 定义一个FactoryBean实现类 2. 在配置文件中注册 3. ioc容器启动时不会创建实例（无论单实例还是多实例） 4. FactoryBean 获取的时候才创建实例&lt;bean id=\"book03\" class=\"xyz.lyhcc.factory.MyFactoryBeanImpl\"&gt;&lt;/bean&gt;--&gt;public class MyFactoryBeanImpl implements FactoryBean&lt;Book&gt; { /** * 返回创建的对象 */ @Override public Book getObject() throws Exception { return new Book(UUID.randomUUID().toString()); } /** * 返回创建对象的类型 * spring自动创建这个方法来确认创建的对象是什么类型 */ @Override public Class&lt;?&gt; getObjectType() { return Book.class; } /** * 是否为单实例 */ @Override public boolean isSingleton() { return false; }} 实验6：通过继承实现bean配置信息的重用123456789&lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"李四\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"王五\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"男\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abc@123.com\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"person02\" class=\"xyz.lyhcc.bean.Person\" parent=\"person01\"&gt; &lt;property name=\"name\" value=\"SDTYFUGHJGJHGFGHJHG\"&gt;&lt;/property&gt;&lt;/bean&gt; 实验7：通过abstract属性创建一个模板bean12345678无法获取报异常 org.springframework.beans.factory.BeanIsAbstractException &lt;!-- abstract=\"true\", 声明这个bean只能被继承，不能被获取 --&gt; &lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\" abstract=\"true\"&gt; &lt;property name=\"lastName\" value=\"李四\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"王五\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"男\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abc@123.com\"&gt;&lt;/property&gt; &lt;/bean&gt; 实验8：bean之间的依赖1234&lt;!-- 默认是按照配置文件的顺序创建 --&gt;&lt;!-- 通过depends-on=\"\"改变创建顺序 创建Person之前先创建Person2--&gt; &lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\" depends-on=\"person02\"&gt;&lt;/bean&gt; &lt;bean id=\"person02\" class=\"xyz.lyhcc.bean.Person2\"&gt;&lt;/bean&gt; 实验9：测试bean的作用域，分别创建单实例和多实例的bean★123456789101112&lt;!-- bean的作用域：指定bean是否为单实例，默认单实例 --&gt;&lt;!-- 使用scope=\"\"指定 prototype：多实例 1）容器启动默认不会创建多实例 2）获取时被创建 3）每次获取时创建一个对象 singleton:单实例 1）在容器启动之前就已经创建好对象，保存在容器中 2）任何时候获取都是之前创建好的那个对象 request:在web环境下，同一次请求创建一个Bean实例（没用） session: 在web环境下，同一个会话中创建一个Bean实例（没用） --&gt; 实验10：创建带有生命周期方法的bean 生命周期：bean的创建到销毁 单例Bean, 容器启动的时候就会创建好，容器关闭也会销毁创建的bean 多实例bean, 获取的时候才创建自定义一些生命周期的方法，spring在创建或者销毁的时候就会调用指定的方法自定义初始化方法和销毁方法，可以抛任意异常，但不能有参数Bean的生命周期 单实例：构造器–&gt;初始化方法–&gt;（容器关闭）销毁方法 多实例：获取bean（构造器–&gt;初始化方法）–&gt;容器关闭不会调用bean的销毁方法 123&lt;bean id=\"book04\" class=\"xyz.lyhcc.bean.Book\" init-method=\"init\" destroy-method=\"destory\"&gt;&lt;/bean&gt;&lt;bean id=\"book04\" class=\"xyz.lyhcc.bean.Book\" scope=\"prototype\" init-method=\"init\" destroy-method=\"destory\"&gt;&lt;/bean&gt; 实验11：测试bean的后置处理器 Spring有一个接口（BeanPostProcessor）；后置处理器，可以在bean初始化前调用 初始化方法， 后置处理器可以干扰初始化无论bean是否有初始化方法，后置处理器都会默认有，并继续执行BeanPostProcessor 有两个抽象方法postProcessBeforeInitialization 初始化之前调用postProcessAfterInitialization 初始化之后调用 使用方法 编写后置处理器的实现类 将后置处理器注册在配置文件中 123456789101112131415161718192021// beanName是在bean配置文件中的idpublic class MyBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"postProcessAfterInitialization: [\"+beanName+\"]初始化即将开始调用...\"+bean); return bean; } @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(\"postProcessBeforeInitialization [\"+beanName+\"]初始化方法调用完成...\"+bean); //返回值是初始化之后的bean，返回的是什么，容器保存的就是什么 return bean; } }//注册后置处理器&lt;bean class=\"xyz.lyhcc.processor.MyBeanPostProcessor\"&gt;&lt;/bean&gt; 实验12：引用外部属性文件★123456789101112131415161718192021&lt;!-- 数据库连接池作为单实例是最好的，一个项目一个池，管理多个连接，连接从连接池中获取，可以让Spring创建连接池对象 --&gt;&lt;!-- ${jdbc.username}用户名 --&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql://localhost:3306\"&gt;&lt;/property&gt; &lt;property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 引入外部属性文件 --&gt;1. 将context命名空间引入xmlns:context=\"http://www.springframework.org/schema/context\"2. 引入配置文件&lt;context:property-placeholder location=\"classpath:dbconfig.properties\"/&gt;3. 使用 &lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"user\" value=\"${jdbc.username}\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"${jdbc.password}\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"${jdbc.url}\"&gt;&lt;/property&gt; &lt;property name=\"driverClass\" value=\"${jdbc.driver}\"&gt;&lt;/property&gt; &lt;/bean&gt; 注意： 使用时需要三个jar包， c3p0、mchange、mysql 123mchange-commons-java-0.2.11.jar mysql-connector-java-5.1.27.jarc3p0-0.9.5.2.jar org.springframework.beans.factory.BeanCreationException:出现这个问题是需要一个包，具体参考 实验13：基于XML的自动装配(自定义类型自动赋值) JavaBean（基本类型）（自定义类型是一个对象，这个对象可能存在容器中）自动装配： 123456789101112autowire=&quot;default/no&quot;, 不自动装配 一下的是按照某种规则自动赋值 byName 以属性名作为id去容器中找到这个组件，给其赋值，不存在则为null ioc.getBean(&quot;id&quot;)byType 以属性的类型作为依据去容器找这个组件, 如果容器中有多个这种类型的组件，会报错 找不到，返回null ioc.getBean(*.class)constructor 按照构造器进行赋值 1）先按照有参构造器的类型进行装配,没有装配null 2）按照类型找多个，参数名作为id继续匹配，找到就装配，否则null 3）不会报错，如果是一个列表就会将容器所有符合的对象装配进去 实验14：[SpEL(Spring Expression Language)测试I] 在SpEL中使用字面量 引用其他bean 引用其他bean的某个属性值 调用非静态方法 调用静态方法 使用运算符 12345678910111213&lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;!-- 字面量 和使用运算符 --&gt; &lt;property name=\"age\" value=\"#{12*3}\"&gt;&lt;/property&gt; &lt;!-- 引用其他bean --&gt; &lt;property name=\"person\" value=\"#{person02}\"&gt;&lt;/property&gt; &lt;!-- 应用其他bean的属性值 --&gt; &lt;property name=\"lastName\" value=\"#{person02.lastName}\"&gt;&lt;/property&gt; &lt;!-- 非静态方法的调用 --&gt; &lt;property name=\"name\" value=\"#{person02.getName()}\"&gt;&lt;/property&gt; &lt;!-- 静态方法调用#{T(类名).静态方法} --&gt; &lt;property name=\"email\" value=\"#{T(java.util.UUID).randomUUID().toString()}\"&gt;&lt;/property&gt; &lt;/bean&gt; 实验15：通过注解分别创建Dao、Service、Controller（控制器，控制网站跳转逻辑Servlet）★ 通过给bean上添加注解，可以快速的将bean加入ioc容器中Spring的四个注解添加任何一个注解都能快速将这个组件注册到ioc容器中 @Controller 控制器 我们推荐给控制层的组件（Servlet）添加这个注解 @Service 业务逻辑，我们推荐业务逻辑层的组件添加这个注解， @Resposity 给持久化层组件添加这个注解（Dao层） @Component 不属于以上几层组件 注意： Spring底层不会去验证你的这个组件，是否如你所说的是一个Servlet层或者Dao层组件使用注解快速添加组件到容器需要几步 添加注释（四个的一个） 告诉Spring，自动扫描加了注释的组件，依赖context命名空间 &lt;context:component-scan base-package=””&gt;base-package 指定扫描的基础包 把基础包及他下面所有加了注释的类，自动注册到ioc容器中 导入aop包之后才会支持注解，否则报异常org.springframework.beans.factory.BeanDefinitionStoreException: 使用注解加入到容器中的组件，和使用配置加入到容器中的㢟行为都是默认一样的 组件的id,默认就是类名首字母小写 @Repository(“bookdao123”) 组件的作用域，默认是单例 @Scope(value=”prototype”) 1&lt;!-- Dao层（数据库访问）、Service层（业务处理）、Web层（页面控制action类） --&gt; 实验16：使用context:include-filter指定扫描包时要包含的类使用需禁用默认规则use-default-filters=&quot;false&quot;1234&lt;context:component-scan base-package=\"xyz.lyhcc\" use-default-filters=\"false\"&gt; &lt;context:include-filter type=\"annotation\" expression=\"org.springframework.stereotype.Repository\"/&gt;&lt;/context:component-scan&gt; 实验17：使用context:exclude-filter指定扫描包时不包含的类type=&quot;annotation&quot; 指定排除规则，按照注解进行排除，标注了指定注解组件不需要 expression=&quot;&quot; 注解的全类名 如 org.springframework.stereotype.Controller type=&quot;assignable&quot; 排除某个具体的类 expression=&quot;&quot; 类的全类名 后面一般不使用 type=&quot;custom&quot; 自定义一个 org.springframework.core.type.filter.TypeFilter 里面只有一个match方法，该方法返回true就要 type=&quot;aspectj&quot; aspectj表达式 type=&quot;regex&quot; 还可以使用正则表达式12345&lt;context:component-scan base-package=\"xyz.lyhcc\"&gt; &lt;!-- 排除一些不需要的组件 --&gt; &lt;context:exclude-filter type=\"regex\" expression=\"\"/&gt;&lt;/context:component-scan&gt; 实验18：使用@Autowired注解实现根据类型实现自动装配★ 使用@Autowired注解，Spring会自动为这个属性赋值 首先进行告诉Spring自动扫描组件 &lt;context:component-scan base-package=”xyz.lyhcc”&gt; 使用@Autowired BookServlet.java 12345678910@Controllerpublic class BookServlet { @Autowired private BookService bookService; public void doGet() { System.out.println(\"保存数据中...\"); bookService.save(); }} BookDao.java 1234567@Repositorypublic class BookDao { public void saveBook() { System.out.println(\"保存图书...\"); }} BookService.java 1234567891011@Servicepublic class BookService { @Autowired private BookDao bookDao; public void save() { System.out.println(\"BookService正在调用DAO存储数据\"); bookDao.saveBook(); }} 说明： 先按照类型去找相应的组件： bookSevice = ioc.getBean(BookServlet.class) 找到一个就直接赋值 没找到，报异常 找到多个？装配上 按照变量名作为id,继续匹配， 匹配上？装配 没有匹配？报异常 （原因是按照变量名作为id进行匹配，@Qualifier可以指定id名） 找到装配上，没找到报异常 @Autowired标注的自动装配的属性默认一定装配上的，找到就装配，找不到就不装配 @Autowired(required=false) 实验19：如果资源类型的bean不止一个，默认根据@Autowired注解标记的成员变量名作为id查找bean，进行装配★实验20：如果根据成员变量名作为id还是找不到bean，可以使用@Qualifier注解明确指定目标bean的id★实验21：在方法的形参位置使用@Qualifier注解1234567891011//ElementType.CONSTRUCTOR, ElementType.FIELD, ElementType.METHOD, ElementType.ANNOTATION_TYPE//@Autowired可以放到构造方法上，变量上，方法上，注解上 /** * 方法上有@AutoWired的话 * 1）这个方法会在bean创建的时候自动运行 * 2）该方法的每一个形参都会自动注入 */ @Autowired public void doIt(@Qualifier(\"bookDao\") BookDao bookDao) { System.out.println(\"正在运行此方法----\"+bookDao); } 实验22：@Autowired注解的required属性指定某个属性允许不被设置见以上说明实验23：测试泛型依赖注入★ 与@Autowired等同的还有，@Resource,@Inject@Autowired: 最强大，离开Spring不能使用@Resource： java标准,扩展性更强, 如果 切换在其他的容器框架 仍可以使用@Inject ： EJB","link":"/post/40588fed.html"},{"title":"Spring和JavaWeb","text":"Spring和JavaWeb整合使用 Spring来控制事务（DAO[JDBCTemplate]） 所有组件AutoWired 管理数据库 整合步骤 导包 写配置 将所有组件加入容器中，并能正确获取 @Controller：Servlet层，目前不能标注在Servlet层 @Service: 业务逻辑层 @Repository Dao层 @Component 其他组件 每个组件之间自动装配 配置出声明式事务 事务管理器控制数据库连接池IOC容器创建和销毁要在合适的时机完成 123456项目启动{ ioc创建}项目销毁{ IOC销毁} 测试","link":"/post/b363e16d.html"},{"title":"SpringIOC总结","text":"IOC是一个容器，帮我们管理所有的组件 依赖注入 @autowired，自动赋值 某个组件要使用Spring提供的更多功能（IOC,AOP）必须加入到容器中 体会 容器启动，创建所有bean实例 Autowired字符装配是，是从容器中找到符合要求的bean ioc.getBean(“Book”)，也是从容器中找到这个bean 容器中包括了所有的bean 调试Spring的源码，容器到底是什么？ 其实是一个map 这个map中保存所有创建的bean，并提供外界获取功能 bean保存在能map中? 源码调试思路 从HelloWorld开始的，给Helloworld的每一个关键步骤打一个断点，进去看里面都做了什么工作， 如何知道具体是做什么的 翻译这个方法是干什么的 放行这个方法，看控制台，看Debug的每一个变量的变化 看方法注释","link":"/post/fb9e30c5.html"},{"title":"Leetcode/leetcode46","text":"46. Permutations题目 解题思路 先将一个数组从小到大排序 从有向左查找第一个递减的数 再次2找的的数右边查比该数大的最小的数 步骤2,3找到的数互换位置 步骤找到的数的后面的数，进行逆转（可以进行排序O(nlogn)，也可以逆转O(n)），逆转的时间复杂度比较小 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class Solution { public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) { Arrays.sort(nums); List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;List&lt;Integer&gt;&gt;(); int rht = nums.length - 1; int j = rht; list.add(toList(nums)); while(j &gt;= 0) { j = rht; while(j-1 &gt;=0 &amp;&amp; nums[j] &lt; nums[j-1])j--; if(j-1 &lt; 0) return list; int i = j; while(i&lt;=rht &amp;&amp; nums[i] &gt; nums[j-1]) i++; if(i&gt;rht)i=rht+1; swap(nums, i-1, j-1); //Arrays.sort(nums, j, rht+1); reverse(nums, j, rht); list.add(toList(nums)); } return null; } public void reverse(int[] nums, int i, int j) { int mid = (i+j)/2; if(mid==nums.length-1)return; if((i+j)%2==1) { for(int a = mid; a&gt;=i;a--) { swap(nums, a, 2*mid-a+1); } }else { for(int a = mid-1; a&gt;=i;a--) { swap(nums, a, 2*mid-a); } } } public List toList(int[] nums) { ArrayList&lt;Object&gt; arrayList = new ArrayList&lt;&gt;(); for(Integer e: nums) { arrayList.add(e); } return arrayList; } public void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; } public static void main(String[] args) { int[] nums = {2,6,1}; System.out.println(new Solution().permute(nums).toString()); }}","link":"/post/5cf8f464.html"},{"title":"Spring/Spring的单元测试","text":"使用步骤 导包： Spring单元测试包spring-test-4.0.0.RELEASE.jar @ContextConfiguration(locations=””)来指定Spring的配置文件 @RunWith指定用哪种驱动进行单元测试，默认JUnit @RunWith(SpringJUnit4ClassRunner.class)使用Spring的单元测试模块来执行标了@Test的测试方法好处是不需要获取Bean 12345678910111213@ContextConfiguration(locations=\"classpath:ioc.xml\")@RunWith(SpringJUnit4ClassRunner.class)public class SpringIOCTest { ApplicationContext ioc = null; @Autowired BookServlet bookServlet; @Test public void test() { System.out.println(\"--\"+bookServlet); }}","link":"/post/c62ee385.html"},{"title":"Shell 之Bash数学运算","text":"expr语法格式 expr 操作符对照表 操作符 含义 num1 | num2 num1不为空且非0 ,返回num1 ;否则返回num2 num1 &amp; num2 num1不为空且非0，返回num1 ;否则返回0 hum1 &lt; num2 num1小于num2 ,返回1 ;否则返回0 num1 &lt;= num2 num1小于等于num2 ,返回1 ;否则返回0 num1 = num2 num1等于num2 ,返回1 ;否则返回0 num1 != num2 num1不等于num2 ,返回1 ;否则返回0 num1 &gt; num2 num1大于num2 ,返回1 ;否则返回0 num1 &gt;= num2 num1大于等于num2 ,返回1 ;否则返回0 num1 + num2 求和 num1 - num2 求差 num1 * num2 求积 num1 / num2 求商 num1 % num2 求余 =号判断使用$(($num1 = $num2))会出错，但使用==时可以运行成功时 echo $?返回0 12345678910111213141516num1=20num2=100expr $num1 \\| $num2expr $num1 \\&amp; $num2expr $numl \\&lt; $num2expr $num1 \\&lt; $num2expr $num1 \\&lt;= $num2expr $num1 \\&gt; $num2expr $num1 \\&gt;= $num2expr $num1 = $num2expr $num1 != $num2expr $num1 + $num2expr $num1 - $num2expr $num1 \\* $num2expr $num1 / $num2expr $num1 % $num2 求和案例12345678910111213141516171819202122#!/bin/bashwhile truedo read -p \"Please input a postive number: \" num # 不需要expr $num + 1的返回值 expr $num + 1 &amp;&gt; /dev/null #判断是不是整形 if [ $? -eq 0 ];then if [ `expr $num \\&gt; 0` -eq 1 ];then for((i=1;i&lt;=$num;i++)) do sum=`expr $sum + $i` done echo \"The ans is $sum\" exit fi fi echo \"illegal input\" continuedone bc bc是bash内建的运算器, 支持浮点数运算 内建变量scale可以设置,默认为0 bc操作符对照表 操作符 含义 num1 + num2 求和 num1 - num2 求差 num1 * num2 求积 num1 / num2 求商 num1 % num2 求余 案例 12345[root@master datas]# echo &quot;23+35&quot; | bc58[root@master datas]# echo &quot;scale=5;23/35&quot; | bc.65714","link":"/post/7b00cacb.html"},{"title":"Spring AOP之基于注解的AOP","text":"基于注解AOP步骤 将目标类和切面类加到IOC容器中。@Component 告诉Spring哪个是切面类。@Aspect 在切面类中使用五个通知注解中的这些通知方法都何时运行 开启基于AOP注解功能 123456789101112131415161718192021222324&lt;bean id=\"mathCalculator\" class=\"xyz.lyhcc.calculate.MathCalculator\"&gt;&lt;/bean&gt;&lt;bean id=\"loggerUtils\" class=\"xyz.lyhcc.logger.LoggerUtils\"&gt;&lt;/bean&gt;&lt;bean id=\"validAspect\" class=\"xyz.lyhcc.logger.ValidAspect\"&gt;&lt;/bean&gt;&lt;aop:config&gt; &lt;aop:pointcut expression=\"execution(* xyz.lyhcc.calculate.MathCalculator.*(int,int))\" id=\"glabalPointcut\"/&gt; &lt;aop:aspect ref=\"loggerUtils\"&gt; &lt;aop:before method=\"logStart\" pointcut-ref=\"glabalPointcut\"/&gt; &lt;aop:after-returning method=\"logFinished\" pointcut-ref=\"glabalPointcut\" returning=\"result\"/&gt; &lt;aop:after-throwing method=\"logError\" pointcut-ref=\"glabalPointcut\" throwing=\"exception\"/&gt; &lt;aop:after method=\"logFinal\" pointcut-ref=\"glabalPointcut\"/&gt; &lt;aop:around method=\"aroundMethod\" pointcut-ref=\"glabalPointcut\"/&gt; &lt;/aop:aspect&gt; &lt;aop:aspect ref=\"validAspect\"&gt; &lt;aop:before method=\"logStart\" pointcut-ref=\"glabalPointcut\"/&gt; &lt;aop:after-returning method=\"logFinished\" pointcut-ref=\"glabalPointcut\" returning=\"result\"/&gt; &lt;aop:after-throwing method=\"logError\" pointcut-ref=\"glabalPointcut\" throwing=\"exception\"/&gt; &lt;aop:after method=\"logFinal\" pointcut-ref=\"glabalPointcut\"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 脚下留心注解： 快速方便配置： 功能完善重要的用配置，不重要的使用注解","link":"/post/2e488ab5.html"},{"title":"MySQL数据备份","text":"备份mysql数据库或表 命令 mysqldump 常用参数详解 1234567-u 用户名-p 密码 -h 服务器IP地址 -d 等价于--no-datab 只导出表结构 -t 等价于--no-create- info只 导出数据，不导出建表语句 -A 等价于--all-databases -B 等价于--databases导 出一个或多个数据库 ftp命令123常用命令 open 与ftp服务器建立连接 如open 192.168.37.101 user 登录用户和密码 如 user ftp123 abc 参考资料 操作123456Usage: mysqldump [OPTIONS] database [tables]OR mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]OR mysqldump [OPTIONS] --all-databases [OPTIONS]For more options, use mysqldump --helpmysqldump -udbuser -p1RootRoot@ school &gt; school.sql 示例 需求:将schoo1中 的score表备份，并且将备份数据通过FTP传输到192.168.37.101的/data/backup目录下 1234ftp -i 表示进入交互界面 -n 自动登录 -v 打印信息 12345678910111213141516171819202122232425262728#!/bin/bash#db_user=\"dbuser\"db_passwd=\"1RootRoot@\"db_host=\"192.168.37.101\"ftp_user=\"ftpUser\"ftp_passwd=\"!1qaz@2wsx\"ftp_host=\"192.168.37.101\"src_dir=\"/opt/datasets/test\"dest_dir=\"/home/ftpUser/backup\"time_date=\"`date +%Y%m%d%H%M%S`\"file_name=\"school_$time_date\"function auto_ftp{ ftp -inv &lt;&lt;EOF open 192.168.37.101 user ftpUser !1qaz@2wsx cd $src_dir put $1 $dest_dir/$file_name byeEOF}mysqldump -u$db_user -p$db_passwd -h$db_host school &gt; $src_dir/$file_name &amp;&amp; auto_ftp $src_dir/$file_name 总结 linux中ftp提示–553 Could not create file ftp服务安装 Linux crontab实现定时任务","link":"/post/3e2b1b8c.html"},{"title":"Spring 源码","text":"Spring AOP 动态代理多层切面在环绕通知，内部切面的返回值会影响外部的返回值 Spring IOC IOC是一个容器 容器启动的时候创建所有单实例对象 可以直接从容器中获取到这个对象 问题 1) ioc容器的启动过程？启动时间都做了什么？什么时候创建所有单实例bean2) ioc是如何创建单实例bean，并如何管理，这些Bean保存在哪里 调试 ioc = new ClassPathXmlApplicationContext(“application.xml”);12345678910111. this(new String[] {configLocation}, true, null);2. public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException { super(parent); setConfigLocations(configLocations); if (refresh) { refresh(); } } refresh();运行后所有Bean创建完成 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Override public void refresh() throws BeansException, IllegalStateException { synchronized (this.startupShutdownMonitor) { // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try { // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); } catch (BeansException ex) { // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; } } } 解析xml配置文件并创建所有配置文件下创建的Bean，将这些信息保存起来 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();initMessageSource(); 支持国际化initApplicationEventMulticaster(); 初始化事件转换器onRefresh(); 留给子类的方法，方便自定义容器finishBeanFactoryInitialization(beanFactory); 初始化单实例Bean调用上一个方法后进入， AbstractApplicationContext类中 1234567891011121314151617181920212223protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) { // Initialize conversion service for this context. if (beanFactory.containsBean(CONVERSION_SERVICE_BEAN_NAME) &amp;&amp; beanFactory.isTypeMatch(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)) { beanFactory.setConversionService( beanFactory.getBean(CONVERSION_SERVICE_BEAN_NAME, ConversionService.class)); } // Initialize LoadTimeWeaverAware beans early to allow for registering their transformers early. String[] weaverAwareNames = beanFactory.getBeanNamesForType(LoadTimeWeaverAware.class, false, false); for (String weaverAwareName : weaverAwareNames) { getBean(weaverAwareName); } // Stop using the temporary ClassLoader for type matching. beanFactory.setTempClassLoader(null); // Allow for caching all bean definition metadata, not expecting further changes. beanFactory.freezeConfiguration(); // Instantiate all remaining (non-lazy-init) singletons. beanFactory.preInstantiateSingletons(); } beanFactory.preInstantiateSingletons(); 初始化单实例DefaultListableBeanFactory 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Override public void preInstantiateSingletons() throws BeansException { if (this.logger.isDebugEnabled()) { this.logger.debug(\"Pre-instantiating singletons in \" + this); } //拿到所有Bean创建的名字 List&lt;String&gt; beanNames; synchronized (this.beanDefinitionMap) { // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. beanNames = new ArrayList&lt;String&gt;(this.beanDefinitionNames); } //按顺序创建Bean for (String beanName : beanNames) { //根据BeanId获取Bean的定义信息 RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); //如果Bean是单实例并且不是懒加载 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) { //是否工厂bean（即是否实现FactoryBean） if (isFactoryBean(beanName)) { final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName); boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) { isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() { @Override public Boolean run() { return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit(); } }, getAccessControlContext()); } else { isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); } if (isEagerInit) { //创建Bean的细节 getBean(beanName); } } else { getBean(beanName); } } } } getBean调用的是doGetBean(name, null, null, false);（位于AbstractBeanFactory） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException { //先获取Bean名 final String beanName = transformedBeanName(name); Object bean; // Eagerly check singleton cache for manually registered singletons. //先从已经创建的Bean中是否已存在 Object sharedInstance = getSingleton(beanName); if (sharedInstance != null &amp;&amp; args == null) { if (logger.isDebugEnabled()) { if (isSingletonCurrentlyInCreation(beanName)) { logger.debug(\"Returning eagerly cached instance of singleton bean '\" + beanName + \"' that is not fully initialized yet - a consequence of a circular reference\"); } else { logger.debug(\"Returning cached instance of singleton bean '\" + beanName + \"'\"); } } bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); } else { // Fail if we're already creating this bean instance: // We're assumably within a circular reference. if (isPrototypeCurrentlyInCreation(beanName)) { throw new BeanCurrentlyInCreationException(beanName); } // Check if bean definition exists in this factory. BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) { // Not found -&gt; check parent. String nameToLookup = originalBeanName(name); if (args != null) { // Delegation to parent with explicit args. return (T) parentBeanFactory.getBean(nameToLookup, args); } else { // No args -&gt; delegate to standard getBean method. return parentBeanFactory.getBean(nameToLookup, requiredType); } } if (!typeCheckOnly) { markBeanAsCreated(beanName); } try { final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. //拿到 才创建当前Bean之前需要提前创建的Bean,depends-on,有就循环创建 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) { for (String dependsOnBean : dependsOn) { if (isDependent(beanName, dependsOnBean)) { throw new BeanCreationException(\"Circular depends-on relationship between '\" + beanName + \"' and '\" + dependsOnBean + \"'\"); } registerDependentBean(dependsOnBean, beanName); getBean(dependsOnBean); } } // Create bean instance. if (mbd.isSingleton()) { sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() { @Override public Object getObject() throws BeansException { try { return createBean(beanName, mbd, args); } catch (BeansException ex) { // Explicitly remove instance from singleton cache: It might have been put there // eagerly by the creation process, to allow for circular reference resolution. // Also remove any beans that received a temporary reference to the bean. destroySingleton(beanName); throw ex; } } }); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); } else if (mbd.isPrototype()) { // It's a prototype -&gt; create a new instance. Object prototypeInstance = null; try { beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); } else { String scopeName = mbd.getScope(); final Scope scope = this.scopes.get(scopeName); if (scope == null) { throw new IllegalStateException(\"No Scope registered for scope '\" + scopeName + \"'\"); } try { Object scopedInstance = scope.get(beanName, new ObjectFactory&lt;Object&gt;() { @Override public Object getObject() throws BeansException { beforePrototypeCreation(beanName); try { return createBean(beanName, mbd, args); } finally { afterPrototypeCreation(beanName); } } }); bean = getObjectForBeanInstance(scopedInstance, name, beanName, mbd); } catch (IllegalStateException ex) { throw new BeanCreationException(beanName, \"Scope '\" + scopeName + \"' is not active for the current thread; \" + \"consider defining a scoped proxy for this bean if you intend to refer to it from a singleton\", ex); } } } catch (BeansException ex) { cleanupAfterBeanCreationFailure(beanName); throw ex; } } // Check if required type matches the type of the actual bean instance. if (requiredType != null &amp;&amp; bean != null &amp;&amp; !requiredType.isAssignableFrom(bean.getClass())) { try { return getTypeConverter().convertIfNecessary(bean, requiredType); } catch (TypeMismatchException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Failed to convert bean '\" + name + \"' to required type [\" + ClassUtils.getQualifiedName(requiredType) + \"]\", ex); } throw new BeanNotOfRequiredTypeException(name, requiredType, bean.getClass()); } } return (T) bean; } DefaultSingletonBeanRegistry类getSingleton 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Return the (raw) singleton object registered under the given name, * creating and registering a new one if none registered yet. * @param beanName the name of the bean * @param singletonFactory the ObjectFactory to lazily create the singleton * with, if necessary * @return the registered singleton object */public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) { Assert.notNull(beanName, \"'beanName' must not be null\"); synchronized (this.singletonObjects) { //先将Bean get出来 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { if (this.singletonsCurrentlyInDestruction) { throw new BeanCreationNotAllowedException(beanName, \"Singleton bean creation not allowed while the singletons of this factory are in destruction \" + \"(Do not request a bean from a BeanFactory in a destroy method implementation!)\"); } if (logger.isDebugEnabled()) { logger.debug(\"Creating shared instance of singleton bean '\" + beanName + \"'\"); } beforeSingletonCreation(beanName); boolean recordSuppressedExceptions = (this.suppressedExceptions == null); if (recordSuppressedExceptions) { this.suppressedExceptions = new LinkedHashSet&lt;Exception&gt;(); } try { singletonObject = singletonFactory.getObject(); } catch (BeanCreationException ex) { if (recordSuppressedExceptions) { for (Exception suppressedException : this.suppressedExceptions) { ex.addRelatedCause(suppressedException); } } throw ex; } finally { if (recordSuppressedExceptions) { this.suppressedExceptions = null; } afterSingletonCreation(beanName); } //添加创建的bean addSingleton(beanName, singletonObject); } return (singletonObject != NULL_OBJECT ? singletonObject : null); }} 12/** Cache of singleton objects: bean name --&gt; bean instance */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(64); addSingleton方法(创建的Bean最终保存)12345678protected void addSingleton(String beanName, Object singletonObject) { synchronized (this.singletonObjects) { this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); this.singletonFactories.remove(beanName); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); }} BeanFacory总结 BeanFactory和ApplicationContext的区别？ApplicationContext是BeanFactory的子接口BeanFactory ： Bean工厂，赋值Bean的创建，容器中保存的单例Bean其实是一个mapApplicationContext：容器接口，负责容器容器功能的实现(可以基于BeanFactory创建好的对象之上完成强大的容器)容器可以从map获取这个Bean，并且aop，DI(依赖注入)这些功能是在ApplicationContext接口下的这些类里面BeanFactory最底层的接口，ApplicationContext留给程序员使用的ioc容器接口Spring最大的模式就是工厂模式","link":"/post/eb698fbd.html"},{"title":"Shell文本三剑客之awk","text":"简介 awk是一个文本处理工具,通常用于处理数据并生成结果报告awk的命名是它的创始人Alfred Aho、Peter Weinberger和Brian Kernighan姓氏的首个字母组成的 工作模式 类似sed，首先读取第一行，处理完后处理下一行，它可以在处理前面加一些数据，处理后面也可以加一些数据 语法格式 第一种形式:awk ‘BEGIN{}pattern{commands}END{}’ file_name BEGIN: 正式处理之前执行pattern: 匹配模式commands: 对匹配出来的数据的处理命令END: 处理完所有匹配数据后执行file_name: 文件名 第二种形式: standard output |awk ‘BEGIN{}pattern{commands}END{}’ standard output表示对标准输出数据进行处理其他同上 awk的内置变量 内置变量 含义 $0 整行内容 $1-$n 当前行的第1-n个字段 NF 当前行的字段个数，也就是有多少列 NR 当前行的行号，从1开始计数 FNR 多文件处理时,每个文件行号单独计数,都是从0开始 FS 输入字段分隔符。不指定默认以空格或tab键分割 RS 输入行分隔符。默认回车换行\\n OFS 输出字段分隔符。默认为空格 ORS 输出行分隔符。默认为回车换行 FIL ENAME 当前输入的文件名字 ARGC 命令行参数个数 ARGV 命令行参数数组 123456789101112131415161718192021222324251. /etc/passwd 的所有内容显示 awk '{print $0}' /etc/passwd2. 指定分隔符分割后的一列，/etc/passwd awk 'BEGIN{FS=\":\"}{print $7}' /etc/passwd3. 输出当前行某个文件分割后的数据的个数,NF(Number Field) awk 'BEGIN{FS=\":\"}{print NF}' /etc/passwd4. NR(Number Row),处理行号 awk 'BEGIN{FS=\":\"}{print NR}' /etc/passwd 当有多个文件时，直接加，如果是FNR(File Number Row)就会每个文件的行号从1开始5. FS(Field Separator) 字段分隔符，不指定是以空格或tab键分割 awk 'BEGIN{FS=\":\"}{print $7}' /etc/passwd6. RS(Row Separator) 行分割符，默认换行符 awk 'BEGIN{RS=\"---\"}{print $0}' rs.txt # rs.txt 内容aaaaaaaaaaa---bbbbbbbbbbbbbbbb---cccccccccccccc7. OFS(Output Field Separator) 字段输出分隔符，默认空格 awk 'BEGIN{FS=\":\";OFS=\"##\"}{print $1,$2}' /etc/passwd # 如果$1 $2不以逗号分割OFS不起作用8. ORS(Output Row Separator) 输出行分隔符，默认换行 awk 'BEGIN{ORS=\"-_+\";FS=\":\"}{print $1,$2}' /etc/passwd9. FILENAME 文件名 awk '{print FILENAME}' /etc/passwd # 多少行数据，输出多少个10. ARGC 变量个数 awk '{print ARGC}' /etc/passwd # 2个参数， awk和/etc/passwd printf输出 格式符 含义 %s 打印字符串 %d 打印十进制数 %f 打印一一个浮点数 %X 打印十六进制数 %o 打印八进制数 %e 打印数字的科学计数法形式 %c 打印单个字符的ASCII码 修饰符 含义 - 左对齐 + 右对齐 # 显示8进制在前面加0 ,显示16进制在前面加0x 123456789101112131415161718192021格式符示例: 1、以字符串格式打印/etc/passwd中的第7个字段，以\":\"作 为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%s\\n\",$7}' /etc/passwd 2、以10进制格式打印/etc/passwd中的第3个字段，以\":\"作 为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%d\\n\",$3}' /etc/passwd 3、以浮点数格式打印/etc/passwd中的第3个字段，以\":\"作为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%f\\n\",$3}' /etc/passwd 4、以16进制数格式打印/etc/passwd中的第3个字段，以\":\"作 为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%x\\n\",$3}' /etc/passwd 5、以8进制数格式打印/etc/passwd中的第3个字段，以\":\"作为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%o\\n\",$3}' /etc/passwd 6、以科学计数法格式打印/etc/passwd中的第3个字段，以\":\"作为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%e\\n\",$3}' /etc/passwd修饰符示例: 1、左对齐格式 awk 'BEGIN{FS=\":\"}{printf \"%-20s %-20s\\n\",$1,$7}' /etc/passwd 2、右对齐格式 awk 'BEGIN{FS=\":\"}{printf \"%20s %20s\\n\",$1,$7}' /etc/passwd 3、打印8进制或16进制数字是在前面加# awk 'BEGIN{FS=\":\"}{printf \"%#o\\n\",$3}' /etc/passwd awk 'BEGIN{FS=\":\"}{printf \"%#x\\n\",$3}' /etc/passwd 模式匹配 RegExp(表示可以是一个敞亮字符串或者是一个正则表达式写法) 1234匹配/etc/passwd文件行中含有root字符串的所有行 awk 'BEGIN{}/^root/{print $0}' /etc/passwd匹配/etc/passwd文件行中以yarn开头的所有行 awk 'BEGIN{}/^yarn/{print $0}' /etc/passwd 关系运算匹配 123456789101112131415161718192021222324252627282930关系运算符匹配:&lt; 小于 &gt; 大于 &lt;= 小于等于 &gt;= 大于等于== 等于 != 不等于 ~ 匹配正则表达式 !~ 不匹配正则表达式 不匹配正则表达式(1)、以:为分隔符， 匹配/etc/passwd文件中第3个字段小于50的所有行信息 awk 'BEGIN{FS=\":\"}$3&lt;50' /etc/passwd(2)、以:为分隔符，匹配/etc/passwd文件中 第3个字段大于50的所有行信息 awk 'BEGIN{FS=\":\"}$3&gt;50' /etc/passwd(3)、以:为分隔符，匹配/etc/passwd文 件中第7个字段为/bin/bash的所有行信息 awk 'BEGIN{FS=\":\"}$7==\"\\/bin\\/bash\"' /etc/passwd(4)、以:为分隔符，匹配/etc/passwd文件中 第7个字段不为/bin/bash的所有行信息 awk 'BEGIN{FS=\":\"}$7==\"/bin/bash\"' /etc/passwd(5)、以:为分隔符，匹配/etc/passwd中第3个字段包含3个以上数字的所有行 awk 'BEGIN{FS=\":\"}$3~/[0,9]{3,}/' /etc/passwd布尔运算符匹配:|| 或&amp;&amp; 与! 非(1)、以:为分隔符，匹配/etc/passwd文 件中包含root或mysql的所有行信息 awk 'BEGIN{FS=\":\"}$1==\"root\" || $1==\"mysql\"' /etc/passwd(2)、以:为分隔符，匹配/etc/passwd文件中第3个字段小于50并且第4个字段大于50的所有行信息 awk 'BEGIN{FS=\":\"}$3&lt;50 &amp;&amp; $4&gt;50' /etc/passwd 动作表达式 运算符 含义 + 加 - 减 * 乘 / 除 % 模 ^或** 乘方 ++x 在返回x变量之前, x变量加1 x++ 在返回变量之后，x变量加1 12345678910111213141、使用awk计算/etc/services中的空白行数量) # 空白行的表示^$ awk '/^$/{sum++}END{print sum}' /etc/services2、计算学生课程分数平均值，学生课程文件内容如下: Allen 80 90 96 98 Mike 93 98 92 91 Zhang 78 76 87 92 Jerry 86 89 68 92 Han 85 95 75 90 Li 78 88 98 100 awk '{sum=$2+$3+$4+$5; AVG=sum/4; printf \"%-8s%-8d%-8d%-8d%-8d%0.2f\\n\",$1,$2,$3,$4,$5,AVG}' stu.txt 3、使用awk输出/etc/passwd文件的行数。分两种方式显示行数，一种是正序如1.2.3.4.. awk '{line++; printf \"%d:%s\\n\", line,$0}' /etc/passwd awk条件语句1234567891011121、以:为分隔符，只打印/etc/passwd中第3个字段的数值在50-100范围内的行信息 awk 'BEGIN{FS=\":\"}{if(50&lt;$3 &amp;&amp; $3&lt;100) print $0}' /etc/passwd2、计算下列每个同学的平均分数，并且只打印平均分数大于90的同学姓名和分数信息 Allen 80 90 96 98 Mike 93 98 92 91 Zhang 78 76 87 92 Jerry 86 89 68 92 Han 85 95 75 90 Li 78 88 98 100 awk '{sum=$2+$3+$4+$5; AVG=sum/4; if(AVG&gt;90)printf \"%-8s%-8d%-8d%-8d%-8d%0.2f\\n\",$1,$2,$3,$4,$5,AVG}' stu.txt awk循环语句1234# 三种循环awk 'BEGIN {i = 1; while (i &lt; 6) { print i; ++i } }'awk 'BEGIN { for (i = 1; i &lt;= 5; ++i) print i }'awk 'BEGIN {i = 1; do { print i; ++i } while (i &lt; 6) }' awk字符函数 函数名 解释 函数返回值 length(str) 计算字符串长度 整数长度值 index(str1 ,str2) 在str1中查找str2的位置 返回值为位置索引,从1计数 tolower(str) 转换为小写 转换后的小写字符串 toupper(str) 转换为大写 转换后的大写字符串 substr(str,m,n) 从str的m个字符开始，截取n位 截取后的子串 split(str,arr,fs) 按fs切割字符串,结果保存arr 切割后的子串的个数 match(str,RE) 在str中按照RE查找，返回位置 返回索引位置 sub(RE,RepStr,str) 在str中搜索符合RE的字串,将其替换为RepStr ;只替换第一个 替换的个数 gsub(RE,RepStr,str) 在str中搜索符合RE的字串,将其替换为RepStr;替换所有 替换的个数 12345678910111213141516171819202122232425262728291、以:为分隔符，返回/etc/passwd中每行中每个字段的长度 awk 'BEGIN{ FS=\":\" }{ i=0; while(i&lt;=NF){ printf \"%d\", length($i); if(i!=NF)printf \":\"; i++; } printf \"\\n\" }' /etc/passwd2、搜索字符串\"I have a dream\"中出现\"ea\"字符串的位置 awk 'BEGIN{str=\"I have a dream\"; print index(str,\"ea\")}'3、将字符串\"Hadoop is a bigdata Framawork\"全部转换为小写 awk 'BEGIN{str=\"Hadoop is a bigdata Framawork\"; print tolower(str)}'4、将字符串\"Hadoop is a bigdata Framawork\" 全部转换为大写 awk 'BEGIN{str=\"Hadoop is a bigdata Framawork\"; print toupper(str)}'5、将字符串\"Hadoop Kafka Spark Storm HDFS YARN Zookeeper\"， 按照空格为分隔符 # 从1开始 awk 'BEGIN{str=\"Hadoop Kafka Spark Storm HDFS YARN Zookeeper\"; split(str, arr, \" \"); print arr[1]}'6、搜索字符串\"Tranction 2345 Start:Select * from master\"第一个数字出现的位置 awk 'BEGIN{str=\"Tranction 2345 Start:Select * from master\"; print match(str, /[0-9]+/)}'7、截取字符串\" transaction start\"的子串，截取条件从第4个字符开始，截取5位 awk 'BEGIN{str=\" transaction start\"; print substr(str, 4,5)}'8、替换字符串\"Tranction 243 start, Event ID:9002\"中第一个匹配到的数字串为$符号 awk 'BEGIN{str=\"Tranction 243 start, Event ID:9002\"; sub(/[0-9]+/, \"$\", str); print str}' awk常用选项 选项 解释 -v 参数传递 -f 指定脚本文件 -F 指定分隔符 -V 查看awk的版本号 1234567891011121314151617181920212223# -v[root@master ~]# num1=100[root@master ~]# var=\"You\"[root@master ~]# awk -v num2=$num1 -v var1=$var 'BEGIN{print num2, var1}'100 You# 字符串有空格需要引号[root@master ~]# var=\"Hello Shell\"[root@master ~]# awk -v num2=$num1 -v var1=$var 'BEGIN{print num2, var1}'awk: fatal: cannot open file `BEGIN{print num2, var1}' for reading (No such file or directory)[root@master ~]# awk -v num2=$num1 -v var1=\"$var\" 'BEGIN{print num2, var1}'100 Hello Shell# -f awk后面的BEGIN放入test.awk后执行[root@master datas]# awk -v num2=$num1 -v var1=\"$var\" -f test.awk 100 Hello Shell# -Fawk -F : 'BEGIN{}{print $1}' /etc/passwd# -V[root@master ~]# awk -VGNU Awk 4.0.2 awk数组与Shell数组 awk数组与Shell数组的区别前者从0开始，后者从1开始索引 awk数组格式 1234array_name[index]=valuearray_name：数组的名称index：数组索引value：数组中元素所赋予的值 操作 123创建数组 array_name[index]=value删除数组中的元素 delete array_name[index]多维数组 array_name[rowIndex, colIndex]=value 应用 123456789101112131415161718192021221. 统计主机上所有的TCP连接状态数，按照每个TCP状态分类 netstat -an | grep tcp | awk '{array[$6]++}END{for(a in array) print a, array[a]}'2. 计算横向数据总和，计算纵向数据总和Allen 80 90 96 98Mike 93 98 92 91Zhang 78 76 87 92Jerry 86 89 68 92Han 85 95 75 90Li 78 88 98 100 awk 'BEGIN{printf \"%-10s%-10s%-10s%-10s%-10s%-10s\\n\", \"Name\",\"Chinese\",\"English\",\"Math\",\"Physical\",\"Total\" }{ array[\"Chinese\"]+=$2 array[\"English\"]+=$3 array[\"Math\"]+=$4 array[\"Physical\"]+=$5 total = $2+$3+$4+$5 printf \"%-10s%-10d%-10d%-10d%-10d%-10d\\n\", $1,$2,$3,$4,$5,total }END{ printf \"%-10s\", \"\" for(a in array) printf \"%-10d\",array[a] printf \"\\n\" }' student.txt Shell数组123456789101112131415array=(\"If\" \"we\" \"can\" \"only\" \"encounter\" \"each\" \"other\" \"rather\" \"than\" \"stay\" \"with\" \"each\" \"other,then\" \"I\" \"wish\" \"we\" \"had never encountered\")打印元素: echo ${array[2] } 打印元素个数: echo ${#array[@] } 打印元素长度: echo ${#array[3] } 给元素赋值: array[3]=\"Li\" 删除元素: unset array[2] ;unset array 分片访问: echo ${array[@] :1:3} 元素内容替换: ${array[@]/e/E}只替换第-一个e;${array[@]//e/E}替换所有的e 数组的遍历：for a in arraydo echo $adone 操作 123456789101112131415161718root@ubuntu:~# array=(\"If\" \"we\" \"can\" \"only\" \"encounter\" \"each\" \"other\" \"rather\" \"than\" \"stay\" \"with\" \"each\" \"other,then\" \"I\" \"wish\" \"we\" \"had never encountered\")root@ubuntu:~# echo ${array[2]}canroot@ubuntu:~# echo ${array[*]}If we can only encounter each other rather than stay with each other,then I wish we had never encounteredroot@ubuntu:~# echo ${#array[*]}17root@ubuntu:~# echo ${#array[@]}17root@ubuntu:~# echo ${array[@]}If we can only encounter each other rather than stay with each other,then I wish we had never encounteredroot@ubuntu:~# unset array[2]root@ubuntu:~# echo ${array[@]}If we only encounter each other rather than stay with each other,then I wish we had never encounteredroot@ubuntu:~# echo ${array[@]:1:3}we only encounterroot@ubuntu:~# echo ${array[@]/we/we and you}If we and you only encounter each other rather than stay with each other,then I wish we and you had never encountered awk小案例利用awk处理日志，并生成结果报告insert.sh生成数据 12345678910111213141516171819202122#!/bin/absh# function create_random(){ min=$1 max=$(($2-$min+1)) num=$(date+%s%N) echo $(($num%$max+$min))}INDEX=1while truedo for user in A B C D E F G H I J K L do COUNT=$RANDOM NUM1=`create_random 1 $COUNT` NUM2=`expr $COUNT - $NUM1` echo \"`date '+%Y-%m-%d %H:%M:%S'` $INDEX Batches:$user INSERT $COUNT DATA INTO DATABASE:mydb table log, INSERT $NUM1 records successfully, failed $NUM2 records\" &gt;&gt; 123.txt INDEX=`expr $INDEX + 1` donedone 需求 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556(1)、统计每个人员分别插入了多少条record进数据库 awk 'BEGIN{ FS=\" \" }{ array[substr($4, 9, 1)]++ }END{ for(a in array) print a, array[a] }' 123.txt(2)、统计每个人分别插入成功了多少record,失败了多少record13 17 awk 'BEGIN{ FS=\" \" }{ str=substr($4, 9, 1) success[str]+=$13 fail[str]+=$17 }END{ for(a in success) print a, success[a],fail[a] }' 123.txt(3)、 将例子1和例子2结合起来，一起输出，输出每个人分别插入多少数据，多少成功,多少条失败 awk 'BEGIN{ FS=\" \" printf \"%-10s%-10s%-10s%-10s\\n\",\"Name\",\"Number\",\"Success\",\"Failure\" }{ str=substr($4, 9, 1) array[str]++ success[str]+=$13 fail[str]+=$17 }END{ for(a in success){ printf \"%-10s%-10d%-10d%-10d\\n\",a, array[a],success[a],fail[a] } }' 123.txt(4)、在例子3的基础上，加上结尾，统计全部插入记录数，成功记录数，失败记录数awk 'BEGIN{ FS=\" \" printf \"%-10s%-10s%-10s%-10s\\n\",\"Name\",\"Number\",\"Success\",\"Failure\" }{ str=substr($4, 9, 1) array[str]++ success[str]+=$13 fail[str]+=$17 }END{ for(a in success){ Total+=array[a] sTotal+=success[a] fTotal+=fail[a] printf \"%-10s%-10d%-10d%-10d\\n\",a, array[a],success[a],fail[a] } printf \"%-10s%-10d%-10d%-10d\\n\",\"TOTAL\", Total,sTotal,fTotal }' 123.txt(5)、查找丢失数据的现象，也就是成功+失败的记录数，不等于一共插入的记录数。找出这些数据并显示行号和对应日志信息 修改123.txt文件的数据进行测试 awk '{if($6!=$13+$17) printf \"%d:%s\\n\", NR,$0 }' 123.txt Shell进行数据库操作 学生数据库创建，定义在school.sql12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273DROP TABLE IF EXISTS `db_school`.`Student`;create table Student( Sno varchar(20), Sname varchar(50), primary key (Sno)) ENGINE=InnoDB DEFAULT CHARSET=utf8; DROP TABLE IF EXISTS `db_school`.`Course`;create table Course( Cno varchar(20), Cname varchar(50), Tno varchar(20), primary key (Cno)) ENGINE=InnoDB DEFAULT CHARSET=utf8; DROP TABLE IF EXISTS `db_school`.`SC`;create table SC( Sno varchar(20), Cno varchar(20), score int, primary key (Sno,Cno)) ENGINE=InnoDB DEFAULT CHARSET=utf8; DROP TABLE IF EXISTS `db_school`.`Teacher`;create table Teacher( Tno varchar(20), Tname varchar(50), primary key (Tno)) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `Student`(Sno,Sname) VALUES ('001','陈一');INSERT INTO `Student`(Sno,Sname) VALUES ('002','郭二');INSERT INTO `Student`(Sno,Sname) VALUES ('003','张三');INSERT INTO `Student`(Sno,Sname) VALUES ('004','李四');INSERT INTO `Student`(Sno,Sname) VALUES ('005','王五'); INSERT INTO `Teacher`(Tno,Tname) VALUES ('001','张老师');INSERT INTO `Teacher`(Tno,Tname) VALUES ('002','王老师');INSERT INTO `Teacher`(Tno,Tname) VALUES ('003','钱老师');INSERT INTO `Teacher`(Tno,Tname) VALUES ('004','刘老师');INSERT INTO `Teacher`(Tno,Tname) VALUES ('005','胡老师'); INSERT INTO `Course`(Cno,Cname,Tno) VALUES ('001','语文','张老师');INSERT INTO `Course`(Cno,Cname,Tno) VALUES ('002','数学','王老师');INSERT INTO `Course`(Cno,Cname,Tno) VALUES ('003','英语','钱老师');INSERT INTO `Course`(Cno,Cname,Tno) VALUES ('004','物理','刘老师');INSERT INTO `Course`(Cno,Cname,Tno) VALUES ('005','政治','胡老师'); INSERT INTO `SC`(Sno,Cno,score) VALUES ('001','001',50);INSERT INTO `SC`(Sno,Cno,score) VALUES ('001','002',60);INSERT INTO `SC`(Sno,Cno,score) VALUES ('001','003',70);INSERT INTO `SC`(Sno,Cno,score) VALUES ('001','004',80);INSERT INTO `SC`(Sno,Cno,score) VALUES ('001','005',90); INSERT INTO `SC`(Sno,Cno,score) VALUES ('002','001',90);INSERT INTO `SC`(Sno,Cno,score) VALUES ('002','002',80);INSERT INTO `SC`(Sno,Cno,score) VALUES ('002','003',70);INSERT INTO `SC`(Sno,Cno,score) VALUES ('002','004',60);INSERT INTO `SC`(Sno,Cno,score) VALUES ('002','005',50); INSERT INTO `SC`(Sno,Cno,score) VALUES ('003','001',81);INSERT INTO `SC`(Sno,Cno,score) VALUES ('003','002',82);INSERT INTO `SC`(Sno,Cno,score) VALUES ('003','003',83);INSERT INTO `SC`(Sno,Cno,score) VALUES ('003','004',84);INSERT INTO `SC`(Sno,Cno,score) VALUES ('003','005',85); 引入数据库的命令1[root@node1 datasets]# mysql -uroot -proot school &lt; school.sql 在建立数据库之后可以先进行用户授权12345grant all on school.* to dbuser@&apos;%&apos; identified by &apos;123456&apos;all 表示授权所有操作，增删改查on 后面的school.* 表示school数据库的所有表dbuser 表示授权用户名后面的@&apos;%&apos;表示授权所有网段， 如&apos;192.168.37.%&apos;","link":"/post/209b93b1.html"},{"title":"Shell 函数的定义和使用","text":"函数的介绍 Linux Shell中的函数和大多数编程语言中的函数一样 将相似的任务或代码封装到函数中,供其他地方调用 语法格式 格式1 格式2 1234567function name{ command1 command2 ... conmandn} 如何使用函数 直接使用函数名调用,可以将其想象成Shell中的一条命令 函数内部可以直接使用参数$1、$…$n. 调用函数: function_ name $1 $2 案例–MySQL守护进程 需求描述：写一个监控mysql的桥本，如果mysql服务宕掉，则该脚本可以检测到并将进程启动,(守护进程) 123456789101112131415161718192021#!/bin/bash# 获取进程号this_pid=$$while truedo ps -ef | grep mysqld | grep -v grep | grep -v $this_pid &amp;&gt; /dev/null # 判断进程是否存在 if [ $? -eq 0 ];then echo \"Mysql is runing well\" else systemctl start mysqld echo \"Mysql is down, start it...\" fidone 函数的传递参数高级语言函数的定义 1234567int example_1 (int arg1,int arg2){ arg1 = arg2 ........ ........ return null} 高级语言函数的调用 Shell函数传参12345function name{ echo \"Hello $1\" echo \"Hello $2\"} Shell 函数调用123456name Lily lyhcc[root@master datas]# function greeting { echo \"Hello $1, $2\"; }[root@master datas]# greeting Allen lyhccHello Allen, lyhcc 简单计算器 123456789101112131415161718192021222324function cal{ echo \"$2\" case $2 in +) echo $(($1 + $3)) ;; -) echo $(($1 - $3)) ;; \\*) echo \"$1 $2 $3\" echo \"`expr $1 \\* $3`\" ;; /) echo $(($1 / $3)) ;; esac}cal 2 \\* 3cal 2 + 3 注意\\t 函数返回值返回值方式 方法 格式 方法一 return 方法二 echo 使用return返回值 使用return返回值，只能返回1-255的整数 函数使用return返回值, 通常只是用来供其他地方调用获取状态,因此通常仅返回0或1 ; 0表示成功, 1表示失败 案例–判断MySQL是否在运行 1234567891011121314151617181920212223242526272829#!/bin/bashthis_pid=$$function mysql_isRunning{ ps -ef | grep mysqld | grep -v grep | grep -v $this_pid &amp;&gt; /dev/null if [ $? -eq 0 ];then return else return 1 fi}mysql_isRunning &amp;&amp; echo \"Mysql Server is running\" || echo \"Mysql Server is stoped\"执行过程[root@master datas]# sh -x mysqld_check.sh + this_pid=14205+ mysql_isRunning+ grep -v 14205+ ps -ef+ grep mysqld+ grep -v grep+ '[' 0 -eq 0 ']'+ return+ echo 'Mysql Server is running'Mysql Server is running 使用echo返回值 使用echo可以返回任何字符串结果 通常用于返回数据,比如一个字符串值或者列表值 案例–获取系统用户名 12345678#!/bin/bashfunction get_users{ users=`cat /etc/passwd | cut -d \":\" -f 1` echo $users}get_users","link":"/post/dc0c80f8.html"},{"title":"find、which、locate、whereis总结","text":"locate命令介绍 文件查找命令，所属软件包mlocate 不同于find命令是在整块磁盘中搜索, locate命令在数据库文件中查找 find是默认全部匹配, locate则是默认部分匹配 updatedb命令更新 对应数据库文件，就可以通过locate命令查找，默认定时执行 用户更新/var/lib/mlocate/mlocate.db 所使用配置文件/etc/updatedb.conf，如排除某些目录 该命令在后台cron计划任务中定期执行 whereis命令 选项 含义 -b 只返回二进制文件 -m 只返回帮助文档文件 -S 只返回源代码文件 which 命令 作用： 仅查找二进制文件 选项 含义 -b 只返回二进制文件 总结 命令 适用场景 优缺点 find 查找某一类文件,比如文件名部分一致 功能强大，速度慢 locate 只能查找单个文件 功能单一, 速度快 whereis 查找程序的可执行文件、帮助文档等 不常用 which 只查找程序的可执行文件 常用于查找程序的绝对路径","link":"/post/f7db85fb.html"},{"title":"mysql命令详情","text":"参数说明123456789101112-u 用户名-p 用户密码-h 服务器ip-D 连接的数据库-N 不输出列信息-B 使用tab键代替默认的交互分隔符-e 执行sql语句其他选项-E 垂直输出-H 以HTML格式输出-X 以XML格式输出 操作 说明： 使用ip时使用-h指定，以下不进行ip指定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[root@node1 datasets]# mysql -udbuser -p1RootRoot@ -D school -e 'select * from Student' mysql: [Warning] Using a password on the command line interface can be insecure.+-----+--------+| Sno | Sname |+-----+--------+| 001 | 陈一 || 002 | 郭二 || 003 | 张三 || 004 | 李四 || 005 | 王五 |+-----+--------+[root@node1 datasets]# mysql -udbuser -p1RootRoot@ -N -D school -e 'select * from Student' mysql: [Warning] Using a password on the command line interface can be insecure.+-----+--------+| 001 | 陈一 || 002 | 郭二 || 003 | 张三 || 004 | 李四 || 005 | 王五 |+-----+--------+[root@node1 datasets]# mysql -udbuser -p1RootRoot@ -B -N -D school -e 'select * from Student' mysql: [Warning] Using a password on the command line interface can be insecure.001 陈一002 郭二003 张三004 李四005 王五[root@node1 datasets]# mysql -udbuser -p1RootRoot@ -E -B -N -D school -e 'select * from Student' mysql: [Warning] Using a password on the command line interface can be insecure.*************************** 1. row ***************************001陈一*************************** 2. row ***************************002郭二*************************** 3. row ***************************003张三*************************** 4. row ***************************004李四*************************** 5. row ***************************005王五[root@node1 datasets]# mysql -udbuser -p1RootRoot@ -H -B -N -D school -e 'select * from Student' mysql: [Warning] Using a password on the command line interface can be insecure.&lt;TABLE BORDER=1&gt;&lt;TR&gt;&lt;TR&gt;&lt;TD&gt;001&lt;/TD&gt;&lt;TD&gt;陈一&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;002&lt;/TD&gt;&lt;TD&gt;郭二&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;003&lt;/TD&gt;&lt;TD&gt;张三&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;004&lt;/TD&gt;&lt;TD&gt;李四&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;005&lt;/TD&gt;&lt;TD&gt;王五&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;[root@node1 datasets]# mysql -udbuser -p1RootRoot@ -X -B -N -D school -e 'select * from Student' mysql: [Warning] Using a password on the command line interface can be insecure.&lt;?xml version=\"1.0\"?&gt;&lt;resultset statement=\"select * from Student\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;001&lt;/field&gt; &lt;field name=\"Sname\"&gt;陈一&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;002&lt;/field&gt; &lt;field name=\"Sname\"&gt;郭二&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;003&lt;/field&gt; &lt;field name=\"Sname\"&gt;张三&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;004&lt;/field&gt; &lt;field name=\"Sname\"&gt;李四&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;005&lt;/field&gt; &lt;field name=\"Sname\"&gt;王五&lt;/field&gt; &lt;/row&gt;&lt;/resultset&gt; 实例 写一个脚本，该脚本可以接收一个参数，参数为需要执行的SQI语句 1234567891011121314151617181920212223242526272829[root@node1 shell]# sh example_1.sh school \"select * from Student\"mysql: [Warning] Using a password on the command line interface can be insecure.+-----+--------+| Sno | Sname |+-----+--------+| 001 | 陈一 || 002 | 郭二 || 003 | 张三 || 004 | 李四 || 005 | 王五 |+-----+--------+[root@node1 shell]# sh example_1.sh school \"select * from Student\" &gt; result.txtmysql: [Warning] Using a password on the command line interface can be insecure.[root@node1 shell]# cat result.txt Sno Sname001 陈一002 郭二003 张三004 李四005 王五[root@node1 shell]# cat example_1.sh #!/bin/bash# user=\"dbuser\"passwd=\"1RootRoot@\"host=\"192.168.37.101\"mysql -u$user -p$passwd -D $1 -e \"$2\" 查询MySQL任意表的数据，并将查询到的结果保存到HTML文件中 12345678910111213[root@node1 shell]# sh example_2.sh school \"select * from Student\" &gt; result.htmlmysql: [Warning] Using a password on the command line interface can be insecure.[root@node1 shell]# cat result.html &lt;TABLE BORDER=1&gt;&lt;TR&gt;&lt;TH&gt;Sno&lt;/TH&gt;&lt;TH&gt;Sname&lt;/TH&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;001&lt;/TD&gt;&lt;TD&gt;陈一&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;002&lt;/TD&gt;&lt;TD&gt;郭二&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;003&lt;/TD&gt;&lt;TD&gt;张三&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;004&lt;/TD&gt;&lt;TD&gt;李四&lt;/TD&gt;&lt;/TR&gt;&lt;TR&gt;&lt;TD&gt;005&lt;/TD&gt;&lt;TD&gt;王五&lt;/TD&gt;&lt;/TR&gt;&lt;/TABLE&gt;[root@node1 shell]# [root@node1 shell]# cat example_2.sh #!/bin/bash# user=\"dbuser\"passwd=\"1RootRoot@\"host=\"192.168.37.101\"mysql -u$user -p$passwd -H -D $1 -e \"$2\" 查询MySQL任意表的数据，并将查询到的结果保存到XML文件中 1234567891011121314151617181920212223242526272829303132333435363738394041[root@node1 shell]# sh example_3.sh school \"select * from Student\" &gt; result.xmlmysql: [Warning] Using a password on the command line interface can be insecure.[root@node1 shell]# cat result.xml &lt;?xml version=\"1.0\"?&gt;&lt;resultset statement=\"select * from Student\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;001&lt;/field&gt; &lt;field name=\"Sname\"&gt;陈一&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;002&lt;/field&gt; &lt;field name=\"Sname\"&gt;郭二&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;003&lt;/field&gt; &lt;field name=\"Sname\"&gt;张三&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;004&lt;/field&gt; &lt;field name=\"Sname\"&gt;李四&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;field name=\"Sno\"&gt;005&lt;/field&gt; &lt;field name=\"Sname\"&gt;王五&lt;/field&gt; &lt;/row&gt;&lt;/resultset&gt;[root@node1 shell]# cat example_3.sh #!/bin/bash# user=\"dbuser\"passwd=\"1RootRoot@\"host=\"192.168.37.101\"mysql -u$user -p$passwd -X -D $1 -e \"$2\" Shell脚本导入数据 12345678910111213141516171819202122# IFS可以指定read的分隔符[root@node1 shell]# sh import.sh [root@node1 shell]# cat import.sh #!/bin/bash# user=\"dbuser\"passwd=\"1RootRoot@\"cat data.txt | while read id namedo mysql -u$user -p$passwd -Dschool -e \"insert into Student values('$id','$name')\"done[root@node1 shell]# cat data.txt 110 Allen122 Kilen123 abc124 qw234 wew345 wsd","link":"/post/f21319aa.html"},{"title":"Shell全局变量和局部变量&函数库","text":"全局变量 不做特殊声明, Shell中变量都是全局变量 Tips: 大型脚本程序中函数中慎用全局变量 局部变量 定义变量时,使用local关键字 函数内和外若存在同名变量,则函数内部变量覆盖外部变量 函数库为什么要定义函数库 经常使用的重复代码封装成函数文件 一般不直接执行,而是由其他脚本调用 案例–定义函数库 定义一个函数库，该函数库实现以下几个函数: 加法函数add&emsp;add 12 89 减法函数reduce&emsp;reduce 80 20 乘法函数multip1e 除法函数divide 打印系统运行情况的函数sys_load，该函数可以显示内存运行情况,磁盘使用情况 函数库文件 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bash#function add{ echo \"`expr $1 + $2`\"}function reduce{ echo \"`expr $1 - $2`\"}function multiple{ echo \"`expr $1 \\* $2`\"}function divide{ echo \"`expr $1 / $2`\"}function sys_load{ echo \"Memory Info:\" echo \"`free -m`\" echo echo \"Disk Capacity:\" echo df -u echo } shell脚本 12345678#!/bin/bash#. /opt/datas/lib/base_functionadd 2 3reduce 3 4multiple 3 6divide 34 2 经验之谈 库文件名的后缀是任意的,但一般使用.lib 库文件通常没有可执行选项 库文件无需和脚本在同级目录,只需在脚本中引用时指定 第一行一般使用#!/bin/echo ,输出警告信息,避免用户执行","link":"/post/e8089af9.html"},{"title":"Shell命令的替换","text":"1 方法一 方法二 语法格式 `command` $(command) 例子 例子1:获取系统得所有用户并输出 123456[root@master datas]# cat /etc/passwdroot:x:0:0:root:/root:/bin/bash# cut 切分 -d 指定分隔符 -f 取段[root@master datas]# cat /etc/passwd | cut -d \":\" -f 1root 123456789#!/bin/shind=1for user in `cat /etc/passwd | cut -d \":\" -f 1`do echo \"This is $ind user: $user\" ind=$(($ind + 1))done 例子2:根据系统时间计算今年或明年 123456789[root@master datas]# date Sat Dec 7 00:01:44 CST 2019[root@master datas]# date +%Y2019[root@master datas]# echo &quot;This is $(date +%Y) year&quot;This is 2019 year[root@master datas]# echo &quot;This is $(($(date +%Y) + 1)) year&quot;This is 2020 year 例子3:根据系统时间获取今年还剩下多少星期，已经过了多少星期12345[root@slave2 datasets]# echo \"This yaer have passed $(date +%j) days\"This yaer have passed 341 days[root@slave2 datasets]# echo \"This yaer have passed $(($(date +%j)/7)) weeks\"This yaer have passed 48 weeks 例子4:判定nginx进程是否存在，若不存在则自动拉起该进程 说明：-v grep 去掉grep本身进程wc -l 统计结果行数V 1ps -ef | grep nginx -v grep |wc -l 1234567#!/bin/bashmysql_process_num=$(ps -ef | grep mysql | grep -v grep | wc -l)if [ $mysql_process_num -eq 0 ]; then systemctl start mysqldfi 总结: ``和$()两者是等价的，但推荐初学者使用$()，易于掌握;缺点是极少数UNIX可能不支持$(())主要用来进行整数运算，包括加减乘除，引用变量前面可以加$，也可以不加$","link":"/post/7297f3f0.html"},{"title":"Shell变量的替换和测试","text":"变量的替换 语法 说明 ${变量名#匹配规则} 从变量 开头 进行规则匹配，将符合 最短 的数据删除 ${变量名##匹配规则} 从变量开头进行规则匹配,将符合最长的数据删除 ${变量名%匹配规则} 从变量尾部进行规则匹配,将符合最短的数据删除 ${变量名%%匹配规则} 从变量尾部进行规则匹配,将符合最长的数据删除 ${变量名旧字符串/新字符串} 变量内容符合旧字符串则,则第一个旧字符串会被新字符串取代 ${变量名旧字符串/新字符串} 变量内容符合旧字符串则，则全部的旧字符串会被新字符串取代 案例 123456789101112131415161718192021222324252627[root@master opt]# var=&quot;Hello! I am Kiki. What&apos;s your name?&quot;[root@master opt]# echo $varHello! I am Kiki. What&apos;s your name?[root@master opt]# var1=${var#*am}[root@master opt]# echo $var1Kiki. What&apos;s your name?[root@master opt]# var2=${var##*am}[root@master opt]# echo $var2e?[root@master opt]# var3=${var%am*}[root@master opt]# echo $var3Hello! I am Kiki. What&apos;s your n[root@master opt]# var4=${var%%am*}[root@master opt]# echo $var4Hello! I[root@master opt]# var5=${var/I am/He is}[root@master opt]# echo $var5Hello! He is Kiki. What&apos;s your name?[root@master opt]# var6=${var//am/ma}[root@master opt]# echo $var6Hello! I ma Kiki. What&apos;s your nmae? 变量的测试","link":"/post/31fca8c2.html"},{"title":"Shell的字符串处理","text":"计算字符串的长度 a 语法 说明 方法1 ${ #string} 无 方法2 expr length “$string” string有空格必须加双引号 123456789[root@master opt]# var=&quot;Hello World&quot;[root@master opt]# len=${#var}[root@master opt]# echo ${len}11[root@master opt]# len=`expr length &quot;${var}&quot;`[root@master opt]# echo ${len}11 获取子串在字符串中的索引位置语法 1234567891011121314151617expr index $string $substring# 例：[root@master opt]# ind=`expr index &quot;${var}&quot; start`[root@master opt]# echo $ind6# 说明：不是一个字符串的子串的索引，而是切分字符后查找，先找第一个字符，找不到再找第二个，以此类推[root@master opt]# ind=`expr index &quot;${var}&quot; abc`[root@master opt]# echo $ind4# 按照以上的方法仍然找不到，返回0[root@master opt]# ind=`expr index &quot;${var}&quot; z`[root@master opt]# echo $ind0 计算子串长度语法 123456789101112expr match $string substr例：[root@master opt]# var=&quot;quickstart is app&quot;[root@master opt]# sub_len=`expr match &quot;${var}&quot; app`[root@master opt]# echo $sub_len0[root@master opt]# sub_len=`expr match &quot;${var}&quot; quic`[root@master opt]# echo $sub_len4说明：必须从头开始匹配 抽取子串 a 语法 说明 方法一 ${string:position} 从string中的position开始 方法二 ${string:position:length} 从position开始，匹配长度为length 方法三 ${string: -position} 从右边开始匹配 方法四 ${string:(position)} 从左边开始匹配 方法五 expr substr $string $position $length 从position开始,匹配长度为length 123456789101112[root@master opt]# var=&quot;Hadoop Kafka Zookeeper HBase&quot;[root@master opt]# echo ${var:10}ka Zookeeper HBase[root@master opt]# echo ${var:10:2}ka[root@master opt]# echo ${var:(-1)}e[root@master opt]# echo ${var:(0)}Hadoop Kafka Zookeeper HBase[root@master opt]# echo `expr substr &quot;${var}&quot; 3 4`doop 字符串处理小案例需求描述: 变量string=”Bigdata process framework is Hadoop , Hadoop is an open source project”执行脚本后，打印输出string字符串变量，并给出用户以下选项: 打印string长度 删除字符串中所有的Hadoop 替换第一个Hadoop为Mapreduce 替换全部Hadoop为Mapreduce用户输入数字1121314，可以执行对应项的功能;输入q1Q则退出交互模式 思路： 将不同功能模块划分，并编写函数12345function print_tipsfunction len_of_stringfunction del_hadoopfunction rep_hadoop_mapreduce_firstfunction rep_hadoop_mapreduce_all 实现以上所定义的功能函数 程序主流程设计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!/bin/shstring=\"Bigdata process framework is Hadoop , Hadoop is an open source project\"function print_tips{ echo \"****************\" echo \"1. 打印string长度\" echo \"2. 删除字符串中所有的hadoop\" echo \"3. 替换第一个Hadoop为MapReduce\" echo \"4. 替换全部Hadoop为MapReduce\" echo \"****************\"}function len_of_string{ echo \"${#string}\"}function del_hadoop{ echo \"${string//Hadoop/}\"}function rep_hadoop_mapreduce_first{ echo \"${string/Hadoop/MapReduce}\"}function rep_hadoop_mapreduce_all{ echo \"${string//Hadoop/MapReduce}\"}while truedo echo \"string=$string\" echo print_tips read -p \"Please input your choice(1|2|3|4|q|Q): \" op case $op in 1) len_of_string ;; 2) del_hadoop ;; 3) rep_hadoop_mapreduce_first ;; q|Q) rep_hadoop_mapreduce_all exit ;; *) echo \"Error, input on;y in {1|2|3|4|5|q|Q}\" ;; esacdone","link":"/post/8d5a03cf.html"},{"title":"Shell文件查找find命令","text":"语法格式 - 格式 语法格式1 find [路径] [选项] [操作] 选项参数 选项 含义 -name 根据文件名查找 -perm 根据文件权限查找 -prune 该选项可以排除某些查找目录 -user 根据文件属主查找 -group 根据文件属组查找 -mtime- n | +n 根据文件更改时间查找 -nogroup 查找无有效属组的文件 -nouser 查找无有效属主的文件 -newer file1 ! file2 查找更改时间比file1新但比file2旧IDE文件 -type 按文件类型查找 -size- n +n 按文件大小查找 -mindepth n 从n级子目录开始搜索 -maxdepth n 最多搜索到n级子目录 使用选项常用选项 1234567891011121314151617181920212223242526272829303132333435- name 查找/etc目录下以conf结尾的文件 find /etc -name '*conf' ,- iname 查找当前目录下文件名为aa的文件，不区分大小写” find . -iname aa-user 查找文件属主为hdfs的所有文件 find . -user lyhcc-group 查找文件属组为yarn的所有文件 find . -group mysql-type f 文件 find . -type f d 目录 find . -type d c 字符设备文件 find . -type c b 块设备文件 find . -type b l 链接文件 find . -type 1 p 管道文件 find . -type P-size -n 大小大于n的文件 +n 大小小于n的文件 例子1:查找/etc目录下小于10000字节的文件 find /ete -size -10000c 例子2:香找/ete目录下大干1M的文件 find /etc -size +1M-mtime -n n天以内修改的文件 +n n天以外修改的文件 n 正好n天修改的文件 例子1:查找/etc目录下5天之内修改且以conf结尾的文件find /etc -mtime -5 -name '*.conf' 例子2:查找/etc目录下10天之前修改且属主为root的文件 find /etc -mtime +10 -user root-mmin -n n分钟以内修改的文件 +n n分钟以外修改的文件 例子1:查找/etc目录下30分钟之前修改的文件 find /etc -mmin +30 例子2:查找/etc目录下30分钟之内修改的目录 find /etc -mmin -30 -type d-mindepth n 表示从n级子目录开始搜索 例子:在/etc下的3级子目录开始搜索 find /etc -mindepth 3-maxdepth n 表示最多搜索到n级子目录 例子1:在/etc下搜索符合条件的文件，但最多搜索到2级子目录find /etc -maxdepth 3 -name \"文件条件\" 例子2: find ./etc/ -type f -name \"*.conf\" -size +10k -maxdepth 2 其他选项 1234567891011121314151617-nouser 查找没有属主的用户 例子: find . -type f -nouser-nogroup 查找没有属组的用户 例子: find . -type f -nogroup-perm 例子: find . -perm 664-prune 通常和-path-起使用，用于将特定目录排除在搜索条件之外 例子1:查找当前目录下所有普通文件，但排除test目录 find . -path ./test -prune -o -type f 例子2:查找当前目录下所有普通文件，但排除etc和opt 目录 find . -path ./etc -prune -o -path ./opt -prune -o -type f 例子3:查找当前目录下所有普通文件，但排除etc和opt 目录，但属主为hdfs find . -path ./etc -prune -o -path ./opt -prune -o -type f -a -user hdfs 例子4:查找当前目录下所有普通文件，但排除etc和opt目录，但属主为hdfs，且文件大小必须大于500字节 find . -path ./etc -prune -o -path ./opt -prune -o -type f -a -user hdfs -a -size +500c-newer file1 例子: find /etc -newer a 案例–找短命对应的cnf文件配置项个数 操作12345678910111213-print 打印输出(默认)-exec 对搜索到的文件执行特定的操作，格式为-exec 'command' {} \\; {}表示前面搜索的结果 \\;固定格式 例子1:搜索/etc下的文件(非目录)，文件名以conf结尾，且大于10k， 然后将其删除 find ./etc/ -type f -name '*.conf' -size +10k -exec rm -f {} \\; 例子2:将/vax/1og/ 目录下以1og结尾的文件，且更改时间在7天以上的删除 find /var/1og/ -name '*.1og' -mtime +7 -exec rm -rf {} \\; . 例子3:搜索条件和例子1一样， 只是不删除，而是将其复制到/root/conf目录下 find ./etc/ -size +10k -type f -name '*.conf' -exec cp {} /root/conf/ \\;-ok 和exec功能一样，只是每次操作都会给用户提示 逻辑运算符 12345678910-a 与-o 或-not|! 非 例子1:查找当前目录下，属主不是hdfs的所有文件 find . -not -user hdfs I find . ! -user hdfs 例子2:查找当前目录下，属主属于hdfs，且大小大于300字节的文件 find . -type f -a -user hdfs -a -size +300c 例子3:查找当前目录下的属主为hdfs或者以xml结尾的普通文件 find . -type f -a \\( -user hdfs -o -name '*.xml' \\)","link":"/post/f449e7e2.html"},{"title":"Shell有类型变量","text":"declare和typeset命令◆ declare命令 和typeset命令两者等价◆ declare、 typeset命令 都是用来定义变量类型的 declare 参数 含义 -r 将变量设为只读 -i 将变量设为整数 -a 将变量定义为数组 -f 显示此脚本前定义过的所有函数及内容 -F 仅显示此脚本前定义过的函数名 -x 将变量声明为环境变量","link":"/post/96cd4f46.html"},{"title":"文本处理三剑客(grep/sed/awk)","text":"grep和egrep过滤器grep 第一种形式: grep [option] [pattern] [file1,file..] 第二种形式: command| grep [option] [pattern] 选项option常用选项 选项 含义 -V 不显示匹配行信息 -i 搜索时忽略大小写 -n 显示行号 -r 递归搜索 -E 支持扩展正则表达式 -F 不按正则表达式匹配,按照字符串字面意思匹配 不常用选项 选项 含义 -c 只显示匹配行总数 -w 匹配整词 -x 匹配整行 -l 只显示文件名,不显示内容 -s 不显示错误信息 grep和egrep: grep默认不支持扩展正则表达式，只支持基础正则表达式 使用grep-E可以支持扩展正则表达式 使用egreep可以支持扩展正则表达式，与grep -E等价 sed 流编辑器 sed(Stream Editor)，流编辑器。对标准输出或文件逐行进行处理格式 第一种形式: stdout | sed [option] “pattern command” 第二种形式: sed [option] “pattern command” file sed选项 选项 含义 -n 只打印模式匹配行 -e 直接在命令行进行sed编辑,默认选项 -f 编辑动作保存在文件中,指定文件执行 -r 支持扩展正则表达式 -i 直接修改文件内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@master datas]# cat sed.txt I love shellI love SHELLaxczcaxa[root@master datas]# sed -n '/Shell/p' sed.txt I love shell# 多个comand时必须加-e[root@master datas]# sed -n '/shell/p' sed.txt I love shell[root@master datas]# sed -n -e '/shell/p' -e '/SHELL/p' sed.txt I love shellI love SHELL# -f# edit.sed中有 /SHELL/p[root@master datas]# sed -n '/SHELL/p' sed.txt I love SHELL[root@master datas]# sed -n -f edit.sed sed.txt I love SHELL# -r[root@master datas]# sed -n -r '/SHELL|shell/p' sed.txt I love shellI love SHELL# -i[root@master datas]# sed -n 's/love/like/g;p' sed.txt I like shellI like SHELLaxczcaxa[root@master datas]# cat sed.txt I love shellI love SHELLaxczcaxa# 加-i直接将原文件修改[root@master datas]# sed -n -i 's/love/like/g;p' sed.txt [root@master datas]# cat sed.txt I like shellI like SHELLaxczcaxa sed Pattern用法 匹配模式 含义 10command 匹配到第10行 10,20command 匹配从第10行开始,到第20行结束 10, +5command 匹配从第10行开始,到第16行结束 /pattern1/command 匹配到pattern1的行 /pattern1/,/pattern2/command 匹配到pattern1的行开始,到匹配到pattern2的行结束 10,/pattern1/command 匹配从第10行开始,到匹配到pettern1的行结束 /pattern1/,10command 匹配到pattern1的行开始,到第10行匹配结束 1234567891011121314151、LineNumber ---直接指定行号 sed -n \"17p\" file 打印file文件的第17行2、StartLine , EndLine --指定起始行号和结束行号 sed -n \"10,20p\" file 打印file文件的10到20行3、Star tLine, +N 指定起始行号，然后后面N行 sed -n \"10,+5p\" file 打印file文件中从第10行开始，往后面加5行的内容4、/pattern1/ ----------------正则表达式匹配的行 sed -n \"/^root/p\" file 打印file文件中以root开头的行5、/pattern1/,/pattern2/ ---从匹配到pattern1的行，到匹配到pattern2 ... 案例–查找配置文件中配置项段名与段的个数 12345678910111213141516171819202122#!/bin/bashFILE_NAME=/opt/datas/openssl.cnffunction get_all_segment{ segments=\"`sed -n '/\\[ .* \\]/p' $FILE_NAME | sed -n 's/\\[ \\(.*\\) \\]/\\1/g;p'`\" echo $segments}function count_segment_num{ echo \"`sed -n '/\\[ '$1' \\]/,/\\[/p' $FILE_NAME | sed -n -e '/=/p' | grep -v ^\\# -c`\"}number=1for segment in `get_all_segment`do echo \"$number: $segment `count_segment_num $segment`\" number=`expr $number + 1`done 案例 sed 中的编辑命令详解 查询 p 打印 增加 a 行后追加 i 行前追加 r 外部文件读入，行后追加 w 匹配行写入外部文件 追加内容 1234567891011121314151617181920212223241、a (1)、passwd文件第10行后面追加\"Add Line Behind\" sed -i '10a Add line behind' passwd (2)、passwd文件第10行到第20行，每一-行后面都追加\"Test Line Behind\" sed -i '10,20a Test Line Behind' passwd (3)、passwd文 件匹配到/bin/bash的行后面追加\"Insert Line For /bin/bash Behind\" sed -i '/\\/bin\\/bash/a Insert Line For /bin/bash Behind' passwd2、i (1)、passwd文件匹配到以tss开头的行，在匹配航前面追加\"AddLineBefore\" sed -i '/^tss/i Add a line before it' passwd (2)、passwd文件每一行前面都追加\"Insert Line Before Every Line\" sed -i 'i Insert a line before every line' passwd3、r (1)、将/opt/datas/anotation文 件的内容追加到passwd文件的第20行后面 sed -i '20r /opt/datas/anotation' passwd (2)、将/etc/inittab文件内容追加到passwd文件匹配/sbin/nologin行的后面 sed -i '/\\/sbin\\/nologin/r /opt/datas/anotation' passwd (3)、将/opt/datas/anotation文件内容追加到passwd文件中特定行后面，匹配以ftp开头的行,到第18行的所有行 sed -i '/ftp/,18r /opt/datas/anotation' passwd4、w (1)、将passwd文件匹配到/bin/bash的行追加到/opt/datas/sed.txt文件中 sed -i '/\\/bin\\/bash/w /opt/datas/sed.txt' passwd (2)、 将passwd文件从第10行开始，到匹配到tss开头的所有行内容追加到/opt/datas/sed-1.txt sed -i '10,/^tss/w /opt/datas/sed-1.txt' passwd 删除 d 删除12345678910111213141516171819202122231、删除/etc/passwd中的第15行 sed -i '15d' /etc/passwd2、删除/etc/passwd中的第8行到第14行的所有内容 sed -i '8,14d' /etc/passwd3、删除/etc/passwd中的不能登录的用户(筛选条件: /sbin/nologin) sed -i '/\\/sbin\\/nologin/d' /etc/passwd4、删除/etc/passwd中以mai1开头的行，到以yarn开头的行的所有内容 sed -i '/^mai1/,/^yarn/d' /etc/passwd5、删除/etc/passwd中第一个不能登录的用户，到第13行的所有内容(少用) sed -i '/\\/sbin\\/nologin/,13d' /etc/passwd6、删除/etc/passwd中 第5行到以ftp开头的所有行的内容 sed -i '5,/^ftp/d' /etc/passwd7、删除/etc/passwd中以yarn开头的行到最后行的所有内容 sed -i '/^yarn/,$d' /etc/passwd8、删除/etc/passwd中不能登录的所有用户 sed -i '/\\/sbin\\/nologin/d' /etc/passwd典型需求 1、删除配置文件中的所有注释行和空行 sed -i '/^#/d;/^$/d' openssl.cnf sed -i '/[:blank:]*#/d' openssl.cnf # 删除空格行的注释，但对openssl.cnf无效 2、在配置文件中所有不以#开头的行前面添加*符号，注意:以#开头的行不添加 sed -i 's/^[^#]/\\*&amp;/g' openssl.cnf 修改 s/old/new 将行内第一个old替换为new s/old/new/g 将行内全部的old替换为new s/old/new/2g 将行内前2个old开始替换所有为new s/old/new/ig 将行内olg全部替换为new ,忽略大小写修改命令对照表 | 编辑命令 | 含义 || —————————— | ———————————————————— | || 1s/old/new/ | 替换第1行内容old为new || 1,10s/old/new/ | 替换1行到10行的内容old为new || 1, +5s/old/newl | 替换1行到6行的内容old为new || /pattern1/s/old/new/ | 替换匹配到pattern1的行内容old为new || /pattrn1/,/pattern2/s/old/new/ | 替换匹配到pattern1的行直到匹配到pattern2的所有行内容old为new || /pattern1/, 10s/old/new/ | 替换匹配到pattern1的行到10行的所有行内容old为new || 10,/pattern1/s/old/new/ | 替换第10行直到匹配到pattern1的所有行内容old为new | 1234567891011121、修改/etc/passwd中第1行中第1个root为ROOT sed -i '1s/root/ROOT/' /etc/passwd2、修改/etc/passwd中第5行到第10行中所有的/sbin/nologin为/bin/bash sed -i '1,10s/\\/sbin\\/nologin/\\/bin\\/bash/' /etc/passwd3、修改/etc/passwd中匹配到/sbin/nologin的行， 将匹配到行中的login改为大写的LOGIN sed -i '/\\/sbin\\/nologin/s/login/LOGIN/g' /etc/passwd4、修改/etc/passwd中从匹配到以root开头的行，到匹配到行中包含mail的所有行。修改内为将这些所有匹配的行中的bin改为HADOOP sed -i '/^root/,/mail/s/bin/HADOOP/g' passwd5、修改/etc/passwd中从匹配到以root开头的行，到第15行中的所有行，修改内容为将这些行中的nologin修改为SAPRK sed -i '/^root/,15s/nologin/SPARK/g' passwd6、修改/etc/passwd中从第15行开始，到匹配到以tss开头的所有行，修改内容为将这些行中的bin换为BIN sed -i '15,/^tss/s/bin/BIN/g' passwd 其他 = 查看行号不显示内容 什么是反向引用？ 什么是反向引用？ 使用&amp;或\\1可以表示前面匹配到的,使用\\1需要加括号，其中&amp;查找部分原封不动的取代 12345678910111213141516171819202122232425262728293031[root@master datas]# cat test hadoophadAAphadBBphadCCphadDDpsed -i 's/had..p/&amp;s/g' test[root@master datas]# cat test hadoopshadAApshadBBpshadCCpshadDDps[root@master datas]# sed -i 's/\\(had..p\\)/\\1S/g' test [root@master datas]# cat test hadoopSshadAApSshadBBpSshadCCpSshadDDpSs# 部分取代进行修改，用括号括住引用部分[root@master datas]# sed -i 's/\\(had\\)...../\\1oop/g' test [root@master datas]# cat test hadoophadoophadoophadoophadoop 有变量时使用双引号或将变量用单引号引起来 awk报告生成器","link":"/post/e4677a0d.html"},{"title":"Shell之脚本功能概述","text":"需求描述: 实现一个脚本工具，该脚本提供类似supervisor功能， 可以对进程进行管理; 一键查看所有进程运行状态 单个或批量启动进程，单个或批量停止进程 提供进程分组功能，可以按组查看进行运行状态，可以按组启动或停止该组内所有进程 脚本拆分–函数 function get_all_process 返回“http nginx mysql datanode” 进程名称字符串 function get_all_group 返回组进程名称 function get_process_info 返回进程详细信息字符串 详细信息包括：运行状态、PID、CPU、MEM、启动时间 注：该函数可以接受一个参数，参数为进程名称 分解： function get_process_pid_by_name process function get_process_ny_pid process_id function get_all_process_by_group 返回进程组内所有进名称列表字符串 如 DB组-&gt;”mysql postgresql oracle” funcction get_group_by_pn 根据进程名称获取组名 判断函数 is_group_in_config/is_process_in_config 注意： ^$表示以空行开始 $# 参数的个数 $$ 表示子进程id $0 表示进程本身 shift命令左移，如果调用函数或运行脚本时，参数丢掉第一个 实现 status-app.sh12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#!/bin/bash# # Function: show group dataHOME_DIR=\"/opt/datasets/shell/demo01\"CONFIG_FILE=\"process.cfg\"function get_all_group{ glist=$(sed -n '/\\[GROUP_LIST\\]/,/\\[.*\\]/p' $HOME_DIR/$CONFIG_FILE | egrep -v '(^$|\\[.*\\])') echo $glist }# echo `get_all_group`function get_all_process{ for g in `get_all_group` do plist=$(sed -n \"/\\[$g\\]/,/\\[.*\\]/p\" $HOME_DIR/$CONFIG_FILE | egrep -v \"(^$|\\[.*\\])\") echo $plist done}function get_process_pid_by_name{ if [ $# -ne 1 ];then return 1 else pids=`ps -ef | grep $1 | grep -v $0 | grep -v grep | awk '{print $2}' echo $pids` fi}function get_process_info_by_pid{ if [ `ps -ef | awk -v pid=$1 '$2==pid{print }' | wc -l` -eq 1 ];then process_status=\"STARTED\" else process_status=\"STOPPEND\" fi process_cpu=`ps -aux | awk -v pid=$1 '$2==pid{print $3}'` process_mem=`ps -aux | awk -v pid=$1 '$2==pid{print $4}'` pro_start_time=`ps -p $1 -o lstart | grep -v STARTED`}function is_group_in_config{ for gn in `get_all_group` do if [ $gn == $1 ];then return fi done return 1}function get_all_process_by_group{ is_group_in_config $1 if [ $? -eq 1 ];then echo \"GroupName $1 is not in config...\" fi p_list=$(sed -n \"/\\[$1\\]/,/\\[.*\\]/p\" $HOME_DIR/$CONFIG_FILE | egrep -v \"(^$|\\[.*\\])\") echo $p_list}# get_process_pid_by_name $1# echo $pids# get_process_info_by_pid 1380# echo \"$process_status $process_cpu $process_mem $pro_start_time\"if [ ! -e $CONFIG_FILE ]; then echo \"$CONFIG_FILE not foud!\" exit 1figet_all_process_by_group DB# # # for g in `get_all_process`# do# echo $g# done 程序设计主流程1234app-status.sh 执行有三种情况 1 无参数 列出配置文件中所有进程的运行信息 2 -g GroupName 列出GroupName组内的所有进程 3 process_name1 列出指定进程的运行信息 s测试12345678sed -n '//,//p' | egrep -v '^$ || \\[.*\\]'[root@node1 ~]# ps -aux | grep mysqlmysql 1380 0.2 5.3 1280344 207416 ? Sl 15:06 0:26 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pidroot 4453 0.0 0.0 112728 996 pts/3 S+ 18:27 0:00 grep --color=auto mysql[root@node1 ~]# ps -p 1380 -o lstart STARTEDSat Jan 11 15:06:23 2020 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177#!/bin/bash# # Function: show group dataHOME_DIR=\"/opt/datasets/shell/demo01\"CONFIG_FILE=\"process.cfg\"this_pid=$$function get_all_group{ glist=$(sed -n '/\\[GROUP_LIST\\]/,/\\[.*\\]/p' $HOME_DIR/$CONFIG_FILE | egrep -v '(^$|\\[.*\\])') echo $glist }# echo `get_all_group`function get_all_process{ for g in `get_all_group` do plist=$(sed -n \"/\\[$g\\]/,/\\[.*\\]/p\" $HOME_DIR/$CONFIG_FILE | egrep -v '(^$|\\[.*\\])') echo $plist done}function get_process_pid_by_name{ if [ $# -ne 1 ];then return 1 else pids=`ps -ef | grep $1 | grep -v grep | grep -v $this_pid | awk -v pname=$1 '$1==pname{print $2}'` echo $pids fi}function get_process_info_by_pid{ if [ `ps -ef | awk -v pid=$1 '$2==pid{print }' | wc -l` -eq 1 ];then process_status=\"STARTED\" else process_status=\"STOPPEND\" fi process_cpu=`ps -aux | awk -v pid=$1 '$2==pid{print $3}'` process_mem=`ps -aux | awk -v pid=$1 '$2==pid{print $4}'` pro_start_time=`ps -p $1 -o lstart | grep -v STARTED`}function is_group_in_config{ for gn in `get_all_group` do if [ $gn == $1 ];then return fi done return 1}function get_all_process_by_group{ is_group_in_config $1 if [ $? -eq 1 ];then echo \"GroupName $1 is not in config...\" fi p_list=$(sed -n \"/\\[$1\\]/,/\\[.*\\]/p\" $HOME_DIR/$CONFIG_FILE | egrep -v \"(^$|\\[.*\\])\") echo $p_list}function get_group_by_pn{ for gn in `get_all_group` do for pn in `get_all_process_by_group $gn` do if [ $1 == $pn ];then echo $gn fi done done}function is_process_in_config{ for pn in `get_all_process` do if [ $1 == $pn ];then return fi done return 1}function format_output{ ps -ef | grep $1 |grep -v grep | grep -v $this_pid &amp;&gt; /dev/null if [ $? -eq 0 ];then pids=`get_process_pid_by_name $1` for pid in $pids do get_process_info_by_pid $pid awk -v p_name=$1 \\ -v g_name=$2 \\ -v p_status=$process_status \\ -v p_cpu=$process_cpu \\ -v p_mem=$process_mem \\ -v p_time=\"$pro_start_time\" \\ 'BEGIN{printf \"%-15s%-15s%-15s%-15s%-15s%-15s\\n\",p_name,g_name,p_status,p_cpu,p_mem,p_time}' done else awk -v p_name=$1 \\ -v g_name=$2 \\ -v p_status=$process_status \\ -v p_cpu=$process_cpu \\ -v p_mem=$process_mem \\ -v p_time=\"$pro_start_time\" \\ 'BEGIN{printf \"%-15s%-15s%-15s%-15s%-15s%-15s\\n\",p_name,g_name,\"NULL\",\"NULL\",\"NULL\",\"NULL\"}' fi}# get_process_pid_by_name $1# echo $pids# get_process_info_by_pid 1380# echo \"$process_status $process_cpu $process_mem $pro_start_time\"if [ ! -e $CONFIG_FILE ]; then echo \"$CONFIG_FILE not foud!\" exit 1fi # for pn in `get_all_process`# do# get_process_pid_by_name $pn# echo $pids# # doneif [ $# -gt 0 ];then if [ $1 == \"-g\" ];then shift for gn in $@ do is_group_in_config $gn || continue for pn in `get_all_process_by_group $gn` do format_output $pn $gn done done else for pn in $@ do is_process_in_config $pn || continue gname=`get_group_by_pn $pn` format_output $pn $gn done fielse for pn in `get_all_process` do gname=`get_group_by_pn $pn` format_output $pn $gname donefi## get_group_by_pn mysql## get_all_process_by_group DB1## ## ## for g in `get_all_process`## do## echo $g## done## get_process_info_by_pid 1380#get_process_pid_by_name mysql","link":"/post/cfaf6679.html"},{"title":"SpringMVC之HelloWorld","text":"Servlet启动加载，Servlet原本是第一次访问创建对象load-on-startup: 服务器启动的时候创建对象，值越小优先级越高 SpringMVC概述 Spring 为展现层提供的基于 MVC 设计理念的优秀的 Web 框架，是目前最主流的 MVC 框架之一。 Spring3.0 后全面超越 Struts2，成为最优秀的 MVC 框架。 Spring MVC 通过一套 MVC 注解，让 POJO（Plain Old Java Object） 成为处理请求的控制器，而无须实现任何接口。 支持 REST 风格的 URL 请求。 采用了松散耦合可插拔组件结构，比其他 MVC 框架更具扩展性和灵活性。 SpringMVC是什么 一种轻量级的、基于MVC的Web层应用框架。偏前端而不是基于业务逻辑层。 是Spring框架的一个后续产品。 Spring框架结构图(新版本) Spring框架结构图(老版本)： 以前的MVC和现在的MVC的区别 普通MVC 12345678910MVC: 新的软件架构模式M： Model模型，封装和映射数据（JavaBean）V: View视图，界面显示工作（JSP）C: Controller控制器： 控制整个网站的跳转逻辑（Servlet）MVC提倡：每一层只编写自己的东西，不写其他任何代码控制器{ 调用业务逻辑处理 调整到某个页面}分层为了解耦，解耦为了维护方便和分工合作 SpringMVC 1234SpringMVC多一个前端控制器，就相当于一个导诊台DispatchServlet是一个前端控制器，它是一个Servlet，应该在web.xml中配置这个Servlet来拦截所有请求 SpringMVC的主要控件 DispatcherServlet：前端控制器 Controller：处理器/页面控制器，做的是MVC中的C的事情，但控制逻辑转移到前端控制器了，用于对请求进行处理 HandlerMapping ：请求映射到处理器，找谁来处理，如果映射成功返回一个HandlerExecutiongChain对象（包含一个Handler处理器(页面控制器)对象、多个HandlerInterceptor拦截器对象） ViewResolver : 视图解析器，找谁来处理返回的页面。把逻辑视图解析为具体的View,进行这种策略模式，很容易更换其他视图技术； 如InternalResourceViewResolver将逻辑视图名映射为JSP视图 LocalResolver：本地化、国际化 MultipartResolver：文件上传解析器 HandlerExceptionResolver：异常处理器SpringMVC可以干什么 天生与Spring框架集成，如：(IOC,AOP) 支持Restful风格 进行更简洁的Web层开发 支持灵活的URL到页面控制器的映射 非常容易与其他视图技术集成，如:Velocity、FreeMarker等等。 因为模型数据不存放在特定的API里，而是放在一个Model里(Map数据结构实现，因此很容易被其他框架使用) 非常灵活的数据验证、格式化和数据绑定机制、能使用任何对象进行数据绑定，不必实现特定框架的API 更加简单、强大的异常处理 对静态资源的支持 支持灵活的本地化、主题等解析 SpringMVC需要的jar包1spring-webmvc-4.0.0.RELEASE.jar SpringMVC 实现HelloWorld操作步骤 eclipse console配置（可以不配置） 切换JavaEE视图 在Quick Access中找Package explore 调整custom perspective，使得鼠标邮件可以找到并创建 新建工作集 创建动态web工程 如果选择Dynamic web module version为3.0则需要自己创建web.xml或点击下一步勾选生成，2.5则不需要 创建Tomcat运行环境，并引入Tomcat环境 导包 SpringMVC是Spring的Web模块，所有模块的运行都依赖于核心模块（IOC模块）12345678spring-aop-4.0.0.RELEASE.jarspring-beans-4.0.0.RELEASE.jarspring-context-4.0.0.RELEASE.jarspring-core-4.0.0.RELEASE.jarspring-expression-4.0.0.RELEASE.jarcommons-logging-1.1.3.jarspring-web-4.0.0.RELEASE.jarspring-webmvc-4.0.0.RELEASE.jar 在 web.xml 中配置 DispatcherServlet(ALT+/) 123456789101112131415161718192021 &lt;!-- The front controller of this Spring Web application, responsible for handling all application requests --&gt;&lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- SpringMVC 配置文件的位置 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- Servlet启动加载，Servlet原本是第一次访问创建对象 load-on-startup: 服务器启动的时候创建对象，值越小优先级越高 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- Map all requests to the DispatcherServlet for handling --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 加入 Spring MVC 的配置文件：springmvc.xml 添加命名空间 增加配置 1&lt;context:component-scan base-package=&quot;xyz.lyhcc&quot;&gt;&lt;/context:component-scan&gt; 配置视图解析器 1234&lt;bean id=\"viewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/views/\"&gt;&lt;/property&gt; &lt;property name=\"suffix\" value=\".jsp\"&gt;&lt;/property&gt;&lt;/bean&gt; 需要创建一个入口页面，index.jsp(创建之前，修改JSP文件配置为UTF-8:菜单栏Windows-&gt;Preference-&gt;搜索JSP-&gt;修改编码)1&lt;a href=\"${pageContext.request.contextPath }/helloworld\"&gt;Hello World&lt;/a&gt; 编写处理请求的处理器，并标识为处理器 12345678910@Controllerpublic class HelloWorldController { @RequestMapping(value=\"/hello\", method=RequestMethod.GET) public String hello() { System.out.println(\"processing the request...\"); return \"success\"; }} 编写视图/WEB-INF/views/success.jsp1&lt;h4&gt;Sucess Page&lt;/h4&gt; 部署测试： HelloWorld的运行流程 客户端点击链接发送http://localhost:8080/springmvc/hello 来到Tomcat服务器 SpringMVC的前端控制器收到所有请求 来看请求地址和@RequestMapping标注匹配，来找到相应的方法 前端控制找到目标处理器类和目标方法，直接利用返回执行目标方法 方法执行完成会有返回值，SpringMVC认为这个返回值就是要去的地址 拿到方法返回值以后，用视图解析器进行拼串得到完整的页面地址 拿到页面地址，前端控制器帮我们转发到页面 如果前端控制器不指定配置文件位置 会去寻找默认的文件 前端控制器名-servlet.xml url-pattern说明 /: 拦截所有请求，不拦截jsp页面*.jsp /*:拦截所有请求，包括jsp 处理*.jsp是Tomcat做的事，所有项目的小web.xml都是集成大的web.xmlDefaultServlet是Tomcat周玲处理静态资源的，除了jsp,Servlet外都是静态资源 1) 服务器的大的web.xml中有一个DefaultServlet的url-pattern=/2) 自定义的配置中前端控制器url-pattern=/ 这样就会禁用父web.xml，静态资源就会来到DispatchServlet，看那个方法的RequestMapping是这个，所以此时的html会报错3) jsp可以访问是因为没有覆盖父web.xml的jspServlet4) /* 直接拦截所有请求，一般写/;也是为了迎合后来Rest风格的URL","link":"/post/1d06ef41.html"},{"title":"HiddenHttpMethodFilter (REST)","text":"概述 REST即表述性状态传递（英文：Representational State Transfer，简称REST）是Roy Fielding博士在2000年他的博士论文中提出来的一种软件架构风格。它是一种针对网络应用的设计和开发方式，可以降低开发的复杂性，提高系统的可伸缩性。目前在三种主流的Web服务实现方案中，因为REST模式的Web服务与复杂的SOAP和XML-RPC对比来讲明显的更加简洁，越来越多的web服务开始采用REST风格设计和实现。例如，Amazon.com提供接近REST风格的Web服务进行图书查找；雅虎提供的Web服务也是REST风格的。 参考资料 理解本真的REST架构风格 深入浅出REST 什么是REST风格 REST：即 Representational State Transfer。（资源）表现层状态转化 。是目前最流行的一种互联网软件架构。它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用 资源（Resources）：网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的存在。可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的 URI 。获取这个资源，访问它的URI就可以，因此 URI 即为每一个资源的独一无二的识别符。 表现层（Representation）：把资源具体呈现出来的形式，叫做它的表现层（Representation） 。比如，文本可以用 txt 格式表现，也可以用 HTML 格式、XML 格式、JSON 格式表现，甚至可以采用二进制格式。 状态转化（State Transfer） ：每发出一个请求，就代表了客户端和服务器的一次交互过程。HTTP协议，是一个无状态协议，即所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生“状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是 “表现层状态转化”。 具体说，就是 HTTP 协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET 用来获取资源，POST 用来新建资源，PUT 用来更新资源，DELETE 用来删除资源。 URL风格 1234/order/1 HTTP GET ：得到 id = 1 的 order /order/1 HTTP DELETE：删除 id = 1的 order /order/1 HTTP PUT：更新id = 1的 order /order HTTP POST：新增 order HiddenHttpMethodFilter 浏览器 form 表单只支持 GET 与 POST 请求，而DELETE、PUT 等 method 并不支持，Spring3.0 添加了一个过滤器，可以将这些请求转换为标准的 http 方法，使得支持 GET、POST、PUT 与 DELETE 请求。 实验代码 配置HiddenHttpMethodFilter过滤器 123456789&lt;filter&gt; &lt;filter-name&gt;HiddenHTTPMethodFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHTTPMethodFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 配置前端控制器 1234567891011 &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- Map all requests to the DispatcherServlet for handling --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 配置springmvc-servlet 12345&lt;context:component-scan base-package=\"xyz.lyhcc\"&gt;&lt;/context:component-scan&gt;&lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/\"&gt;&lt;/property&gt; &lt;property name=\"suffix\" value=\".jsp\"&gt;&lt;/property&gt;&lt;/bean&gt; 代码 1234567891011121314151617181920 @RequestMapping(value=\"/book/{id}\", method=RequestMethod.GET)public String RESTGet(@PathVariable(value=\"id\")Integer id) { System.out.println(\"GET \" + id); return \"success\";}@RequestMapping(value=\"/book\", method=RequestMethod.POST)public String RESTPost() { System.out.println(\"Post \"); return \"success\";}@RequestMapping(value=\"/book/{id}\", method=RequestMethod.PUT)public String RESTPut(@PathVariable(value=\"id\")Integer id) { System.out.println(\"Put \" + id); return \"success\";}@RequestMapping(value=\"/book/{id}\", method=RequestMethod.DELETE)public String RESTDelete(@PathVariable(value=\"id\")Integer id) { System.out.println(\"Delete \" + id); return \"success\";} 请求链接 123456789101112131415161718192021&lt;fieldset&gt; &lt;!-- 实验1 测试 REST风格 GET 请求 --&gt; &lt;a href=\"book/1\"&gt;testREST GET&lt;/a&gt;&lt;br/&gt;&lt;br/&gt; &lt;!-- 实验2 测试 REST风格 POST 请求 --&gt; &lt;form action=\"book\" method=\"POST\"&gt; &lt;input type=\"submit\" value=\"testRESTPost\"&gt; &lt;/form&gt; &lt;!-- 实验3 测试 REST风格 PUT 请求 --&gt; &lt;form action=\"book/1\" method=\"POST\"&gt; &lt;input type=\"hidden\" name=\"_method\" value=\"PUT\"&gt; &lt;input type=\"submit\" value=\"testRESTPut\"&gt; &lt;/form&gt; &lt;!-- 实验4 测试 REST风格 DELETE 请求 --&gt; &lt;form action=\"book/1\" method=\"POST\"&gt; &lt;input type=\"hidden\" name=\"_method\" value=\"DELETE\"&gt; &lt;input type=\"submit\" value=\"testRESTDelete\"&gt; &lt;/form&gt;&lt;/fieldset&gt; 源码分析1234567891011121314@Overrideprotected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { String paramValue = request.getParameter(this.methodParam); if (\"POST\".equals(request.getMethod()) &amp;&amp; StringUtils.hasLength(paramValue)) { String method = paramValue.toUpperCase(Locale.ENGLISH); HttpServletRequest wrapper = new HttpMethodRequestWrapper(request, method); filterChain.doFilter(wrapper, response); } else { filterChain.doFilter(request, response); }} 注意高版本Tomcat会出现一个问题 123&lt;!-- 在相应的位置添加isError=true --&gt;&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\" isErrorPage=\"true\"%&gt;","link":"/post/fdd838d6.html"},{"title":"SpringMVC之@RequestMapping","text":"@RequestMapping 映射请求注解 SpringMVC使用@RequestMapping注解为控制器指定可以处理哪些 URL 请求 在控制器的类定义及方法定义处都可标注 @RequestMapping 标记在类上：提供初步的请求映射信息。相对于 WEB 应用的根目录 标记在方法上：提供进一步的细分映射信息。相对于标记在类上的 URL。 若类上未标注 @RequestMapping，则方法处标记的 URL 相对于 WEB应用的根目录 作用：DispatcherServlet 截获请求后，就通过控制器上 @RequestMapping 提供的映射信息确定请求所对应的处理方法。 @RequestMapping源码参考12345678910111213package org.springframework.web.bind.annotation;@Target({ElementType.METHOD, ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Mappingpublic @interface RequestMapping { String[] value() default {}; RequestMethod[] method() default {}; String[] params() default {}; String[] headers() default {}; String[] consumes() default {}; String[] produces() default {};} @RequestMapping使用代码 界面连接 1&lt;a href=\"springmvc/helloworld\"&gt;test @RequestMapping&lt;/a&gt; 控制器方法 123456789101112131415161718@Controller //声明Bean对象，为一个控制器组件@RequestMapping(\"/springmvc\")public class HelloWorldController { /** * 映射请求的名称：用于客户端请求；类似Struts2中action映射配置的，action名称 *1 使用@RequestMapping 注解来映射请求的 URL *2 返回值会通过视图解析器解析为实际的物理视图, 对于 InternalResourceViewResolver 视图解析器, * 会做如下的解析: * 通过 prefix + returnVal + 后缀 这样的方式得到实际的物理视图, 然会做转发操作. * /WEB-INF/views/success.jsp */ @RequestMapping(value=\"/helloworld\") public String helloworld(){ System.out.println(\"hello,world\"); return \"success\"; //结果如何跳转呢？需要配置映射解析器 } } RequestMapping的映射请求方式 标准的HTTP请求头 映射请求参数、请求方法或请求头 @RequestMapping 除了可以使用请求 URL 映射请求外，还可以使用请求方法、请求参数及请求头映射请求@RequestMapping 的 value【重点】、method【重点】、params【了解】 及 heads【了解】分别表示请求 URL、请求方法、请求参数及请求头的映射条件，他们之间是与的关系，联合使用多个条件可让请求映射更加精确化。 params 和 headers支持简单的表达式 param1: 表示请求必须包含名为 param1 的请求参数 !param1: 表示请求不能包含名为 param1 的请求参数 param1 != value1: 表示请求包含名为 param1 的请求参数，但其值不能为 value1 {“param1=value1”, “param2”}: 请求必须包含名为 param1 和param2 的两个请求参数，且 param1 参数的值必须为 value1 produces:只接受内容类型是那种类型，规则请求头的Content-Type 告诉浏览器返回的内容类型是什么，给响应头中加上Content-Type:text/html;Charset=UTF-8 测试代码 定义控制器方法1234567@RequestMapping(value=\"/hello\", method=RequestMethod.GET) public String hello() { System.out.println(\"processing the request...\"); return \"success\"; }} 以GET方法请求1&lt;a href=\"springmvc/testMethord\"&gt;testMethord&lt;/a&gt; 出现异常 以POST发送请求123&lt;form action=\"springmvc/hello\" method=\"post\"&gt;&lt;input type=\"submit\" value=\"submit\"&gt;&lt;/form&gt; RequestMapping 映射请求参数和请求头 RequestMapping请求头&amp;请求参数1234567//了解: 可以使用 params 和 headers 来更加精确的映射请求. params 和 headers 支持简单的表达式.@RequestMapping(value=\"/testParamsAndHeaders\",params= {\"username\",\"age!=10\"}, headers = { \"Accept-Language=en-US,zh;q=0.8\" })public String testParamsAndHeaders(){ System.out.println(\"testParamsAndHeaders...\"); return \"success\"; } 可以更改Use-Agent使得只能在特定浏览器中访问 RequestMapping支持Ant路径风格 Ant风格资源支持3中匹配符【了解】 ?:匹配文件中的一个字符 *：匹配文件中任意字符 **：** 匹配多层路径 123456/user/*/createUser匹配 /user/aaa/createUser、/user/bbb/createUser 等 URL/user/**/createUser匹配 /user/createUser、/user/aaa/bbb/createUser 等 URL/user/createUser??匹配 /user/createUseraa、/user/createUserbb 等 URL 注意：Ant风格和精确匹配同时存在，优先选择精确匹配 实验 定义控制器方法12345678//Ant 风格资源地址支持 3 种匹配符//@RequestMapping(value=\"/testAntPath/*/abc\")//@RequestMapping(value=\"/testAntPath/**/abc\")@RequestMapping(value=\"/testAntPath/abc??\")public String testAntPath(){System.out.println(\"testAntPath...\");return \"success\";} 页面链接1234&lt;!-- Ant 风格资源地址支持 3 种匹配符 --&gt;&lt;a href=\"springmvc/testAntPath/*/abc\"&gt;testAntPath&lt;/a&gt;&lt;a href=\"springmvc/testAntPath/xxx/yyy/abc\"&gt;testAntPath&lt;/a&gt;&lt;a href=\"springmvc/testAntPath/abcxx\"&gt;testAntPath&lt;/a&gt; ReqeusetMapping映射请求占位符PathVariable注解 @PathVariable 映射 URL 绑定的占位符带占位符的 URL 是 Spring3.0 新增的功能，该功能在 SpringMVC 向 REST 目标挺进发展过程中具有里程碑的意义通过 @PathVariable 可以将 URL 中占位符参数绑定到控制器处理方法的入参中：URL 中的 {xxx} 占位符可以通过 @PathVariable(“xxx”) 绑定到操作方法的入参中。 实验 定义控制器方法12345@RequestMapping(value=\"/testPathVariable/{id}\")public String testPathVariable(@PathVariable(\"id\") Integer id){ System.out.println(\"testPathVariable...id=\"+id); return \"success\";} 定义页面链接12&lt;!-- 测试 @PathVariable --&gt;&lt;a href=\"springmvc/testPathVariable/1\"&gt;testPathVariable&lt;/a&gt;","link":"/post/aa7cd430.html"},{"title":"SpringMVC之响应数据传出","text":"Spring MVC除了在原生的request和session外还能怎么样把数据带给页面 1) 可以在方法处传入Map，或者Model或者ModelMap。给这些参数里面保存的所有数据都会放在所有的数据都会放在请求域中。可以在页面中获取 在PageContext、Request、Session、ApplicationMap(interface(jdk)) ModelMap(class)Model(interface(spring))2) 方法的返回值可以变为ModeAndView类型既包含视图信息（页面地址）也包含模型数据（给页面带的数据）而且数据是放在请求域中request、session、application3) Spring MVC提供了一种可以临时给Session域中保存数据的方式使用一个注解 @SessionAttributes(只能标注在类上)给BindingAwareModelMap中保存的数据，同时给session中放一份 Spring MVC输出模型数据概述 提供了以下几种途径输出模型数据： ModelAndView: 处理方法返回值类型为 ModelAndView 时, 方法体即可通过该对象添加模型数据 Map 及 Model: 入参为 org.springframework.ui.Model、org.springframework.ui.ModelMap 或 java.uti.Map 时，处理方法返回时，Map 中的数据会自动添加到模型中。 @SessionAttributes: 将模型中的某个属性暂存到 HttpSession 中，以便多个请求之间可以共享这个属性 @ModelAttribute: 方法入参标注该注解后, 入参的对象就会放到数据模型中 处理模型数据之 ModelAndView 控制器处理方法的返回值如果为 ModelAndView, 则其既包含视图信息，也包含模型数据信息。 添加模型数据:12MoelAndView addObject(String attributeName, Object attributeValue)ModelAndView addAllObject(Map&lt;String, ?&gt; modelMap) 设置视图:12void setView(View view)void setViewName(String viewName) 实验代码 控制器方法123456@RequestMapping(\"testModelAndView\")public ModelAndView testModelAndView() { ModelAndView modelAndView = new ModelAndView(\"success\"); modelAndView.addObject(\"time\", new Date().toString());//放到requestScope中 return modelAndView;} 页面链接1&lt;a href=\"testModelAndView\"&gt;testModelAndView&lt;/a&gt; 成功页面，显示数据1time:${requestScope.time } 处理数据之Map Spring MVC 在内部使用了一个 org.springframework.ui.Model 接口存储模型数据 具体使用步骤 1) Spring MVC 在调用方法前会创建一个隐含的模型对象作为模型数据的存储容器。 2) 如果方法的入参为 Map 或 Model 类型，Spring MVC 会将隐含模型的引用传递给这些入参。 3) 在方法体内，开发者可以通过这个入参对象访问到模型中的所有数据，也可以向模型中添加新的属性数据 123456789101112131415161718@RequestMapping(\"testMap\")public String testMap(Map&lt;String, Object&gt;map) { System.out.println(\"testMap-&gt;\"+map.getClass()); map.put(\"map\", \"map\"); return \"testMap\";}@RequestMapping(\"testModel\")public String testModel(Model model) { System.out.println(\"testModel-&gt;\"+model.getClass()); model.addAttribute(\"model\", \"model\"); return \"testmodel\";}@RequestMapping(\"testModelMap\")public String testModelMap(ModelMap map) { System.out.println(\"testModelMap-&gt;\"+map.getClass()); map.put(\"modelmap\", \"modelmap\"); return \"testmodelMap\";} 界面显示相应的数据获取以及访问链接 123456http://localhost:8200/SpringMVC_Output/testModelhttp://localhost:8200/SpringMVC_Output/testMaphttp://localhost:8200/SpringMVC_Output/testModelMapmap: ${requestScode.map }model: ${requestScode.model }ModelMap: ${requestScope.modelmap } 处理模型数据之@SessionAttributes（推荐不用） 若希望在多个请求之间共用某个模型属性数据，则可以在控制器类上标注一个 @SessionAttributes, Spring MVC 将在模型中对应的属性暂存到 HttpSession 中。 @SessionAttributes 除了可以通过属性名指定需要放到会话中的属性外，还可以通过模型属性的对象类型指定哪些模型属性需要放到会话中 例如： @SessionAttributes(types=User.class) 会将隐含模型中所有类型为 User.class 的属性添加到会话中。 @SessionAttributes(value={“user1”, “user2”}) @SessionAttributes(types={User.class, Dept.class}) @SessionAttributes(value={“user1”, “user2”}, types={Dept.class}) SessionAttributes源码1234567891011121314151617package org.springframework.web.bind.annotation; import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Inherited;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target; @Target({ElementType.TYPE}) //说明这个注解只能应用在类型上面@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface SessionAttributes { String[] value() default {}; //推荐使用 Class&lt;?&gt;[] types() default {}; //范围太广 } 实验代码 类上添加注解123456789@Controller@SessionAttributes(value = {\"modelmap\"},types= {String.class})/** * @SessionAttributes * 除了可以通过属性名指定需要放到会话中的属性外(实际上是通过value指定key值)， * 还可以通过模型属性的对象类型指定哪些模型属性需要放到会话中(实际上是通过types指定类型) * 注意：只能放在类的上面，不能修饰方法 */public class OutputController { 注意： 123④异常//org.springframework.web.HttpSessionRequiredException: Session attribute &apos;user&apos; required - not found in session//出现这个异常，是@SessionAttributes(value={&quot;user&quot;},types={String.class})导致的，去掉类上的这个注解","link":"/post/f51ba3e7.html"},{"title":"SpringMVC 请求数据传入","text":"请求处理方法签名 Spring MVC 对控制器处理方法签名的限制是很宽松的，几乎可以按喜欢的任何方式对方法进行签名。 必要时可以对方法及方法入参标注相应的注解（ @PathVariable 、@RequestParam、@RequestHeader 等）、 Spring MVC 框架会将 HTTP 请求的信息绑定到相应的方法入参中，并根据方法的返回值类型做出相应的后续处理。 @RequestParam注解 在处理方法入参处使用 @RequestParam 可以把请求参数传递给请求方法 value：参数名 required：是否必须。默认为 true, 表示请求参数中必须包含对应的参数，若不存在，将抛出异常 defaultValue: 默认值，当没有传递参数时使用该值 实验代码 增加控制方法12345 @RequestMapping(\"testRequestParam\")public String testRequestParam(@RequestParam(value=\"username\")String username, @RequestParam(value=\"age\")Integer age) { System.out.println(\"testRequestParam---username:\"+username+\",age:\"+age); return \"success\";} 页面链接代码1&lt;a href=\"testRequestParam?username=atguigu&amp;age=10\"&gt;testRequestParam&lt;/a&gt; @RequestHeader注解 使用 @RequestHeader 绑定请求报头的属性值 请求头包含了若干个属性，服务器可据此获知客户端的信息，通过 @RequestHeader 即可将请求头中的属性值绑定到处理方法的入参中 也有三个参数value、defaultValue、required 实验代码 控制器代码12345@RequestMapping(\"testRequestHeader\")public String testRequestHeader(@RequestHeader(value=\"User-Agent\")String aa) { System.out.println(\"testRequestHeader---UserAgent\" + aa); return \"success\";} 页面链接1&lt;a href=\"testRequestHeader\"&gt;testRequestHeader&lt;/a&gt; @CookieValue注解 使用 @CookieValue 绑定请求中的 Cookie 值 @CookieValue 可让处理方法入参绑定某个 Cookie 值 控制器代理12345 @RequestMapping(\"testCookieMethod\")private String testCookieMethod(@CookieValue(value=\"JSESSIONID\")String sessionID) { System.out.println(\"Cookie:JSESSIONID=\"+sessionID); return \"success\";} 页面链接1&lt;a href=\"testCookieMethod\"&gt;testCookieMethod&lt;/a&gt; 使用POJO作为参数 使用 POJO 对象绑定请求参数值 Spring MVC 会按请求参数名和 POJO 属性名进行自动匹配，自动为该对象填充属性值。支持级联属性。如：dept.deptId、dept.address.tel 等 控制器方法 12345@RequestMapping(\"testPOJO\")public String testPOJO(Person person) { System.out.println(person); return \"success\";} 表单页面代码 12345678&lt;form action=\" testPOJO\" method=\"POST\"&gt; username: &lt;input type=\"text\" name=\"name\"/&gt;&lt;br&gt; password: &lt;input type=\"password\" name=\"pwd\"/&gt;&lt;br&gt; email: &lt;input type=\"text\" name=\"email\"/&gt;&lt;br&gt; gender: &lt;input type=\"text\" name=\"gender\"/&gt;&lt;br&gt; &lt;input type=\"submit\" value=\"Submit\"/&gt;&lt;/form&gt; 增加实体类 12345678public class Person { private String name; private String pwd; private String gender; private String email; //getter/setter} 使用POJO出现乱码如何解决？ 如果中文有乱码，需要配置字符编码过滤器，且配置其他过滤器之前，如（HiddenHttpMethodFilter），否则不起作用。（思考method=”get”请求的乱码问题怎么解决的）12345678请求乱码： GET请求： 改server.xml，在8080端口处URIEncoding=”UTF-8” POST请求： 在第一次请求参数之前设置 Request.setCharacterEncoding(“UTF-8”) 自定义Filter, SpringMVC封装有org.springframework.web.filter.CharacterEncodingFilter响应乱码： Response.setContentType(“text/html;charset=utf-8”) 过滤器配置 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 使用Servlet原生API作为参数 MVC 的 Handler 方法可以接受哪些 ServletAPI 类型的参数 1) HttpServletRequest 2) HttpServletResponse 3) HttpSession 4) java.security.Principal 5) Locale 6) InputStream 7) OutputStream 8) Reader 9) Writer 源码参考：AnnotationMethodHandlerAdapter L866 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 @Overrideprotected Object resolveStandardArgument(Class&lt;?&gt; parameterType, NativeWebRequest webRequest) throws Exception { HttpServletRequest request = webRequest.getNativeRequest(HttpServletRequest.class); HttpServletResponse response = webRequest.getNativeResponse(HttpServletResponse.class); if (ServletRequest.class.isAssignableFrom(parameterType) || MultipartRequest.class.isAssignableFrom(parameterType)) { Object nativeRequest = webRequest.getNativeRequest(parameterType); if (nativeRequest == null) { throw new IllegalStateException( \"Current request is not of type [\" + parameterType.getName() + \"]: \" + request); } return nativeRequest; } else if (ServletResponse.class.isAssignableFrom(parameterType)) { this.responseArgumentUsed = true; Object nativeResponse = webRequest.getNativeResponse(parameterType); if (nativeResponse == null) { throw new IllegalStateException( \"Current response is not of type [\" + parameterType.getName() + \"]: \" + response); } return nativeResponse; } else if (HttpSession.class.isAssignableFrom(parameterType)) { return request.getSession(); } else if (Principal.class.isAssignableFrom(parameterType)) { return request.getUserPrincipal(); } else if (Locale.class.equals(parameterType)) { return RequestContextUtils.getLocale(request); } else if (InputStream.class.isAssignableFrom(parameterType)) { return request.getInputStream(); } else if (Reader.class.isAssignableFrom(parameterType)) { return request.getReader(); } else if (OutputStream.class.isAssignableFrom(parameterType)) { this.responseArgumentUsed = true; return response.getOutputStream(); } else if (Writer.class.isAssignableFrom(parameterType)) { this.responseArgumentUsed = true; return response.getWriter(); } return super.resolveStandardArgument(parameterType, webRequest);} 实验代码","link":"/post/4c7891d3.html"},{"title":"Java NIO","text":"Java NIO实例 Java基本套接字基本操作 连接远程机器。 发送数据。 接收数据。 关闭连接。 绑定端口。 监听入站数据。 在所绑定端口.上接受来自远程机器的连接。 其中，前四项用于客户端，后六项用于服务器，最后三项只有服务器才需要，即等待客户端的连接，这些操作通过ServerSocket类实现。 Java NIO基础概述 NIO解决一客户一线程所带来的开销大的问题所谓一客户一线程是指，多个客户端访问同一个服务器进程时，服务器需要为每个请求创建一个服务线程 并且非堵塞是NIO实现的重要功能之一，为了实现非堵塞，NIO引入了 选择器 Selector 通道 Channel通道表示到实体(如硬件设备、文件、网络套接字或者可以执行一个或多个不同的的IO操作)的程序组件开放连接 通道可以注册一个选择器实例，通过该实例的select方法，用户可以询问“在一个或一组通道中，哪一个是当前需要的服务（即被读、写或被接受）“在一个准备好的通道执行相应的I/O操作，就不需要等待，也就不会堵塞了 NIO与IO的区别 I0 NIO 面向流(Stream Oriented) 面向缓冲区(Buffer Oriented) 阻塞I0(Blocking I0) 非阻塞I0(Non Blocking I0) (无) 选择器(Selectors) 缓冲区 NIO中一个主要的特性是java.nio.Buffer。缓冲区(Buffer)提供了一个比流抽象的、更高效和可预测的I/O。Buffer 代表了一个有限容量的容器一其本质是一个数组，通道Channel使用Buffer实例来传输数据Buffer包含4个索引 capacity: 缓冲区总容量，可通过Buffer.capacity获取,并且是不可修改 position: 缓冲区位置，即下一个要写入或读取的索引，获取/设置通过position()/position(int) limit: 缓冲区限制，即第一个不应该读取或写入的位置，获取/设置通过limit()/limit(int) mark: 缓冲区位置标记，通过mark()设置一个位置，reset()方法被调用后，position被置为mark 遵循如下规则 0 ≤ mark ≤ position ≤ limit ≤ capacity ByteBuffer的创建1234567//直接创建缓冲区public static ByteBuffer allocate (int capacity) //在某个字节数组上创建缓冲区public static ByteBuffer wrap (byte[] array)//上面的调用下面的//上一个方法的返回 wrap(array, 0, array.length)public static ByteBuffer wrap (byte[] array, int offset, int length) 这种方式与流的区别：流是单向的，而这种方式看读可写 ByteBuffer的读写 put()/get()方法：基于相对位置和绝对位置的读写基于相对位置就是基于目前缓冲区位置position的当前值，从“下一个”位置读取或存放数据，并为position增加适当的值。 绝对位置的put)/get()方法，必须提供写入/读出的位置, 该方式的读写操作，不改变position的值 1234567891011//相对位置public byte get ()public ByteBuffer get (byte[] dst)public ByteBuffer get (byte[] dst, int offset，int length)public ByteBuffer put (byte b)public final ByteBuffer put (byte[] src)public ByteBuffer put (byte[] src, int offset, int length)public ByteBuffer put (ByteBuffer src)//绝对位置public byte get (int index)public ByteBuffer put (int index, byte b) 需要注意的是 ，部分数据的get()/put()是不允许的。以写人为例，如果要写入的数据量超过当前缓冲区允许写入的数据量(可通过Buffer.remaining()方法获得该值)，则所有的数据都不会写入缓冲区，position的位置不变，put()方法抛出BufferOverflowException异常。 工具方法 clear() 通过clear()方法，缓冲区的poistion被设置为0, limit 设置为capacity,这样，缓冲区准备好接收新数据。后续的put()/readO调用，将数据从第-一个元素开始填入缓冲区，最多直到填满该缓冲区，达到limit位置(等于capacity)。 flip()方法用于将缓冲区准备为数据传出状态，该方法将limit设置为position后，将position 设置为0。后续的get()/write()方法将从缓冲区的第一个元素开始传出数据，直到limit位置。通过fip0方法和get(/write()方法配合，可以将前面利用put()/read)方法放入缓冲区的所有数据读出(一直读到limit)。 rewind)方法将position设置为0，但不改变limit的值，如果需要多次读取缓冲区里的数据，可以在两次读取间使用rewind()方法。 compact()方法将position和limit间的数据复制到缓冲区的开始位置，为后续的put()/read()调用让出空间。调用结束后，poistion的值被设置为数据的长度，也就是原来的limit减去position的值，而limit则设置为capacity。和clear()、fip() 等方法不同，compact() 不但改变了position 和limit的位置，还改变了缓冲区中的数据。 compact()主要用于在缓冲区中还有未写出的数据时，为读入数据准备空间:即在write()方法调用后和添加新数据的read)方法前调用compact()方法，将未写出的“剩余”数据移动到缓冲区前面，为后面read()方法提供释放空间。在图中，假设write()方法调用后，缓冲区处于“某工作状态”，这时，position 到limit间的数据为未写出的“剩余”数据，而limit到capacity的空间则是read)方法可以使用的空间，通过compact()操作，position 到limit 间的数据被挪到缓冲区的前面，position 的位置也被设置为“剩余”数据长度。接下来，开发人员就可以直接调用read0/put0方法，从position位置开始放入数据。该数据和原来的“剩余”数据- -起，构成了连续的可用数据。 Buffer还支持一些其他功能，如直接缓冲区(directbuffer)、Java基本类型的put()/get()、缓冲区共享、复制、透视、字符编码转换等 通道 一个Channel的实例代表一个 和设备的 连接 对象的创建 ServerSocketChannel/SocketChannel通过工厂方法创建 1234public static SocketChannel open() throws IOExceptionpublic static SocketChannel open(SocketAddress remote) throws IOExceptionpublic static ServerSocketChannel open() throws IOException SocketChannel创建后，可以通过connect()连接到远程机器，通过close()关闭连接，这些操作和Socket的没有什么差别。 数据的读写123456public int read (ByteBuffer dst)public long read (ByteBuffer[] dsts, int offset, int length)public final long read (ByteBuffer[] dsts)public int write (ByteBuffer src)public long write (ByteBuffer[] srcs, int offset, int length)public final long write (ByteBuffer[] srcs) 和socket类似的操作，可以通过socket方法获取ServerSocket对象 12public SocketChannel accept ()public ServerSocket socket () 是否支持工作在非阻塞状态 Buffer使用阻塞方法相对于基本套接字没有什么优点 12public SelectableChannel configureBlocking (boolean block) //设置堵塞public boolean isBlocking() //是否堵 非阻塞的SocketChannel 的connect()方法会立即返回，用户必须通过isConnected()判断连接是否已经建立，或者通过finishConnect()方法在非阻塞套接字上阻塞等待连接成功:非阻塞的read(),在Socket上没有数据的时候，立即返回(返回值为0)，不会等待;非阻塞的acceptO,如果没有等待的连接，将返回null 12public boolean isConnected ()public boolean finishConnect () 选择器 选择器(Selector) 的使用方法:通过静态的工厂方法创建Selector实例，通过Channel的注册方法，将Selector实例注册到想要监控的Channel实例上，最后调用选择器的select()方法。该方法会阻塞等待，直到有一个或多个通道准备好I/O操作或超时。select() 方法将返回可进行I/O操作的通道数量。现在，在一个单独的线程中，就可以检查多个通道是否可以进行I/O操作，不需要为每一一个通道都准备一个线程了。 选择器的打开关闭123public static Selector open ()public boolean isOpen ()public abstract void close () 获取/设置标志位选择器注册标记SelectionKey维护联的信息保存在java.nio.channels.SelectionKey实例中 OP_ READ (通道上有数据可读) OP_ WRITE (通道已经可写) OP_ CONNECT (通道连接已建立) OP_ ACCEPT (通道上有连接请求) 12public int interestOps ()public SelectionKey interestops (int ops) SeverSocketChannel/SocketChannel配合工作的API1234public SelectionKey register (Selector sel, int ops) // 注册public SelectionKey register (Selector sel, int ops, object att)//带附件注册public SelectionKey keyFor (Selector sel) // 根据选择器，查找对应的selectionKeypublic boolean isRegistered() // 判断通道是否已经注册 选择器实例selector.上注册了- -个SocketChannel对象，支持读操作 1SelectionKey readKey = channel . register (selector, SelectionKey.OP_READ) 注销/获取选择器123Selector selector () //获取选择器selectableChannel channel () //获取相应的channelvoid cancel () //注销选择器 select方法1234/**如果发现select()的返回值大于0，表明有需要处理的I/O事件发生**/public int select()// 阻塞等待，直到一个注册通道有感兴趣的操作就绪public int select (1ong timeout)// 等待一段时间，或一个注册通道有感兴趣的操作就绪public int selectNow()// 非阻塞版本 获取相关建的方法12public Set&lt;SelectionKey&gt; keys () //selector上已注册的所有键public Set &lt;SelectionKey&gt; selectedKeys()// 已选键集 Hadoop IPC上的使用 判断通道上等待操作的方法12345public int readyOps ()public boolean isReadable ()public boolean iswritable ()public boolean isConnectable ()public boolean isAcceptable () 与附件有关的另外两个方法12public Object attach (object ob) //添加附件public object attachment () //获取附件 以上内容来自《Hadoop技术内幕 深入解析HADOOP COMMON和HDFS架构设计与实现原理》","link":"/post/f2d80d11.html"},{"title":"SpringMVC之视图解析器","text":"SpringMVC如何解析视图概述 不论控制器返回一个String,ModelAndView,View都会转换为ModelAndView对象，由视图解析器解析视图，然后，进行页面的跳转。 视图解析源码分析：重要的两个接口 断点调式流程图 视图和视图解析器 请求处理方法执行完成后，最终返回一个 ModelAndView 对象。对于那些返回 String，View 或 ModeMap 等类型的处理方法，Spring MVC 也会在内部将它们装配成一个 ModelAndView 对象 ，它包含了逻辑名和模型对象的视图 Spring MVC 借助 视图解析器（ViewResolver） 得到最终的视图对象（View），最终的视图可以是 JSP ，也可能是 Excel、JFreeChart等各种表现形式的视图 对于最终究竟采取何种视图对象对模型数据进行渲染，处理器并不关心，处理器工作重点聚焦在生产模型数据的工作上，从而实现 MVC 的充分解耦 视图 视图的作用是渲染模型数据，将模型里的数据以某种形式呈现给客户。 为了实现视图模型和具体实现技术的解耦，Spring 在 org.springframework.web.servlet 包中定义了一个高度抽象的 View 接口： 视图对象由视图解析器负责实例化。由于视图是无状态的，所以他们不会有线程安全的问题 常用的视图实现类 视图解析器 SpringMVC 为逻辑视图名的解析提供了不同的策略，可以在 Spring WEB 上下文中配置一种或多种解析策略，并指定他们之间的先后顺序。每一种映射策略对应一个具体的视图解析器实现类。 视图解析器的作用比较单一：将逻辑视图解析为一个具体的视图对象。 所有的视图解析器都必须实现 ViewResolver 接口： 常用的视图解析类实现类 程序员可以选择一种视图解析器或混用多种视图解析器 每个视图解析器都实现了 Ordered 接口并开放出一个 order 属性，可以通过 order 属性指定解析器的优先顺序，order 越小优先级越高。 SpringMVC 会按视图解析器顺序的优先顺序对逻辑视图名进行解析，直到解析成功并返回视图对象，否则将抛出 ServletException 异常 InternalResourceViewResolver JSP 是最常见的视图技术，可以使用 InternalResourceViewResolve作为视图解析器： Spring MVC视图解析器之前缀forward1234567891011121314151617181920212223@RequestMapping(\"hello\")public String hello() { System.out.println(\"---------------------\"); return \"./success\";}/** * 转发到页面 * /success.jsp转发到当前项目的success * 一定要加上/，如果不加就是相对路径 * forward:/hello.jsp forward前缀不会有配置的前缀后缀拼接 * @return */@RequestMapping(\"handle01\")public String handle01() { System.out.println(\"handle01\"); return \"forward:/success.jsp\";}@RequestMapping(\"handle02\")public String handle02() { System.out.println(\"handle02\"); return \"forward:/handle01\";} 重定向redirect1234567891011121314151617181920/** * 重定向到success.jsp页面 * 转发 forward: 转发的路径 * 重定向 redirect： 重定向的路径 * /success.jsp代表就是从当前项目下开始，SpringMVC会为路径自动的拼接上项目名 * * 原生的servlet重定向需要添加项目名 * response.sendRedirect(\"/success.jsp\") * @return */@RequestMapping(\"/handle03\")public String handle03() { System.out.println(\"handle03\"); return \"redirect:/success.jsp\";}@RequestMapping(\"/handle04\")public String handle04() { System.out.println(\"handle04\"); return \"redirect:/handle03\";} JstlView 导包导入jstl的时候自动创建为一个jstlView，可以快速方便支持国际化 JavaWeb国际化步骤 得到一个Locale对象 使用ResourceBundle绑定国际化资源文件 使用ResourceBundle.getString(key);获取到国际化配置文件中的值 Web页面国际化，fmt标签来做123&lt; fmt:setLocale&gt;&lt; fmt:setBundle &gt;&lt;fmt:message&gt; 使用jstlView 让Spring管理国际化资源 去页面取值 若项目中使用了JSTL，则SpringMVC 会自动把视图由InternalResourceView转为 JstlView （断点调试，将JSTL的jar包增加到项目中，视图解析器会自动修改为JstlView） 若使用 JSTL 的 fmt 标签则需要在 SpringMVC 的配置文件中配置国际化资源文件 若希望直接响应通过 SpringMVC 渲染的页面，可以使用 mvc:view-controller 标签实现 *注意：使用jstl国际化是，页面映射不能带前缀 * 实验代码 i18n.properties i18n_en_US.properties i18n_zh_CN.properties i18n.username=username i18n.password=password i18n.username=Username i18n.password=Password i18n.username=\\u7528\\u6237\\u540D i18n.password=\\u5BC6\\u7801 1. 增加jstl标签 jar包（断点调试，这时的View对象就是JstlView） 2. 设置国际化资源文件 123&lt;bean id=\"messageSource\" class=\"org.springframework.context.support.ResourceBundleMessageSource\"&gt; &lt;property name=\"basename\" value=\"i18n\"&gt;&lt;/property&gt;&lt;/bean&gt; 3. 控制器代码 1234@RequestMapping(\"/login\")public String login() { return \"login\";} 4. 成功页面(/success.jsp)使用fmt标签库 12345 &lt;form action=\"\"&gt; &lt;fmt:message key=\"i18n.username\"&gt;&lt;/fmt:message&gt;：&lt;input /&gt; &lt;fmt:message key=\"i18n.password\"&gt;&lt;/fmt:message&gt;： &lt;input /&gt; &lt;input type=\"submit\" value='&lt;fmt:message key='i18n.login'&gt;&lt;/fmt:message&gt;' /&gt;&lt;/form&gt; ## mvc:view-controller标签 - 若希望直接响应通过 SpringMVC 渲染的页面，可以使用 mvc:view-controller 标签实现 12&lt;!-- 直接配置响应的页面：无需经过控制器来执行结果 --&gt;&lt;mvc:view-controller path=\"/success\" view-name=\"success\"/&gt; - 请求的路径： 1http://localhost:8080/SpringMVC_02_View/success - 配置mvc:view-controller会导致其他请求路径失效 - 解决办法： 12&lt;!-- 在实际开发过程中都需要配置mvc:annotation-driven标签，后面讲，这里先配置上 --&gt;&lt;mvc:annotation-driven/&gt; ## 自定义视图 - 自定义视图（需要加入SpringMVC，那么，一定需要实现框架的接口） - 若希望使用 Excel 展示数据列表，仅需要扩展 SpringMVC 提供的 AbstractExcelView 或 AbstractJExcelView 即可。 - 实现 buildExcelDocument() 方法，在方法中使用模型数据对象构建 Excel 文档就可以了。 - AbstractExcelView 基于 POI API，而 AbstractJExcelView 是基于 JExcelAPI 的。 - 视图对象需要配置 IOC 容器中的一个 Bean，使用 BeanNameViewResolver 作为视图解析器即可 - 若希望直接在浏览器中直接下载 Excel 文档，则可以设置响应头 Content-Disposition 的值为 attachment;filename=xxx.xls ## 实验代码 1. 测试链接 1http://localhost:8200/SpringMVC_voewResolver/definitionView 2. 控制方法 12345@RequestMapping(\"/definitionView\")public String test() { System.out.println(\"definitionView\"); return \"hello:time\";} 3. 自定义视图 123456789101112131415public class MyView implements View { @Override public String getContentType() { return \"text/html\"; } @Override public void render(Map&lt;String, ?&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception { response.getWriter().write(\"This is MyView, And Time is \" + new Date().toString()); }} 4. 自定义视图解析器 123456789101112131415161718192021public class MyResourceResolver implements ViewResolver, Ordered { private int order; @Override public View resolveViewName(String viewName, Locale locale) throws Exception { if (viewName.startsWith(\"hello:\")) { return new MyView(); } return null; } @Override public int getOrder() { return this.order; } public void setOrder(int order) { this.order = order; }} 5. 声明视图解析器 123&lt;bean class=\"xyz.lyhcc.MyResourceResolver\"&gt; &lt;property name=\"order\" value=\"10\"&gt;&lt;/property&gt;&lt;/bean&gt; —— 注意：InternalResourceViewResolver默认的优先级：private int order = Integer.MAX_VALUE; 视图解析器源码解析 方法执行后的返回值会作为页面地址参考，转发或者重定向到页面 视图解析器可能会进行页面地址拼串 任何方法的返回值，最终都会被包装成ModelAndView对象 processDispatchResult(HttpServletRequest request, HttpServletResponse response,HandlerExecutionChain mappedHandler, ModelAndView mv, Exception exception) 视图渲染流程：将域中的数据页面显示，页面就是渲染模型数据 render(mv, request, response); View与ViewResolver: ViewResolver对象就是根据视图名返回View对象 怎么能根据方法的返回值得到View对象？ 12345678910111213 protected View resolveViewName(String viewName, Map&lt;String, Object&gt; model, Locale locale, HttpServletRequest request) throws Exception { //遍历所有resolver for (ViewResolver viewResolver : this.viewResolvers) { //ViewResolver视图解析器根据方法的返回值，得到一个View对象 View view = viewResolver.resolveViewName(viewName, locale); if (view != null) { return view; } } return null;} viewResolver.resolveViewName细节 12345678910111213141516171819202122232425262728293031 @Overridepublic View resolveViewName(String viewName, Locale locale) throws Exception { if (!isCache()) { return createView(viewName, locale); } else { Object cacheKey = getCacheKey(viewName, locale); View view = this.viewAccessCache.get(cacheKey); if (view == null) { synchronized (this.viewCreationCache) { view = this.viewCreationCache.get(cacheKey); if (view == null) { // Ask the subclass to create the View object. /*****创建******/ view = createView(viewName, locale); if (view == null &amp;&amp; this.cacheUnresolved) { view = UNRESOLVED_VIEW; } if (view != null) { this.viewAccessCache.put(cacheKey, view); this.viewCreationCache.put(cacheKey, view); if (logger.isTraceEnabled()) { logger.trace(\"Cached view [\" + cacheKey + \"]\"); } } } } } return (view != UNRESOLVED_VIEW ? view : null); }} 创建View的细节 12345678910111213141516171819202122 @Override protected View createView(String viewName, Locale locale) throws Exception { // If this resolver is not supposed to handle the given view, // return null to pass on to the next resolver in the chain. if (!canHandle(viewName, locale)) { return null; } // Check for special \"redirect:\" prefix. /********/ if (viewName.startsWith(REDIRECT_URL_PREFIX)) { String redirectUrl = viewName.substring(REDIRECT_URL_PREFIX.length()); RedirectView view = new RedirectView(redirectUrl, isRedirectContextRelative(), isRedirectHttp10Compatible()); return applyLifecycleMethods(viewName, view); } // Check for special \"forward:\" prefix. if (viewName.startsWith(FORWARD_URL_PREFIX)) { String forwardUrl = viewName.substring(FORWARD_URL_PREFIX.length()); return new InternalResourceView(forwardUrl); } // Else fall back to superclass implementation: calling loadView. return super.createView(viewName, locale);} **视图解析器得到View的流程** &gt; 所有配置的视图解析器都来尝试根据视图名（返回值）得到View对象 &gt; 如果能得到就返回，得不到就换下一个视图解析器 &gt; 再调用View对象的Render, View中的render得到 12345678910111213 @Override public void render(Map&lt;String, ?&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception { if (logger.isTraceEnabled()) { logger.trace(\"Rendering view with name '\" + this.beanName + \"' with model \" + model + \" and static attributes \" + this.staticAttributes); } Map&lt;String, Object&gt; mergedModel = createMergedOutputModel(model, request, response); prepareResponse(request, response); //渲染页面输出的所有数据 renderMergedOutputModel(mergedModel, request, response);} renderMergedOutputModel位于InternalResourceView 1234567891011121314151617181920212223242526272829303132333435363738394041 @Overrideprotected void renderMergedOutputModel( Map&lt;String, Object&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception { // Determine which request handle to expose to the RequestDispatcher. HttpServletRequest requestToExpose = getRequestToExpose(request); // *** Expose the model object as request attributes. //将模型的数据放到request域中 exposeModelAsRequestAttributes(model, requestToExpose); // Expose helpers as request attributes, if any. exposeHelpers(requestToExpose); // Determine the path for the request dispatcher. String dispatcherPath = prepareForRendering(requestToExpose, response); // Obtain a RequestDispatcher for the target resource (typically a JSP). RequestDispatcher rd = getRequestDispatcher(requestToExpose, dispatcherPath); if (rd == null) { throw new ServletException(\"Could not get RequestDispatcher for [\" + getUrl() + \"]: Check that the corresponding file exists within your web application archive!\"); } // If already included or response already committed, perform include, else forward. if (useInclude(requestToExpose, response)) { response.setContentType(getContentType()); if (logger.isDebugEnabled()) { logger.debug(\"Including resource [\" + getUrl() + \"] in InternalResourceView '\" + getBeanName() + \"'\"); } rd.include(requestToExpose, response); } else { // Note: The forwarded resource is supposed to determine the content type itself. if (logger.isDebugEnabled()) { logger.debug(\"Forwarding to resource [\" + getUrl() + \"] in InternalResourceView '\" + getBeanName() + \"'\"); } rd.forward(requestToExpose, response); }} 为什么数据可以在请求域中获得 1234567891011121314151617181920 protected void exposeModelAsRequestAttributes(Map&lt;String, Object&gt; model, HttpServletRequest request) throws Exception { for (Map.Entry&lt;String, Object&gt; entry : model.entrySet()) { String modelName = entry.getKey(); Object modelValue = entry.getValue(); if (modelValue != null) { request.setAttribute(modelName, modelValue); if (logger.isDebugEnabled()) { logger.debug(\"Added model object '\" + modelName + \"' of type [\" + modelValue.getClass().getName() + \"] to request in view with name '\" + getBeanName() + \"'\"); } } else { request.removeAttribute(modelName); if (logger.isDebugEnabled()) { logger.debug(\"Removed model object '\" + modelName + \"' from request in view with name '\" + getBeanName() + \"'\"); } } }} 视图解析器只是为了获得视图对象；视图对象才能真正的转发(将模型数据全部放在请求域中)或者重定向到页面视图对象才能真正渲染视图源码执行流程","link":"/post/652cd08.html"},{"title":"DOM解析","text":"说明：这里主要分析hadoop的DOM解析 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档处理步骤 获得用于创建DOM解析器的工厂对象1DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); 可以设置一下参数[可选]12345678docBuilderFactory.setIgnoringComments(true);docBuilderFactory.setNamespaceAware(true);boolean useXInclude = !wrapper.isParserRestricted();try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28);} 获得解析XML的DocumentBuilder对象1DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); 获取根节点下的所有节点1NodeList props = root.getChildNodes(); 遍历节点123456789101112for(int i = 0; i &lt; props.getLength(); ++i) { //获取节点 Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; //prop.getTagName()获取节点内的值 if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { ... } hadoop配置文件解析的特别说明 对DocumentBuilderFactory做的处理 忽略XML文档中的注释1docBuilderFactory.setIgnoringComments(true); 支持XML命名空间1docBuilderFactory.setNamespaceAware(true); 支持XML包含机制12345try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \")for parser \" + docBuilderFactory, var28);} XInclude机制允许将XML文档分解为多个可管理的块，然后将-一个或多个较小的文档组装成一个大型文档。 hadoop 配置文件解析完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157private Configuration.Resource loadResource(Properties properties, Configuration.Resource wrapper, boolean quiet) { String name = \"Unknown\"; try { Object resource = wrapper.getResource(); name = wrapper.getName(); DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); docBuilderFactory.setIgnoringComments(true); docBuilderFactory.setNamespaceAware(true); boolean useXInclude = !wrapper.isParserRestricted(); try { docBuilderFactory.setXIncludeAware(useXInclude); } catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28); } if (wrapper.isParserRestricted()) { docBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true); } DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); Document doc = null; Element root = null; boolean returnCachedProperties = false; if (resource instanceof URL) { doc = this.parse(builder, (URL)resource); } else if (resource instanceof String) { URL url = this.getResource((String)resource); doc = this.parse(builder, url); } else if (resource instanceof Path) { File file = (new File(((Path)resource).toUri().getPath())).getAbsoluteFile(); if (file.exists()) { if (!quiet) { LOG.debug(\"parsing File \" + file); } doc = this.parse(builder, new BufferedInputStream(new FileInputStream(file)), ((Path)resource).toString()); } } else if (resource instanceof InputStream) { doc = this.parse(builder, (InputStream)resource, (String)null); returnCachedProperties = true; } else if (resource instanceof Properties) { this.overlay(properties, (Properties)resource); } else if (resource instanceof Element) { root = (Element)resource; } if (root == null) { if (doc == null) { if (quiet) { return null; } throw new RuntimeException(resource + \" not found\"); } root = doc.getDocumentElement(); } Properties toAddTo = properties; if (returnCachedProperties) { toAddTo = new Properties(); } if (!\"configuration\".equals(root.getTagName())) { LOG.fatal(\"bad conf file: top-level element not &lt;configuration&gt;\"); } NodeList props = root.getChildNodes(); Configuration.DeprecationContext deprecations = (Configuration.DeprecationContext)deprecationContext.get(); for(int i = 0; i &lt; props.getLength(); ++i) { Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { if (wrapper.isParserRestricted() &amp;&amp; \"http://www.w3.org/2001/XInclude\".equals(prop.getNamespaceURI())) { throw new RuntimeException(\"Error parsing resource \" + wrapper + \": XInclude is not supported for restricted resources\"); } LOG.warn(\"Unexpected tag in conf file \" + wrapper + \": expected &lt;property&gt; but found &lt;\" + prop.getTagName() + \"&gt;\"); } NodeList fields = prop.getChildNodes(); String attr = null; String value = null; boolean finalParameter = false; LinkedList&lt;String&gt; source = new LinkedList(); //遍历所有节点，并根据情况设置对象的成员变量properties和finalParameters for(int j = 0; j &lt; fields.getLength(); ++j) { Node fieldNode = fields.item(j); if (fieldNode instanceof Element) { Element field = (Element)fieldNode; if (\"name\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { attr = StringInterner.weakIntern(((Text)field.getFirstChild()).getData().trim()); } if (\"value\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { value = StringInterner.weakIntern(((Text)field.getFirstChild()).getData()); } if (\"final\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData()); } if (\"source\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { source.add(StringInterner.weakIntern(((Text)field.getFirstChild()).getData())); } } } source.add(name); if (attr != null) { if (deprecations.getDeprecatedKeyMap().containsKey(attr)) { Configuration.DeprecatedKeyInfo keyInfo = (Configuration.DeprecatedKeyInfo)deprecations.getDeprecatedKeyMap().get(attr); keyInfo.clearAccessed(); String[] arr$ = keyInfo.newKeys; int len$ = arr$.length; for(int i$ = 0; i$ &lt; len$; ++i$) { String key = arr$[i$]; this.loadProperty(toAddTo, name, key, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } else { this.loadProperty(toAddTo, name, attr, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } } } } if (returnCachedProperties) { this.overlay(properties, toAddTo); return new Configuration.Resource(toAddTo, name, wrapper.isParserRestricted()); } else { return null; } } catch (IOException var29) { LOG.fatal(\"error parsing conf \" + name, var29); throw new RuntimeException(var29); } catch (DOMException var30) { LOG.fatal(\"error parsing conf \" + name, var30); throw new RuntimeException(var30); } catch (SAXException var31) { LOG.fatal(\"error parsing conf \" + name, var31); throw new RuntimeException(var31); } catch (ParserConfigurationException var32) { LOG.fatal(\"error parsing conf \" + name, var32); throw new RuntimeException(var32); }}","link":"/post/e9fcf702.html"},{"title":"SpringMVC之RESTful实现CRUD","text":"RESTRUL_CRUD_需求显示所有员工信息 URI: emps 请求方式：GET 显示效果 添加员工信息 显示添加页面： URI：emp 请求方式：GET 显示效果 添加员工信息： URI：emp 请求方式：POST 显示效果：完成添加，重定向到 list 页面。 删除操作 URL：emp/{id} 请求方式：DELETE 删除后效果：对应记录从数据表中删除 修改操作：lastName 不可修改！ 显示修改页面 URI：emp/{id} 请求方式：GET 显示效果：回显表单。 修改员工信息 URI：emp 请求方式：PUT 显示效果：完成修改，重定向到 list 页面。 相关的类 省略了Service层 实体类：Employee、Department Controller：EmployeeController Dao：EmployeeDao、DepartmentDao 相关页面 list.jsp input.jsp edit.jsp RESTRUL_CRUD_显示所有员工信息搭建开发环境 导包 123456789101112131415com.springsource.net.sf.cglib-2.2.0.jarcom.springsource.org.aopalliance-1.0.0.jarcom.springsource.org.aspectj.weaver-1.6.8.RELEASE.jarspring-aop-4.0.0.RELEASE.jarspring-aspects-4.0.0.RELEASE.jarcommons-logging-1.1.3.jarspring-beans-4.0.0.RELEASE.jarspring-context-4.0.0.RELEASE.jarspring-core-4.0.0.RELEASE.jarspring-expression-4.0.0.RELEASE.jarspring-jdbc-4.0.0.RELEASE.jarspring-orm-4.0.0.RELEASE.jarspring-tx-4.0.0.RELEASE.jarspring-web-4.0.0.RELEASE.jarspring-webmvc-4.0.0.RELEASE.jar 创建配置文件:springmvc.xml 增加context,mvc,beans名称空间。 配置核心控制器 123456789101112131415&lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- Map all requests to the DispatcherServlet for handling --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 将 POST 请求转换为 PUT 或 DELETE 请求以及字符过滤器 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 创建相关页面 12/WEB-INF/views/list.jspindex.jsp 增加实体类 增加Dao EmployeeDao 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package xyz.lyhcc.dao;import java.util.Collection;import java.util.HashMap;import java.util.Map;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Repository;import xyz.lyhcc.bean.Department;import xyz.lyhcc.bean.Employee;@Repositorypublic class EmployeeDao { private static Map&lt;Integer, Employee&gt; employees = null; @Autowired private DepartmentDao departmentDao; static{ employees = new HashMap&lt;Integer, Employee&gt;(); employees.put(1001, new Employee(1001, \"E-AA\", \"aa@163.com\", 1, new Department(101, \"D-AA\"))); employees.put(1002, new Employee(1002, \"E-BB\", \"bb@163.com\", 1, new Department(102, \"D-BB\"))); employees.put(1003, new Employee(1003, \"E-CC\", \"cc@163.com\", 0, new Department(103, \"D-CC\"))); employees.put(1004, new Employee(1004, \"E-DD\", \"dd@163.com\", 0, new Department(104, \"D-DD\"))); employees.put(1005, new Employee(1005, \"E-EE\", \"ee@163.com\", 1, new Department(105, \"D-EE\"))); } private static Integer initId = 1006; public void save(Employee employee) { if (employee.getId() == null) { employee.setId(initId++); } employee.setDepartment(departmentDao.getDepartment(employee.getDepartment().getId())); employees.put(employee.getId(), employee); } public Collection&lt;Employee&gt; getAll() { return employees.values(); } public Employee get(Integer id) { return employees.get(id); } public void delete(Integer id) { employees.remove(id); }} DeaprtmentDao123456789101112131415161718192021222324252627282930313233343536package xyz.lyhcc.bean;public class Department { private Integer id; private String departmentName; public Department() { super(); } public Department(Integer id, String departmentName) { super(); this.id = id; this.departmentName = departmentName; } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getDepartmentName() { return departmentName; } public void setDepartmentName(String departmentName) { this.departmentName = departmentName; } } 显示所有员工信息 页面链接 1&lt;a href=\"empList\"&gt;To Employee List&lt;/a&gt; 增加处理器 123456789101112@Controllerpublic class MyController { @Autowired EmployeeDao employeeDao; @RequestMapping(\"/retrieveEmps\") public String retrieveEmps(Map&lt;String, Object&gt; map) { map.put(\"empList\", employeeDao.getAll()); return \"list\"; }} SpringMVC中没遍历的标签，需要使用jstl标签进行集合遍历增加jstl标签库jar包 123456789101112131415161718192021222324252627&lt;table border=\"1\" cellpadding=\"10\" cellspacing=\"0\"&gt; &lt;tr&gt; &lt;td&gt;EmpId&lt;/td&gt; &lt;td&gt;LastName&lt;/td&gt; &lt;td&gt;Gender&lt;/td&gt; &lt;td&gt;Email&lt;/td&gt; &lt;td&gt;DepartmentName&lt;/td&gt; &lt;td&gt;Edit&lt;/td&gt; &lt;td&gt;Delete&lt;/td&gt; &lt;/tr&gt; &lt;c:forEach items=\"${requestScope.empList }\" var=\"emp\"&gt; &lt;tr&gt; &lt;td&gt;${emp.id } &lt;/td&gt; &lt;td&gt;${emp.lastName }&lt;/td&gt; &lt;td&gt;${emp.gender==0?\"女\": \"男\" }&lt;/td&gt; &lt;td&gt;${emp.email }&lt;/td&gt; &lt;td&gt;${emp.department.departmentName }&lt;/td&gt; &lt;td&gt; &lt;a href=\"\"&gt;Edit&lt;/a&gt; &lt;/td&gt; &lt;td&gt; &lt;a href=\"\"&gt;Delete&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt;&lt;/table&gt;&lt;a href=\"toaddpage\"&gt;Add Employee&lt;/a&gt; 使用 Spring的表单标签 通过 SpringMVC 的表单标签可以实现将模型数据中的属性和 HTML 表单元素相绑定，以实现表单数据更便捷编辑和表单值的回显 form 标签 一般情况下，通过 GET 请求获取表单页面，而通过 POST 请求提交表单页面，因此获取表单页面和提交表单页面的 URL 是相同的。 只要满足该最佳条件的契约，form:form 标签就无需通过 action 属性指定表单提交的 URL 可以通过 modelAttribute 属性指定绑定的模型属性，若没有指定该属性，则默认从 request 域对象中读取 command 的表单 bean，如果该属性值也不存在，则会发生错误。 SpringMVC 提供了多个表单组件标签，如 form:input/、form:select/ 等，用以绑定表单字段的属性值，它们的共有属性如下： path：表单字段，对应 html 元素的 name 属性，支持级联属性 htmlEscape：是否对表单值的 HTML 特殊字符进行转换，默认值为 true cssClass：表单组件对应的 CSS 样式类名 cssErrorClass：表单组件的数据存在错误时，采取的 CSS 样式 form:input、form:password、form:hidden、form:textarea：对应 HTML 表单的 text、password、hidden、textarea 标签 form:radiobutton：单选框组件标签，当表单 bean 对应的属性值和 value 值相等时，单选框被选中 form:radiobuttons：单选框组标签，用于构造多个单选框 items：可以是一个 List、String[] 或 Map itemValue：指定 radio 的 value 值。可以是集合中 bean 的一个属性值 itemLabel：指定 radio 的 label 值 delimiter：多个单选框可以通过 delimiter 指定分隔符 form:checkbox：复选框组件。用于构造单个复选框 form:checkboxs：用于构造多个复选框。使用方式同 form:radiobuttons 标签 form:select：用于构造下拉框组件。使用方式同 form:radiobuttons 标签 form:option：下拉框选项组件标签。使用方式同 form:radiobuttons 标签 form:errors：显示表单组件或数据校验所对应的错误 &lt;form:errors path= “*” /&gt; ：显示表单所有的错误 &lt;form:errors path= “user*” /&gt; ：显示所有以 user 为前缀的属性对应的错误 &lt;form:errors path= “username” /&gt; ：显示特定表单对象属性的错误 RESTRUL_CRUD_添加操作&amp;表单标签 在list.jsp上增加连接 1&lt;a href=\"empInput\"&gt;Add Employee&lt;/a&gt; 增加处理器方法 12345678910111213@RequestMapping(value=\"toaddpage\")public String addEmpPage(Model model) { Collection&lt;Department&gt; departments = departmentDao.getDepartments(); model.addAttribute(\"depts\", departments); //增加一个属性 return \"add\";} @RequestMapping(value=\"increaseEmp\",method=RequestMethod.POST)public String increaseEmp(Employee employee, Map&lt;String, Object&gt;map) { employeeDao.save(employee); return \"redirect:/retrieveEmps\";} 显示添加页面 非SpringMVC的form标签 123456789101112131415&lt;% pageContext.setAttribute(\"ctp\", request.getContextPath()); %&gt;&lt;form action=\"increaseEmp\" method=\"POST\"&gt; lastName: &lt;input type=\"text\" name=\"lastName\" /&gt; email: &lt;input type=\"text\" name=\"email\" /&gt; gender: 男&lt;input type=\"radio\" name=\"gender\" value=\"1\"/&gt; 女&lt;input type=\"radio\" name=\"gender\" value=\"0\"/&gt; &lt;select name=\"department.id\"&gt; &lt;c:forEach items=\"${depts }\" var=\"dept\"&gt; &lt;option value=\"${dept.id }\"&gt;${dept.departmentName }&lt;/option&gt; &lt;/c:forEach&gt; &lt;/select&gt; &lt;input type=\"submit\" value=\"提交\" /&gt;&lt;/form&gt; SpringMVC的form标签123456789101112 &lt;form:form action=\"increaseEmp\" method=\"POST\" modelAttribute=\"employee\"&gt; &lt;!-- java.lang.IllegalStateException: Neither BindingResult nor plain target object for bean name 'command' available as request attribute --&gt; LastName: &lt;form:input path=\"lastName\"/&gt;&lt;/br&gt;&lt;/br&gt; Email: &lt;form:input path=\"email\"/&gt;&lt;/br&gt;&lt;/br&gt; Gender: 男&lt;form:radiobutton path=\"gender\" value=\"1\"/&gt; 女&lt;form:radiobutton path=\"gender\" value=\"0\"/&gt; &lt;br/&gt; &lt;br/&gt; Department：&lt;form:select path=\"department.id\" items=\"${depts }\" itemLabel=\"departmentName\" itemValue=\"id\"&gt;&lt;/form:select&gt; &lt;br/&gt; &lt;input type=\"submit\" value=\"提交\" /&gt;&lt;/form:form&gt; 注意： 注意modelAttribute的使用，这里没有在域中使用添加对应的属性会报错ava.lang.IllegalStateException: Neither BindingResult nor plain target object for bean name ‘command’ available as request attribute RESTRUL_CRUD_删除操作&amp;处理静态资源 页面链接 RESTRUL_CRUD_修改操作 根据id查询员工对象，表单回显 页面链接 1&lt;td&gt;&lt;a href=\"empEdit/${emp.id }\"&gt;Edit&lt;/a&gt;&lt;/td&gt; 控制器方法 123456 @RequestMapping(value=\"empEdit/{id}\", method=RequestMethod.GET)public String editEmp(@PathVariable(value=\"id\") Integer id, Map&lt;String, Object&gt;map) { map.put(\"employee\", employeeDao.get(id)); map.put(\"depts\", departmentDao.getDepartments()); return \"edit\";} 修改页面 12345678910111213141516171819202122232425262728293031&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%&gt;&lt;%@ taglib prefix=\"form\" uri=\"http://www.springframework.org/tags/form\" %&gt;&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;title&gt;员工信息修改界面&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form:form action=\"${pageContext.request.contextPath }/empUpdate\" method=\"POST\" modelAttribute=\"employee\"&gt; &lt;input type=\"hidden\" name=\"id\" value=\"${employee.id }\" /&gt; &lt;input type=\"hidden\" name=\"_method\" value=\"PUT\"&gt; Email : &lt;form:input path=\"email\" /&gt; &lt;br&gt; &lt;br&gt; Gender: 男&lt;form:radiobutton path=\"gender\" value=\"1\"/&gt; 女&lt;form:radiobutton path=\"gender\" value=\"0\"/&gt; &lt;br/&gt; &lt;br/&gt; DeptName : &lt;form:select path=\"department.id\" items=\"${depts }\" itemLabel=\"departmentName\" itemValue=\"id\"&gt;&lt;/form:select&gt; &lt;br&gt; &lt;br&gt; &lt;input type=\"submit\" value=\"Submit\"&gt;&lt;br&gt;&lt;br&gt; &lt;/form:form&gt; &lt;/body&gt;&lt;/html&gt; 提交表单，修改数据123456789101112131415161718@RequestMapping(value=\"empUpdate\",method=RequestMethod.PUT)public String updateEmp(Employee employee) { System.out.println(employee); employeeDao.save(employee); return \"redirect:/retrieveEmps\";}@ModelAttributepublic void myModelAttribute(@RequestParam(value=\"id\", required=false) Integer id, Model model ) { if(id!=null) { Employee employee = employeeDao.get(id); model.addAttribute(\"employee\", employee); } } RESTRUL_CRUD_删除操作&amp;处理静态资源删除实验代码 页面链接 1&lt;td&gt;&lt;a href=\"/empDelete/${emp.id }\"&gt;Delete&lt;/a&gt;&lt;/td&gt; 控制器方法 12345@RequestMapping(value=\"delEmp/{id}\", method=RequestMethod.DELETE)public String delEmp(@PathVariable(value=\"id\") Integer id) { employeeDao.delete(id); return \"redirect:/retrieveEmps\";} HiddenHttpMethodFilter过滤器 发起请求，无法执行，因为delete请求必须通过post请求转换为delete请求，借助：HiddenHttpMethodFilter过滤器 需要使用jQuery来转换请求方式 加入jQuery库文件1/scripts/jquery-1.9.1.min.js jQuery库文件不起作用1警告: No mapping found for HTTP request with URI [/SpringMVC_03_RESTFul_CRUD/scripts/jquery-1.9.1.min.js] in DispatcherServlet with name &apos;springDispatcherServlet&apos; 解决方法，SpringMVC 处理静态资源 SpringMVC 处理静态资源: a)为什么会有这样的问题: 优雅的 REST 风格的资源URL 不希望带 .html 或 .do 等后缀 若将 DispatcherServlet 请求映射配置为 /, 则 Spring MVC 将捕获 WEB 容器的所有请求, 包括静态资源的请求, SpringMVC 会将他们当成一个普通请求处理, 因找不到对应处理器将导致错误。b)解决: 在 SpringMVC 的配置文件中配置 mvc:default-servlet-handler/ 配置后，原来的请求又不好使了12&lt;!-- 在前端控制器对应的配置文件，增加 --&gt;&lt;mvc:annotation-driven /&gt; 关于mvc:default-servlet-handler/作用 mvc:default-servlet-handler/ 将在 SpringMVC 上下文中定义一个 DefaultServletHttpRequestHandler，它会对进入 DispatcherServlet 的请求进行筛查，如果发现是没有经过映射的请求，就将该请求交由 WEB 应用服务器默认的 Servlet 处理，如果不是静态资源的请求，才由 DispatcherServlet 继续处理一般 WEB 应用服务器默认的 Servlet 的名称都是 default。若所使用的 WEB 服务器的默认 Servlet 名称不是 default，则需要通过 default-servlet-name 属性显式指定参考：CATALINA_HOME/config/web.xml 12345678910111213&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 该标签属性default-servlet-name默认值是”default”,可以省略。mvc:default-servlet-handler/ 1&lt;mvc:default-servlet-handler default-servlet-name=\"default\"/&gt; 通过jQuery转换为DELETE请求1234567891011121314&lt;td&gt;&lt;a class=\"delete\" href=\"empDelete/${emp.id }\"&gt;Delete&lt;/a&gt;&lt;/td&gt;&lt;form action=\"\" method=\"post\"&gt;&lt;input type=\"hidden\" name=\"_method\" value=\"DELETE\"/&gt;&lt;/form&gt;&lt;script type=\"text/javascript\" src=\"scripts/jquery-1.9.1.min.js\"&gt;&lt;/script&gt;&lt;script type=\"text/javascript\"&gt;$(function(){$(\".delete\").click(function(){var href = $(this).attr(\"href\");$(\"form\").attr(\"action\",href).submit();return false ;});});&lt;/script&gt;","link":"/post/14dd2a73.html"},{"title":"SpringMVC源码分析","text":"Spring MVC中DispatchServlet运行流程 ​ doDispatch()详细细节 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try { ModelAndView mv = null; Exception dispatchException = null; try { //1、 检查是否文件上传请求 processedRequest = checkMultipart(request); multipartRequestParsed = processedRequest != request; /****** Determine handler for the current request.*/ //2、根据当前的请求地址找到那个类能来处理 mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) { //3、如果没有找到处理器可以处理当前请求，抛出异常 noHandlerFound(processedRequest, response); return; } // Determine handler adapter for the current request. //4、拿到可以执行这个处理器类的所有方法的适配器（反射工具） HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) { long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) { String requestUri = urlPathHelper.getRequestUri(request); logger.debug(\"Last-Modified value for [\" + requestUri + \"] is: \" + lastModified); } if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) { return; } } if (!mappedHandler.applyPreHandle(processedRequest, response)) { return; } try { /************ ***注意：控制器执行******* ************* */ //5、适配器来回显目标方法：将目标方法执行完成后的返回值作为视图名，设置保存到ModelAndView //目标方法无论怎么写，最终适配器执行完后，都会封装为ModelAndView // Actually invoke the handler. mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); } finally { if (asyncManager.isConcurrentHandlingStarted()) { return; } } applyDefaultViewName(request, mv);//如果没有视图名，取默认 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException = ex; } /************ ***注意：转发目标页面执行******* ************* */ //6、根据方法最终执行完成后封装的ModelAndView，转发到对应页面，而且ModelAndView中的数据可以从请求域中获取 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Error err) { triggerAfterCompletionWithError(processedRequest, response, mappedHandler, err); } finally { if (asyncManager.isConcurrentHandlingStarted()) { // Instead of postHandle and afterCompletion mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); return; } // Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } 所有请求过来DispatchServlet收到请求 调用doDispatch() 方法进行处理 getHandler(): 根据当前请求地址找到能处理这个请求的的目标的类（处理器） ​ 根据当前请求在HandlerMapping中找到这个请求的映射信息，获取到目标处理器类 getHandlerAdapter(）：根据当前处理器获取到能执行这个处理器方法的适配器 根据当前处理器类，找到当前类的HandlerAdapter 使用当前获取到的适配器（RequestMappingHandlerAdapter）执行目标方法 目标方法执行后会返回一个ModelAndView对象 根据ModelAndView的信息转发到具体的页面，并可以在请求域中取出ModelAndView中的模型数据 getHandler()细节：怎么根据当前请求就能找到哪个类能处理 getHandler() 会返回目标处理器类的执行链 1234567891011121314protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { //HandlerMappings处理器映射：他里面保存了每一个处理器能处理那些方法的映射信息 for (HandlerMapping hm : this.handlerMappings) { if (logger.isTraceEnabled()) { logger.trace( \"Testing handler map [\" + hm + \"] in DispatcherServlet with name '\" + getServletName() + \"'\"); } HandlerExecutionChain handler = hm.getHandler(request); if (handler != null) { return handler; } } return null; } handlerMap: ioc容器自动创建Controller对象的时候扫描每个处理器都能处理什么请求，保存在HandlerMapping的handlerMap属性中：下一次请求过来就来查看那个handlerMapping中有相应的请求映射信息 getHandlerAdapter() 细节(如何找到目标处理器类的适配器？) 123456789101112protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException { for (HandlerAdapter ha : this.handlerAdapters) { if (logger.isTraceEnabled()) { logger.trace(\"Testing handler adapter [\" + ha + \"]\"); } if (ha.supports(handler)) { return ha; } } throw new ServletException(\"No adapter for handler [\" + handler + \"]: The DispatcherServlet configuration needs to include a HandlerAdapter that supports this handler\"); } 研究HandlerMappings和HandlerAdapters如何获取值， 可以在web.xml中修改DispatchServlet某些属性的默认配置 1234567891011121314151617181920212223242526272829303132private void initHandlerMappings(ApplicationContext context) { this.handlerMappings = null; if (this.detectAllHandlerMappings) { // Find all HandlerMappings in the ApplicationContext, including ancestor contexts. Map&lt;String, HandlerMapping&gt; matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false); if (!matchingBeans.isEmpty()) { this.handlerMappings = new ArrayList&lt;HandlerMapping&gt;(matchingBeans.values()); // We keep HandlerMappings in sorted order. OrderComparator.sort(this.handlerMappings); } } else { try { HandlerMapping hm = context.getBean(HANDLER_MAPPING_BEAN_NAME, HandlerMapping.class); this.handlerMappings = Collections.singletonList(hm); } catch (NoSuchBeanDefinitionException ex) { // Ignore, we'll add a default HandlerMapping later. } } // Ensure we have at least one HandlerMapping, by registering // a default HandlerMapping if no other mappings are found. if (this.handlerMappings == null) { this.handlerMappings = getDefaultStrategies(context, HandlerMapping.class); if (logger.isDebugEnabled()) { logger.debug(\"No HandlerMappings found in servlet '\" + getServletName() + \"': using default\"); } } } 组件的初始化： ​ 有些组件是通过类型找，有些是通过id ​ 去容器中找这个组件，如果没有找到就用默认配置 如何调用目标方法？ DispatcherServlet.doDispatch(HttpServletRequest, HttpServletResponse) line: 945 AnnotationMethodHandlerAdapter.handle(HttpServletRequest, HttpServletResponse, Object) line: 406 1234567891011121314151617181920protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //拿到方法解析器 ServletHandlerMethodResolver methodResolver = getMethodResolver(handler); //方法解析器根据当前请求地址找到真正的方法 Method handlerMethod = methodResolver.resolveHandlerMethod(request); //创建方法执行器 ServletHandlerMethodInvoker methodInvoker = new ServletHandlerMethodInvoker(methodResolver); //包装原生的response和request ServletWebRequest webRequest = new ServletWebRequest(request, response); //创建隐含模型 ExtendedModelMap implicitModel = new BindingAwareModelMap(); //真正执行目标方法 //目标方法执行期间利用反射执行期间确定参数值 Object result = methodInvoker.invokeHandlerMethod(handlerMethod, handler, webRequest, implicitModel); ModelAndView mav = methodInvoker.getModelAndView(handlerMethod, handler.getClass(), result, implicitModel, webRequest); methodInvoker.updateModelAttributes(handler, (mav != null ? mav.getModel() : null), implicitModel, webRequest); return mav; } AnnotationMethodHandlerAdapter$ServletHandlerMethodInvoker(HandlerMethodInvoker).invokeHandlerMethod 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public final Object invokeHandlerMethod(Method handlerMethod, Object handler, NativeWebRequest webRequest, ExtendedModelMap implicitModel) throws Exception { Method handlerMethodToInvoke = BridgeMethodResolver.findBridgedMethod(handlerMethod); try { boolean debug = logger.isDebugEnabled(); for (String attrName : this.methodResolver.getActualSessionAttributeNames()) { Object attrValue = this.sessionAttributeStore.retrieveAttribute(webRequest, attrName); if (attrValue != null) { implicitModel.addAttribute(attrName, attrValue); } } /*************/ /* 获取Modelattribute标注的方法 */ /* */ /*************/ for (Method attributeMethod : this.methodResolver.getModelAttributeMethods()) { //先确定modelattribute方法执行是要使用的每一个参数的值 Method attributeMethodToInvoke = BridgeMethodResolver.findBridgedMethod(attributeMethod); //args Object[] args = resolveHandlerArguments(attributeMethodToInvoke, handler, webRequest, implicitModel); if (debug) { logger.debug(\"Invoking model attribute method: \" + attributeMethodToInvoke); } String attrName = AnnotationUtils.findAnnotation(attributeMethod, ModelAttribute.class).value(); if (!\"\".equals(attrName) &amp;&amp; implicitModel.containsAttribute(attrName)) { continue; } ReflectionUtils.makeAccessible(attributeMethodToInvoke); Object attrValue = attributeMethodToInvoke.invoke(handler, args); if (\"\".equals(attrName)) { Class&lt;?&gt; resolvedType = GenericTypeResolver.resolveReturnType(attributeMethodToInvoke, handler.getClass()); attrName = Conventions.getVariableNameForReturnType(attributeMethodToInvoke, resolvedType, attrValue); } if (!implicitModel.containsAttribute(attrName)) { implicitModel.addAttribute(attrName, attrValue); } } Object[] args = resolveHandlerArguments(handlerMethodToInvoke, handler, webRequest, implicitModel); if (debug) { logger.debug(\"Invoking request handler method: \" + handlerMethodToInvoke); } ReflectionUtils.makeAccessible(handlerMethodToInvoke); return handlerMethodToInvoke.invoke(handler, args); } catch (IllegalStateException ex) { // Internal assertion failed (e.g. invalid signature): // throw exception with full handler method context... throw new HandlerMethodInvocationException(handlerMethodToInvoke, ex); } catch (InvocationTargetException ex) { // User-defined @ModelAttribute/@InitBinder/@RequestMapping method threw an exception... ReflectionUtils.rethrowException(ex.getTargetException()); return null; } } 确定方法运行时使用的每一个参数值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146private Object[] resolveHandlerArguments(Method handlerMethod, Object handler, NativeWebRequest webRequest, ExtendedModelMap implicitModel) throws Exception { Class&lt;?&gt;[] paramTypes = handlerMethod.getParameterTypes(); //创建了一个和参数个数一样多的数组，用来保存每一个参数的值 Object[] args = new Object[paramTypes.length]; for (int i = 0; i &lt; args.length; i++) { MethodParameter methodParam = new MethodParameter(handlerMethod, i); methodParam.initParameterNameDiscovery(this.parameterNameDiscoverer); GenericTypeResolver.resolveParameterType(methodParam, handler.getClass()); String paramName = null; String headerName = null; boolean requestBodyFound = false; String cookieName = null; String pathVarName = null; String attrName = null; boolean required = false; String defaultValue = null; boolean validate = false; Object[] validationHints = null; int annotationsFound = 0; Annotation[] paramAnns = methodParam.getParameterAnnotations(); //拿到所有注解，解析注解 for (Annotation paramAnn : paramAnns) { if (RequestParam.class.isInstance(paramAnn)) { RequestParam requestParam = (RequestParam) paramAnn; paramName = requestParam.value(); required = requestParam.required(); defaultValue = parseDefaultValueAttribute(requestParam.defaultValue()); annotationsFound++; } else if (RequestHeader.class.isInstance(paramAnn)) { RequestHeader requestHeader = (RequestHeader) paramAnn; headerName = requestHeader.value(); required = requestHeader.required(); defaultValue = parseDefaultValueAttribute(requestHeader.defaultValue()); annotationsFound++; } else if (RequestBody.class.isInstance(paramAnn)) { requestBodyFound = true; annotationsFound++; } else if (CookieValue.class.isInstance(paramAnn)) { CookieValue cookieValue = (CookieValue) paramAnn; cookieName = cookieValue.value(); required = cookieValue.required(); defaultValue = parseDefaultValueAttribute(cookieValue.defaultValue()); annotationsFound++; } else if (PathVariable.class.isInstance(paramAnn)) { PathVariable pathVar = (PathVariable) paramAnn; pathVarName = pathVar.value(); annotationsFound++; } else if (ModelAttribute.class.isInstance(paramAnn)) { ModelAttribute attr = (ModelAttribute) paramAnn; attrName = attr.value(); annotationsFound++; } else if (Value.class.isInstance(paramAnn)) { defaultValue = ((Value) paramAnn).value(); } else if (paramAnn.annotationType().getSimpleName().startsWith(\"Valid\")) { validate = true; Object value = AnnotationUtils.getValue(paramAnn); validationHints = (value instanceof Object[] ? (Object[]) value : new Object[] {value}); } } if (annotationsFound &gt; 1) { throw new IllegalStateException(\"Handler parameter annotations are exclusive choices - \" + \"do not specify more than one such annotation on the same parameter: \" + handlerMethod); } //没有找到注解的情况 if (annotationsFound == 0) { //解析普通参数 Object argValue = resolveCommonArgument(methodParam, webRequest);--&gt;解析标准参数 if (argValue != WebArgumentResolver.UNRESOLVED) { args[i] = argValue; } else if (defaultValue != null) { args[i] = resolveDefaultValue(defaultValue); } else { Class&lt;?&gt; paramType = methodParam.getParameterType(); if (Model.class.isAssignableFrom(paramType) || Map.class.isAssignableFrom(paramType)) { if (!paramType.isAssignableFrom(implicitModel.getClass())) { throw new IllegalStateException(\"Argument [\" + paramType.getSimpleName() + \"] is of type \" + \"Model or Map but is not assignable from the actual model. You may need to switch \" + \"newer MVC infrastructure classes to use this argument.\"); } args[i] = implicitModel; } else if (SessionStatus.class.isAssignableFrom(paramType)) { args[i] = this.sessionStatus; } else if (HttpEntity.class.isAssignableFrom(paramType)) { args[i] = resolveHttpEntityRequest(methodParam, webRequest); } else if (Errors.class.isAssignableFrom(paramType)) { throw new IllegalStateException(\"Errors/BindingResult argument declared \" + \"without preceding model attribute. Check your handler method signature!\"); } else if (BeanUtils.isSimpleProperty(paramType)) { paramName = \"\"; } else { attrName = \"\"; } } } if (paramName != null) { args[i] = resolveRequestParam(paramName, required, defaultValue, methodParam, webRequest, handler); } else if (headerName != null) { args[i] = resolveRequestHeader(headerName, required, defaultValue, methodParam, webRequest, handler); } else if (requestBodyFound) { args[i] = resolveRequestBody(methodParam, webRequest, handler); } else if (cookieName != null) { args[i] = resolveCookieValue(cookieName, required, defaultValue, methodParam, webRequest, handler); } else if (pathVarName != null) { args[i] = resolvePathVariable(pathVarName, methodParam, webRequest, handler); } else if (attrName != null) { WebDataBinder binder = resolveModelAttribute(attrName, methodParam, implicitModel, webRequest, handler); boolean assignBindingResult = (args.length &gt; i + 1 &amp;&amp; Errors.class.isAssignableFrom(paramTypes[i + 1])); if (binder.getTarget() != null) { doBind(binder, webRequest, validate, validationHints, !assignBindingResult); } args[i] = binder.getTarget(); if (assignBindingResult) { args[i + 1] = binder.getBindingResult(); i++; } implicitModel.putAll(binder.getBindingResult().getModel()); } } return args; } 如果没有注解： resolveCommonArgument)先看是否普通参数，确定当前的参数是否是原生API 如果是Map或者Map子类，将之前创建的隐含模型赋值过去 方法上标注的ModelAttribute注解如果有value值 @ModelAttribute(value=”as”) attrName=value标注值 标了：attrName=”abc” 没标：attrName=””;attrName就会变为返回值类型首字母小写 @ModelAttribute标在方法的另一个作用；可以把方法运行后的返回值按照@ModelAttribute(“abc”)放到隐含模型中，如果没有key就用返回值类型void {person=Person [name=as, pwd=12, gender=qw, email=asas], void=null} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public final Object invokeHandlerMethod(Method handlerMethod, Object handler, NativeWebRequest webRequest, ExtendedModelMap implicitModel) throws Exception { Method handlerMethodToInvoke = BridgeMethodResolver.findBridgedMethod(handlerMethod); try { boolean debug = logger.isDebugEnabled(); for (String attrName : this.methodResolver.getActualSessionAttributeNames()) { Object attrValue = this.sessionAttributeStore.retrieveAttribute(webRequest, attrName); if (attrValue != null) { implicitModel.addAttribute(attrName, attrValue); } } for (Method attributeMethod : this.methodResolver.getModelAttributeMethods()) { Method attributeMethodToInvoke = BridgeMethodResolver.findBridgedMethod(attributeMethod); Object[] args = resolveHandlerArguments(attributeMethodToInvoke, handler, webRequest, implicitModel); if (debug) { logger.debug(\"Invoking model attribute method: \" + attributeMethodToInvoke); } String attrName = AnnotationUtils.findAnnotation(attributeMethod, ModelAttribute.class).value(); if (!\"\".equals(attrName) &amp;&amp; implicitModel.containsAttribute(attrName)) { continue; } ReflectionUtils.makeAccessible(attributeMethodToInvoke); //@ModelAttribute标注的方法先运行 Object attrValue = attributeMethodToInvoke.invoke(handler, args); if (\"\".equals(attrName)) { Class&lt;?&gt; resolvedType = GenericTypeResolver.resolveReturnType(attributeMethodToInvoke, handler.getClass()); attrName = Conventions.getVariableNameForReturnType(attributeMethodToInvoke, resolvedType, attrValue); } if (!implicitModel.containsAttribute(attrName)) { implicitModel.addAttribute(attrName, attrValue); } } Object[] args = resolveHandlerArguments(handlerMethodToInvoke, handler, webRequest, implicitModel); if (debug) { logger.debug(\"Invoking request handler method: \" + handlerMethodToInvoke); } ReflectionUtils.makeAccessible(handlerMethodToInvoke); //相应目标的方法运行 return handlerMethodToInvoke.invoke(handler, args); } catch (IllegalStateException ex) { // Internal assertion failed (e.g. invalid signature): // throw exception with full handler method context... throw new HandlerMethodInvocationException(handlerMethodToInvoke, ex); } catch (InvocationTargetException ex) { // User-defined @ModelAttribute/@InitBinder/@RequestMapping method threw an exception... ReflectionUtils.rethrowException(ex.getTargetException()); return null; } } 如何确定方法的每一个参数的值 标了注解： &emsp;保存时哪个注解的信息确定值 &emsp;拿到ModelAttribute注解的值让attrName保存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145private Object[] resolveHandlerArguments(Method handlerMethod, Object handler, NativeWebRequest webRequest, ExtendedModelMap implicitModel) throws Exception { Class&lt;?&gt;[] paramTypes = handlerMethod.getParameterTypes(); Object[] args = new Object[paramTypes.length]; for (int i = 0; i &lt; args.length; i++) { MethodParameter methodParam = new MethodParameter(handlerMethod, i); methodParam.initParameterNameDiscovery(this.parameterNameDiscoverer); GenericTypeResolver.resolveParameterType(methodParam, handler.getClass()); String paramName = null; String headerName = null; boolean requestBodyFound = false; String cookieName = null; String pathVarName = null; String attrName = null; boolean required = false; String defaultValue = null; boolean validate = false; Object[] validationHints = null; int annotationsFound = 0; Annotation[] paramAnns = methodParam.getParameterAnnotations(); for (Annotation paramAnn : paramAnns) { if (RequestParam.class.isInstance(paramAnn)) { RequestParam requestParam = (RequestParam) paramAnn; paramName = requestParam.value(); required = requestParam.required(); defaultValue = parseDefaultValueAttribute(requestParam.defaultValue()); annotationsFound++; } else if (RequestHeader.class.isInstance(paramAnn)) { RequestHeader requestHeader = (RequestHeader) paramAnn; headerName = requestHeader.value(); required = requestHeader.required(); defaultValue = parseDefaultValueAttribute(requestHeader.defaultValue()); annotationsFound++; } else if (RequestBody.class.isInstance(paramAnn)) { requestBodyFound = true; annotationsFound++; } else if (CookieValue.class.isInstance(paramAnn)) { CookieValue cookieValue = (CookieValue) paramAnn; cookieName = cookieValue.value(); required = cookieValue.required(); defaultValue = parseDefaultValueAttribute(cookieValue.defaultValue()); annotationsFound++; } else if (PathVariable.class.isInstance(paramAnn)) { PathVariable pathVar = (PathVariable) paramAnn; pathVarName = pathVar.value(); annotationsFound++; } else if (ModelAttribute.class.isInstance(paramAnn)) { ModelAttribute attr = (ModelAttribute) paramAnn; attrName = attr.value(); annotationsFound++; } else if (Value.class.isInstance(paramAnn)) { defaultValue = ((Value) paramAnn).value(); } else if (paramAnn.annotationType().getSimpleName().startsWith(\"Valid\")) { validate = true; Object value = AnnotationUtils.getValue(paramAnn); validationHints = (value instanceof Object[] ? (Object[]) value : new Object[] {value}); } } if (annotationsFound &gt; 1) { throw new IllegalStateException(\"Handler parameter annotations are exclusive choices - \" + \"do not specify more than one such annotation on the same parameter: \" + handlerMethod); } if (annotationsFound == 0) { Object argValue = resolveCommonArgument(methodParam, webRequest); if (argValue != WebArgumentResolver.UNRESOLVED) { args[i] = argValue; } else if (defaultValue != null) { args[i] = resolveDefaultValue(defaultValue); } else { Class&lt;?&gt; paramType = methodParam.getParameterType(); if (Model.class.isAssignableFrom(paramType) || Map.class.isAssignableFrom(paramType)) { if (!paramType.isAssignableFrom(implicitModel.getClass())) { throw new IllegalStateException(\"Argument [\" + paramType.getSimpleName() + \"] is of type \" + \"Model or Map but is not assignable from the actual model. You may need to switch \" + \"newer MVC infrastructure classes to use this argument.\"); } args[i] = implicitModel; } else if (SessionStatus.class.isAssignableFrom(paramType)) { args[i] = this.sessionStatus; } else if (HttpEntity.class.isAssignableFrom(paramType)) { args[i] = resolveHttpEntityRequest(methodParam, webRequest); } else if (Errors.class.isAssignableFrom(paramType)) { throw new IllegalStateException(\"Errors/BindingResult argument declared \" + \"without preceding model attribute. Check your handler method signature!\"); } else if (BeanUtils.isSimpleProperty(paramType)) { paramName = \"\"; } else { attrName = \"\"; } } } if (paramName != null) { args[i] = resolveRequestParam(paramName, required, defaultValue, methodParam, webRequest, handler); } else if (headerName != null) { args[i] = resolveRequestHeader(headerName, required, defaultValue, methodParam, webRequest, handler); } else if (requestBodyFound) { args[i] = resolveRequestBody(methodParam, webRequest, handler); } else if (cookieName != null) { args[i] = resolveCookieValue(cookieName, required, defaultValue, methodParam, webRequest, handler); } else if (pathVarName != null) { args[i] = resolvePathVariable(pathVarName, methodParam, webRequest, handler); } //确定自定义类型参数的值，还要讲七扭去中的每一个参数赋值给这个对象 else if (attrName != null) { WebDataBinder binder = resolveModelAttribute(attrName, methodParam, implicitModel, webRequest, handler); boolean assignBindingResult = (args.length &gt; i + 1 &amp;&amp; Errors.class.isAssignableFrom(paramTypes[i + 1])); if (binder.getTarget() != null) { doBind(binder, webRequest, validate, validationHints, !assignBindingResult); } args[i] = binder.getTarget(); if (assignBindingResult) { args[i + 1] = binder.getBindingResult(); i++; } implicitModel.putAll(binder.getBindingResult().getModel()); } } return args; } 没有标注解 先看是否为普通参数（是否为原生API） 再看是否Model或者Map，如果是传入隐含模型 自定义类型的参数没有注解没有ModelAttribute注解 先看原生API 在看是否是Map或ModelMap 在看看是不是其他类型，比如SessionStatus、HttpEntity、Errors 是否为简单属性，是否Integer、String基本类型 如果paramName=“” AttrName=”” 如果是自定义类型对象，最终会产生两个效果 如果这个参数标注了ModelAttribute注解就给attrName赋值，为这个注解Value值 如果没有，就为“” 确定自定义类型的值（POJO) 1234567891011private void doBind(WebDataBinder binder, NativeWebRequest webRequest, boolean validate, Object[] validationHints, boolean failOnErrors) throws Exception { doBind(binder, webRequest); if (validate) { binder.validate(validationHints); } if (failOnErrors &amp;&amp; binder.getBindingResult().hasErrors()) { throw new BindException(binder.getBindingResult()); } } 123456789101112131415161718192021222324252627private WebDataBinder resolveModelAttribute(String attrName, MethodParameter methodParam, ExtendedModelMap implicitModel, NativeWebRequest webRequest, Object handler) throws Exception { // Bind request parameter onto object... String name = attrName; if (\"\".equals(name)) { name = Conventions.getVariableNameForParameter(methodParam); } Class&lt;?&gt; paramType = methodParam.getParameterType(); //如果隐含模型有这个key（标了ModelAttribute注解就是注解标定的值，没标就是参数类型首字母小写）指定的值 Object bindObject; if (implicitModel.containsKey(name)) { bindObject = implicitModel.get(name); } else if (this.methodResolver.isSessionAttribute(name, paramType)) { bindObject = this.sessionAttributeStore.retrieveAttribute(webRequest, name); if (bindObject == null) { raiseSessionRequiredException(\"Session attribute '\" + name + \"' required - not found in session\"); } } else { bindObject = BeanUtils.instantiateClass(paramType); } WebDataBinder binder = createBinder(webRequest, bindObject, name); initBinder(handler, name, binder, webRequest); return binder; } 如果隐含模型有这个key（标了ModelAttribute注解就是注解标定的值，没标就是参数类型首字母小写）指定的值 如果有将这个值赋值给bindObject 如果是SessionAttributes标注的属性，就从Session中拿 如果都不是，就用反射创建一个 确定方法每个参数的值 标注解：保存注解信息；最终得到这个注解应该对应解析的值 没标注解 先看是否是否为原生API 是否为Model或Map 看是否为简单类型 给attrName赋值，参数标了ModelAttribute就是指定的，没标就位空串 确定自定义类型参数 attrName使用参数类型首字母小写；或者使用之前@ModelAttribute的值 先看隐含模型中是否有这个attrName对应的值；如果有就从隐含模型中获取并赋值 看是否SessionAttrinute标注的属性，如果是从Session中拿 拿不到抛异常 不是SessionAttributes标注的值，利用反射创建一个 拿到之前创建好的对象，使用数据绑定器（WebDataBinder）将请求的么个数据绑定到这个对象中 SpringMVC九大组件 在DispatchServlet中有几个引用类型属性。关键位置都是由这些组件完成的 共同点： 九大组件都是接口：接口就是规范，提供了强大的扩展性 1234567891011121314151617181920212223242526272829303132333435/** MultipartResolver used by this servlet */ //文件上传解析器 private MultipartResolver multipartResolver; /** LocaleResolver used by this servlet */ //区域信息析器与国际化有关 private LocaleResolver localeResolver; /** ThemeResolver used by this servlet */ //主题解析器：主题效果更换 private ThemeResolver themeResolver; /** List of HandlerMappings used by this servlet */ //Handler映射信息：HandlerMapping private List&lt;HandlerMapping&gt; handlerMappings; /** List of HandlerAdapters used by this servlet */ //Handler适配器 private List&lt;HandlerAdapter&gt; handlerAdapters; /** List of HandlerExceptionResolvers used by this servlet */ //SpringMVC强大的异常解析功能 private List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers; /** RequestToViewNameTranslator used by this servlet */ // private RequestToViewNameTranslator viewNameTranslator; /** FlashMapManager used by this servlet */ //SpringMVC中运行重定向携带数据的功能 private FlashMapManager flashMapManager; /** List of ViewResolvers used by this servlet */ //视图解析器 private List&lt;ViewResolver&gt; viewResolvers; DispatchServlet中onRefresh方法 123456789101112131415@Overrideprotected void onRefresh(ApplicationContext context) { initStrategies(context);}protected void initStrategies(ApplicationContext context) { initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context);}","link":"/post/eaa9b691.html"},{"title":"Java动态代理","text":"概述 Hadoop远程过程调用实现使用Java动态代理和新输入1输出系统(NewInput/Output,NIO) Java动态代理类位于java.lang.reflect包下，主要包括java.lang rlfct.Proxy和java.lang.reflect.InvocationHandler 代理对象两大任务 创建代理接口 实现由java.lang,reflect.Proxy完成 调用转发通过java.lang.reflect.InvocationHandler的实例完成 代理接口的创建 在Java中，代理对象往往实现和目标对象-致的接口，并作为目标对象的代替，接收对象用户(Client) 的调用，并将全部或部分调用转发给目标对象 代理也是拥有和目标对象一样的权利的‘人’， 简单的说， 代理可以越俎代庖。实际上他是调用转发，将这个任务交给有权利实施的人 代理时序图 java.lang.eflct.Proxy提供了用于创建动态代理类和对象的静态方法。也就是说，通过java.lang.reflect.Proxy可以动态地创建某个接口实现 java.lang.eflct.Proxy比较重要的方法 public static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces) 获得代理类的java.lang.Class对象。该代理类将定义在指定的类加载器（参数loader）中，并将实现参数interfaces指定所有接口 注意： 这个类只创建一次，如果再次传入相同的loader和interfaces给newProxyInstance()方法，获得的也只是第一次调用创建的那个java.lang.Class对象 通过Proxy.getProxyClass()获得的代理类都包含一个构造函数,该构造函数需要一个java.lang.reflect.InvocationHandler 的实例 如何获取Proxy对象？，请看以下代码 123456Class clss = Proxy.getProxyClass(loader, interfaces);//获得构造函数Constructor constructor = clss.getConstructor(new Class[]{InvocationHandler.class});//创建代理对象Object proxy = constructor.newInstance(new Object[]{invocationHandler}); public static boolean isProxyClass(Class&lt;?&gt; cl) 判断java.lang.Class对象是否是代理类 public static InvocationHandler getInvocationHandler(Object proxy) throws IllegalArgumentException 获取代理实例对应的调用处理程序（即构建代理传入的InvocationHandler实例） 调用转发 InvocationHandler调用实例也叫调用句柄实例 12345678910111213141516/** * * @param proxy 代理对象本身 * @param method 用户调用的代理对象上的方法 * @param args 传递给该方法的参数 * @return 代理对象方法调用结果 * @throws Throwable * * java.lang.reflect.Method * 它提供了关于类或接口上某个方法以及如何访问该方法的信息 * 其中的invoke方法可以在指定对象上调用对象的方法 */@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return null;} Method类中的invoke方法声明如下 123public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException 假设目标对象为target，实现转发代码如下 123public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return method.invoke(target, args);s} Java动态代理实例实例类图 代码 12345678910111213/**DPStatus.java**/public class DPStatus { String name; public DPStatus(String name) { this.name = name; } @Override public String toString() { return \"Hello, \" + name + \"!\"; }} 12345678/** * PDQueryStatus.java * * 动态代理机制与java远程调用不同，不需要继承什么接口 */public interface PDQueryStatus { public DPStatus getStatus();} 1234567891011/** * DPQueryStatusImpl.java * * PDQueryStatus的简单实现 */public class DPQueryStatusImpl implements PDQueryStatus{ @Override public DPStatus getStatus() { return new DPStatus(\"Bitty\"); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.text.MessageFormat;import java.util.Arrays;/** * DPInvocationHandler.java * * 转发由该类实现 * 该类中最重要的一部分是invoke方法， * * 如果代理对象调用某个方法时，DPInvocationHandler.invoke将会被调用， * 传入invoke的method中，也就是说，PDQueryStatus.getStatus()就是是method对象， * 而getStatus()没有参数 * * 在这里完成代理转发的是 * Object res = method.invoke(dpqs, args); * 当method是PDQueryStatus.getStatus()时，其效果就相当于 * res = pdqs.getStatus() * * * */public class DPInvocationHandler implements InvocationHandler { //目标对象 private DPQueryStatusImpl dpqs; public DPInvocationHandler(DPQueryStatusImpl dpqs) { this.dpqs = dpqs; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //实现附加功能，在控制台输出调用参数的String表示 String msg = MessageFormat.format(\"Calling method\", method.getName(), Arrays.toString(args)); System.out.println(msg); //调用转发 Object res = method.invoke(dpqs, args); //其他附加功能 return res; }} 1234567891011121314151617/** * DPMain.java * * 在这个类中使用create方法创建代理 */public class DPMain { public static PDQueryStatus create(DPQueryStatusImpl dpqs){ //newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) //参数准备 Class&lt;?&gt;[] interfaces = new Class[]{PDQueryStatus.class}; DPInvocationHandler handler = new DPInvocationHandler(dpqs); return (PDQueryStatus) Proxy.newProxyInstance(dpqs.getClass().getClassLoader(), interfaces, handler); }} 123456789public class Demo { public static void main(String[] args) throws Exception { PDQueryStatus pdqs = DPMain.create(new DPQueryStatusImpl()); System.out.println(pdqs.getStatus()); }}","link":"/post/e63a7813.html"},{"title":"Linux的一些笔记","text":"coreutils nohub命令使用需要安装 mlocate locate命令使用需要安装 vim 下如何批量注释","link":"/post/fe24bc9e.html"},{"title":"SpringMVC之ModelAttribute","text":"@ModelAttribute实验场景 @ModelAttribute注解之示例代码 在方法定义上使用 @ModelAttribute 注解：Spring MVC 在调用目标处理方法前，会先逐个调用在方法级上标注了 @ModelAttribute 的方法。 在方法的入参前使用 @ModelAttribute 注解：可以从隐含对象中获取隐含的模型数据中获取对象，再将请求参数绑定到对象中，再传入入参 将方法入参对象添加到模型中 页面表单代码 123456789 &lt;form action=\" testPOJO\" method=\"POST\"&gt; &lt;input type=\"hidden\" name=\"id\" value=\"1\"&gt;&lt;br&gt; username: &lt;input type=\"text\" name=\"name\"/&gt;&lt;br&gt; password: &lt;input type=\"password\" name=\"pwd\"/&gt;&lt;br&gt; gender: &lt;input type=\"text\" name=\"gender\"/&gt;&lt;br&gt; &lt;input type=\"submit\" value=\"Submit\"/&gt;&lt;/form&gt; 增加ModelAttribute注解 123456789101112@RequestMapping(\"testPOJO\")public String testPOJO(@ModelAttribute(\"person\") Person person) { System.out.println(person); return \"success\";}@ModelAttributepublic void getPerson(@RequestParam(value=\"id\",required=false)String id,Map&lt;String, Object&gt; map) { System.out.println(\"ModelAttribute方法运行...\"); Person person = new Person(\"as\", \"12\", \"qw\", \"asas\"); System.out.println(person); map.put(\"person\", person);} ModelAttribute的运行流程 @ModelAttribute源码参考1234567891011121314@Target({ElementType.PARAMETER, ElementType.METHOD})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface ModelAttribute {/** * The name of the model attribute to bind to. * &lt;p&gt;The default model attribute name is inferred from the declared * attribute type (i.e. the method parameter type or method return type), * based on the non-qualified class name: * e.g. \"orderAddress\" for class \"mypackage.OrderAddress\", * or \"orderAddressList\" for \"List&amp;lt;mypackage.OrderAddress&amp;gt;\". */String value() default \"\"; } @ModelAttribute注解之源码分析 源代码分析的流程 ① 调用 @ModelAttribute 注解修饰的方法. 实际上把 @ModelAttribute 方法中 Map 中的数据放在了 implicitModel 中. ② 解析请求处理器的目标参数, 实际上该目标参数来自于 WebDataBinder 对象的 target 属性 创建 WebDataBinder 对象: 确定 objectName 属性: 若传入的 attrName 属性值为 “”, 则objectName 为类名第一个字母小写. 注意: attrName. 若目标方法的 POJO 属性使用了 @ModelAttribute 来修饰, 则 attrName 值即为 @ModelAttribute 的 value 属性值 确定 target 属性: 在 implicitModel 中查找 attrName 对应的属性值. 若存在, ok 若不存在: 则验证当前 Handler 是否使用了 @SessionAttributes 进行修饰, 若使用了, 则尝试从 Session 中获取 attrName 所对应的属性值. 若 session 中没有对应的属性值, 则抛出了异常. 若 Handler 没有使用 @SessionAttributes 进行修饰, 或 @SessionAttributes 中没有使用 value 值指定的 key和 attrName 相匹配, 则通过反射创建了 POJO 对象 . SpringMVC 把表单的请求参数赋给了 WebDataBinder 的 target 对应的属性. SpringMVC 会把 WebDataBinder 的 attrName 和 target 给到 implicitModel. 进而传到 request 域对象中. 把 WebDataBinder 的 target 作为参数传递给目标方法的入参. SpringMVC 确定目标方法POJO类型入参的过程① 确定一个 key: ​ 1). 若目标方法的 POJO 类型的参数木有使用 @ModelAttribute 作为修饰, 则 key 为 POJO 类名第一个字母的小写 ​ 2). 若使用了@ModelAttribute 来修饰, 则 key 为 @ModelAttribute 注解的 value 属性值. ② 在 implicitModel 中查找 key 对应的对象, 若存在, 则作为入参传入 若在 @ModelAttribute 标记的方法中在 Map 中保存过, 且 key 和 ① 确定的 key 一致, 则会获取到. ③ 若 implicitModel 中不存在 key 对应的对象, 则检查当前的 Handler 是否使用 @SessionAttributes 注解修饰, ④ 若使用了该注解, 且 @SessionAttributes 注解的 value 属性值中包含了 key, 则会从 HttpSession 中来获取 key 所对应的 value 值, 若存在则直接传入到目标方法的入参中. 若不存在则将抛出异常. ⑤ 若 Handler 没有标识 @SessionAttributes 注解或 @SessionAttributes 注解的 value 值中不包含 key, 则会通过反射来创建 POJO 类型的参数, 传入为目标方法的参数 ⑥ SpringMVC 会把 key 和 POJO 类型的对象保存到 implicitModel 中, 进而会保存到 request 中. @sessionAttributes注解引发的异常 由@SessionAttributes引发的异常 ① 果在处理类定义处标注了@SessionAttributes(“xxx”)，则尝试从会话中获取该属性，并将其赋给该入参，然后再用请求消息填充该入参对象。如果在会话中找不到对应的属性，则抛出 HttpSessionRequiredException 异常","link":"/post/548d7f45.html"},{"title":"Xshell连接Ubuntu","text":"连接操作https://blog.csdn.net/lianghe_work/article/details/47340141 遇到的问题 sudo apt-get install openssh-server报错123456789101112131415ll@ubuntu:~$ sudo apt-get install openssh-serverReading package lists... DoneBuilding dependency tree Reading state information... DoneSome packages could not be installed. This may mean that you haverequested an impossible situation or if you are using the unstabledistribution that some required packages have not yet been createdor been moved out of Incoming.The following information may help to resolve the situation:The following packages have unmet dependencies: openssh-server : Depends: openssh-client (= 1:7.2p2-4ubuntu2.8) Depends: openssh-sftp-server but it is not going to be installed Recommends: ssh-import-id but it is not going to be installedE: Unable to correct problems, you have held broken packages. 解决办法 sudo apt-get install openssh-client=1:7.2p2-4ubuntu2.8执行以上命令后在执行安装就可以了， 不过要和以上报错的版本对应 root用户无法登陆 在其他用户下切换sudo su root切换到root用户 vi /etc/ssh/sshd_config 123&lt;!-- 加一句去一句 --&gt;PermitRootLogin yes# PermitRootLogin prohibit-password /etc/init.d/ssh restart 来重启ssh服务 /usr/bin/xauth: file /root/.Xauthority does not exist1234vi /etc/ssh/sshd_config添加 AllowAgentForwarding yes/etc/init.d/ssh restart 来重启ssh服务 Ubuntu 输入正确的密码后重新返回到登陆界面","link":"/post/393fa9a5.html"},{"title":"环境配置","text":"maven环境配置位置在于下载好的maven目录下apache-maven-3.5.2\\conf 更改本地库位置（防止C盘撑爆）搜索即可找到1&lt;localRepository&gt;D:\\Repository&lt;/localRepository&gt; 换阿里云镜像（加速下载）在mirrors下123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 配置JDK版本，profiles下123456789101112&lt;profile&gt; &lt;id&gt;jdk18&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt; &lt;/profile&gt; 配置环境变量文件夹-&gt;此电脑右键-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;新建系统环境变量 IDEA中的Maven配置","link":"/post/b1c45f58.html"},{"title":"Vim多行注释","text":"方法一 首先按 esc 进入命令行模式下，按下 Ctrl + v ，进入列（也叫区块）模式; 在行首使用上下键选择需要注释的多行; 按下键盘（大写） “I” 键，进入插入模式； 然后输入注释符（ “//”、“#” 等）; 最后按下 “Esc” 键。 方法21在指令模式下，使用.,$s/^/^# (从指定行到最后一行)","link":"/post/e9a730c6.html"},{"title":"Java远程调用","text":"概述 Java远程方法调用(Remote Method Invocation, RMI)是Java的一个核心API和类库,允许一个Java虚拟机上运行的Java程序调用不同虚拟机上运行的对象中的方法，即使这两个虚拟机运行于物理隔离的不同主机上。在某种程度上，RMI可以看成RPC的Java升级版。 和RPC一样,存在服务端和客户端典型服务器端应用程序 创建多个远程对象（Remote Object）,使这些对象能被客户端引用，并等待客户端调用远程对象的方法典型的客户端程序 从服务器获得一个或多个远程对象的引用，然后调用远程对象的方法 Java远程方法调用依赖于Java序列化，调用远程方法传的参数、返回值都是序列化对象 远程方法调用实例123456789/**RMIQueryStatus.java**/import java.rmi.Remote;import java.rmi.RemoteException;public interface RMIQueryStatus extends Remote { String getStatus(String name) throws RemoteException;} RMIQueryStatus的定义要求 远程接口必须声明为public，否则客户端试着装载“实现远程接口”的远端对象时，会收到错误的消息。 远程接口必须继承自java.rmi.Remote。 远程接口中的每-一个方法，除了自定义的异常之外，必须将java.rmi.RemoteException声明于其throws子句中。 在远程方法声明中，作为参数或者返回值的远程对象，或者包含在其他非远程对象中的远程对象，必须声明为其对应的远程接口，而不是实际的实现类。(这点在String类中并没有体现) RMIQueryStatus的实现类 123456789101112/**RMIQueryStatusImp.java**/import java.rmi.RemoteException;import java.rmi.server.UnicastRemoteObject;public class RMIQueryStatusImp extends UnicastRemoteObject implements RMIQueryStatus { protected RMIQueryStatusImp() throws RemoteException { } public String getStatus(String name) throws RemoteException { return \"I'm \" + name + \".\"; }} 客户端和服务端的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/**RMIDemoServer**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.RemoteException;import java.rmi.registry.LocateRegistry;/** * @author lyhcc */public class RMIDemoServer { public static void main(String[] args) throws RemoteException, MalformedURLException { //1. 创建RMIQueryStatus对象 RMIQueryStatusImp queryService = new RMIQueryStatusImp(); //2. 设置服务端口 LocateRegistry.createRegistry(12090); //3. 绑定远端对象名 Naming.rebind(\"rmi://localhost:12090/queryTest\", queryService); System.out.println(\"Server is running!\"); }}/**客户端的代码**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.NotBoundException;import java.rmi.RemoteException;public class RMIDemoClient { public static void main(String[] args) throws RemoteException, NotBoundException, MalformedURLException { //1. 创建RMIQueryStatusImp对象 RMIQueryStatus queryStatus = (RMIQueryStatus) Naming.lookup(\"rmi://localhost:12090/queryTest\"); //2. 调用远程方法 String status = queryStatus.getStatus(\"KiKi\"); System.out.println(status); }} 先运行服务端然后运行客户端查看结果 Java远程调用实例类图 客户端RMIQueryStatusClient的工作依赖于RMI存根(Stub)，这个存根是通过Java的代理机制 java.lang.reflect.Proxy","link":"/post/55cde24e.html"},{"title":"BigData/Kafka/Kafka之broker","text":"Kafka启动过程 首先执行 ./bin/kafka-server-start.sh ./config/server.properties &amp;里面的最后一句 exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka “$@”执行kafka.Kafka类，从里面可以发现，它使用了KafkaServerStartable，里面封装有KafkaServer，最终startup执行的是KafkaServer，KafkaServer里面的参数说明 12","link":"/post/b56fd6c4.html"},{"title":"Kafka通信详情","text":"ProducerRequest:生产者发送消息的请求，生产者将消息发送至Kafka集群中的某个Broker, Broker 接收到此请求后持久化此消息并更新相关元数据信息。 （Producer发送消息） TopicMetadataRequest :获取Topic元数据信息的请求，无论是生产者还是消费者都需要通过此请求来获取感兴趣的Topic的元数据。 （获取感兴趣Topic元数据） FetchRequest:消费者获取感兴趣Topic的某个分区的消息的请求，除此之外，分区状态为Follower的副本也需要利用此请求去同步分区状态为Leader的对应副本数据。 （获取感兴趣Topic分区消息，同步） OffsetRequest:消费者发送至Kafka集群来获取感兴趣Topic的分区偏移量的请求，通过此请求可以获知当前Topic所有分区在不同时间段的偏移量详情。 （获取消费者感兴趣的分区偏移量） OffsetCommitRequest:消费者提交Topic被消费的分区偏移量信息至Broker, Broker接收到此请求后持久化相关偏移量信息。 （提交消费了的分区偏移量） OffsetFetchRequest:消费者发送 获取 提交至Kafka集群的相关Topic被消费的 详细信息，和OffsetCommitRequest相互对应。 （获取被消费的消息） LeaderAndlsrRequest:当Topic的某个分区状态发生变化时，处于Leader状态的KafkaController发送此请求至相关的Broker,通知其做出相应的处理。 （检测Topic分区变化状态） StopReplicaRequest:当Topic的某个分区被删除或者下线的时候，处于Leader状态的KafkaController发送此请求至相关的Broker,通知其做出相应的处理。 （检测分区是否在线） UpdateMetadataRequest:当Topic 的元数据信息发生变化时，处于Leader状态的KafkaContoller发送此请求至相关的Broker,通知其做出相应的处理。 （Topic元数据变化） BrokerContolledShutdownRequest:当Broker正常下线时，发生此请求至处于Leader状态的KafnRaCotoller。 （Broker下线） ConsumerMetadataRequest: 获取保存特定Consumer Group消费详情的分区信息。 （获取CG信息） 交互过程 Producer和Kafka集群: Producer需要利用ProducerRequest和TopicMetadataRequest米完成 Topic元数据的查询、消息的发送。 Consumer和Kafka集群: Consumer需要利用TopicMetadataRequest请求，FetchRequest请求。OffsetRequest 请求、OffsetCommitRequest 请求、fsetFechnRequest 请求和ConsumerMetadataRequest请求来完成Topic元数据的查询、消息的订阅、历史偏移量的查询、偏移量的提交、当前偏移量的查询。 Kafkaontroller状态为Leader的Broker和KafkaController状态为Standby的Broker:KafkaContoller 状态为Leader的Broker需要利用LeaderAndIsrRequest请求、Stop-ReplicaRequest请求，UpdateMetadataRequest 请求来完成对Topic的管理; Kafka-Controller状态为Standby 的Broker需要利用BrokerControlledShutdownRequest请求来通知KafkaContoller状态为Leader的Broker自己的下线动作。 Broker和Broker之间: Broker相互之间需要利用FetchRequest请求来同步Topic分区的副本数据，这样才能使Topic分区各副本数据实时保持一致。","link":"/post/bc0600c8.html"},{"title":"Kafka生产与消费","text":"命令行测试启动命令1./bin/kafka-server-start.sh ./config/server.properties &amp; 创建topic和删除topic –zookeeper ZooKeeper服务器地址 –partitions 分区个数 –topic topic-demo 指定topic名自为topic-demo –replication-factor 指定副本数 –delete 删除指令 –create 创建指令1234567[root@slave1 kafka_2.11-2.2.1]# ./bin/kafka-topics.sh --zookeeper master:2181 --create --topic topic-demo --replication-factor 3 --partitions 4Created topic topic-demo.[root@slave1 kafka_2.11-2.2.1]# ./bin/kafka-topics.sh --zookeeper master:2181 --topic topic-demo --deleteTopic topic-demo is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true. 查看topic的详细信息123456[root@slave1 kafka_2.11-2.2.1]# ./bin/kafka-topics.sh --zookeeper master:2181 --topic topic-demo --describe Topic:topic-demo PartitionCount:4 ReplicationFactor:3 Configs: Topic: topic-demo Partition: 0 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Topic: topic-demo Partition: 1 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: topic-demo Partition: 2 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3 Topic: topic-demo Partition: 3 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 生产者消费者1234567[root@slave1 kafka_2.11-2.2.1]# ./bin/kafka-console-producer.sh --topic topic-demo --broker-list master:9092&gt;Hello, I am lyhcc. &gt;[root@slave1 kafka_2.11-2.2.1]# ./bin/kafka-console-consumer.sh --bootstrap-server master:9092 --topic topic-demo Hello, I am lyhcc. 说明：命令行方式只是用来测试 Java实现生产者和消费者客户端 要往kafka中写入消息， 首先要创建一个生产者客户端并设置一些参数，然后创建消息的ProducerRecord对象，其中必须包含所要发往的主题以及消息的消息体，然后生产者客户端将消息发送出去，最后通过close方法关闭生产者客户端并回收相应的资源消费者消费消息 首先创建一个消费者客户端实例并配置相应的参数，然后订阅主题并消费即可 生产者 123456789101112131415161718192021222324public class KafkaProducerDemo { public static final String brokerList = \"master:9092\"; public static final String topic = \"topic-demo\"; public static void main(String[] args) { Properties properties = new Properties(); properties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"bootstrap.servers\",brokerList); //1. 配置生产者客户端参数并创建kafka实例 KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); //2. 构建所需要发送的信息 ProducerRecord&lt;String,String&gt; record = new ProducerRecord&lt;String, String&gt;(topic, \"hello world\"); //3. 发送消息 try { producer.send(record); } catch (Exception e) { e.printStackTrace(); } //4. 关闭生产者客户端 producer.close(); }} 消费者 1234567891011121314151617181920212223242526public class KafkaComsumerDemo { public static final String brokerList = \"master:9092\"; public static final String topic = \"topic-demo\"; public static final String groupId = \"group.demo\"; public static void main(String[] args) { Properties properties = new Properties(); properties.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); properties.put(\"group.id\",groupId); properties.put(\"bootstrap.servers\", brokerList); //1. 创建一个消费者客户端实例 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(properties); //2. 订阅主题 consumer.subscribe(Collections.singletonList(topic)); //3. 遍历消息 while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.println(record.value()); } } }} 在编写之前需要引入Kafka相关包 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt;&lt;/dependency&gt;","link":"/post/18c3cd0d.html"},{"title":"Hadoop介绍","text":"起源 Google 在大数据方面的三大论文 （谷歌三宝）在github大的当前目录下三宝的介绍 Hadoop 三大发行版本 Apache、Cloudera、Hortonworks Apache版本最原始、最基础：适合零基础 大公司在用 Cloudera Cloudera’s DistributionIncluding Apache Hadoop 简称CDH中小型公司用、简单方便、自带可视化 Hortonworks 文档较好 注：Cloudera 和Hortonworks 在2018年10月，国庆期间宣布合并硬件要求内存 最大支持内存查询：win + R输入 wmic memphysical get maxcapacity计算 MaxCapacity/1024/1024GB 硬盘:500G+","link":"/post/8a9ad9ba.html"},{"title":"Kafka初识","text":"kafka的设计主要目标 以复杂度O(1)的方式提供消息持久化能力，即使对TB级以上的数据也能保证常数的访问性能 高吞吐率，即使在非常廉价的商用机器也能做到单机支持秒100k条消息的传输 支持Kafka Server间消息分区，及分布式消费，同时保证分区内的消息顺序传输 支持离线的数据处理和实时数据处理 支持在线水平扩展 为什么使用消息系统 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可 灵活性和峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证性 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性） 缓冲 消息队列通过一个缓冲层来帮助任务最高效率地执行，写入队列的处理尽可能开始。有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 异步通信 很多时候，用户不想也不需要立即处理信息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但不立即处理，想要队列中放入多少数据就放多少，然后在需要的时候再去处理 Kafka架构 Kafka集群中将消息以Topic命名的消息队列中，消费者订阅发往某个Topic命名的消息队列queue中的消息其中Kafka集群由若干个Broker组成，Topic中含有多个Partition，每个Partition中通过Offset来获取 Producer：消息生产者，即将消息发布到指定的Topic中，同时Producer也能绝对消息所属的Partition Consumer：消息消费者，即向指定的Topic获取消息，根据指定的Topic的分区索引及其对应分区上的的消息偏移量来获取消息 Consumer Group: 消费者组，每一个Consumer属于一个Consumer Group,每一个Consumer Group包含一个或多个Consumer。如果所有的Consumer都具有相同的Consumer Group，那么消息将会在Consumer之间进行负载均衡，这也是传统的消息系统 “队列”模型。也就是说一个Partition中的消息只会被具有相同groupID的Consumer消费，并且每个Consumer Group之间是相互独立的。如果要实现“发布-订阅”模型，则每个消费 者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。 Broker:一台Kafka服务器即使一个Broker，一个集群由多个Broker组成，一个Broker可以容纳多个Topic，Broker之间的关系基本是平等的，并不像Hadoop集群那样存在主从模式和为防止单点故障Standby节点 Topic:每条发送到Kafka集群的消息都属于某个Topic。物理上Topic是分开存储的，逻辑上，用户读写数据时并不需要关心他们是存储到哪里的 Partition：Kafka集群为了实现可扩展性，一个非常大的Topic可以分成多个Partition，从而分布到多台Broker中。Partition中的每条消息都会分配有一个自增ID(Offset)。Kafka保证一个Partition内的消息的有序，不保证一个Topic的Partition之间有序 Offset：消息Topic的partition中的位置，同一个Partition中，随着消息的写入，对应Offset自增 Replica：副本。Topic的Partition含有N个副本。其中一个是Replica的Leader，其他的都是Follower，Leader处理partition的读写请求，与此同时，Follower会定期的去同步Leader上的数据 一主多从 Message：消息，是通信的结伴单位，每个Producer可以向一个Topic发布一些Message Zookeeper：存放Kafka集群的元数据组件。在Zookeeper集群中会保存Topic的状态信息，如分区个数，分区组成，分区分布情况等、保存Broker的状态信息、保存消费者的消费信息等。通过这些信息，Kafka很好地将消息生产、消息存储、消息消费的过程结合起来 Kafka网络拓扑说明 Producer根据指定的路由方法（Round-Robin、Hash等），将消息Push到Topic中的某个Partition中 Kafka集群收到Producer发来的信息后，将其持久化到硬盘，并保留消息的指定时长（可配置），而不关注消息是否被消费 Consumer从Kafka集群中Pull数据，并控制获取消息的Offset Kafka的通信过程详解 先说一下KafkaController,它是Broker内部负责管理分区和副本状态以及异常情况下分区重新分配等功能的模块，每个Kafka集群只有一个KafkaController为Leader其他的为Standby，当一个Leader挂掉后，Zookeeper或选举一个新的KafkaController为Leader 过程详情 一些概念 Assigned Replicas(AR): 分区中的所有副本数。In-Sync Replicas(ISR): 与Leader副本保持一定程度同步的副本。ISR是AR的子集。”一定程度的同步“是指可忍受的之后范围，这个范围可以通过参数进行配置。与Leader副本滞后过多的副本组成OSR(Out-of_Sync Replicas)。AR=ISR+OSR。 只有只有在ISR中的副本才有资格被选为Leader ISR与HW和LEO。HW(High Watermark)，俗称高水位，它标识了一个特定的消息偏移量Offset，消费者只能读取这个offset之前的消息。LEO(Log End Offset)它标识当前日志文件下一条待写入消息的offset。LEO的大小相当于当前日志中的最后一条消息的offset加1 Kafka订单复制机制既不是完全的同步通信也不是单纯的异步通信","link":"/post/df89d65f.html"},{"title":"Java NIO实例","text":"概述 回显服务器是指接收到客户端的数据，原封不动的返回给客户端 小提示 &nbsp;在阅读这里之前，希望先看一个视频和相关API的介绍，API的介绍不是很详细，当然也可以遇到不知道的API在百度/Google搜索视频：https://www.bilibili.com/video/av57390893?t=5655相关API: https://lyhcc.github.io/post/f2d80d11.html#more 源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/**NIOServer.java**/import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;public class NIOServer { public static void main(String[] args) throws IOException { //1. 打开一个选择器 Selector selector = Selector.open(); //2. 打开一个ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //3. 配置异步模式 serverSocketChannel.configureBlocking(false); //4. 绑定到TCP端口上，注意ServerSocketChannel不提供bind方法 //需要使用ServerSocketChannel 内部的socket对象对应的bind方法 serverSocketChannel.socket().bind(new InetSocketAddress(\"127.0.0.1\",12122)); //5. 注册 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //6. 服务器循环，调用Selector.select()方法等待IO事件，如果返回值为0，表明没有事件发生 while (true) { //如果select()带参数，它将不会堵塞到等有感兴趣的数据过来 if (selector.select() == 0) { System.out.println(\"Nothing to do!\"); continue; } //获得连接已选键 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); //如果是事件是”通道上有请求“ if (key.isAcceptable()) { //相应的处理是通过accept()操作获得SocketChannel对象，并配置对象的异步工作方式 SocketChannel channel = serverSocketChannel.accept(); //设置异步工作模式、注册到选择器中，注册事件为通道可读 SelectionKeys.OP_READ channel.configureBlocking(false); SelectionKey connkey = channel.register(selector, SelectionKey.OP_READ); //根据注册分到的SelectionKey 对象构造连接对象，并将对象作为SelectionKey对象附件 NIOConnection conn = new NIOConnection(connkey); connkey.attach(conn); } //key有效，即通道未关闭，并且为可读的OP_READ if (key.isValid() &amp;&amp; key.isReadable()) { NIOConnection conn = (NIOConnection) key.attachment(); conn.handleRead(); } //key有效，即通道未关闭，并且为可写的 OP_WRITE if (key.isValid() &amp;&amp; key.isWritable()) { NIOConnection conn = (NIOConnection) key.attachment(); conn.handleWrite(); } } } }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/**NIOConnection.java**/import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;public class NIOConnection { private SelectionKey key; private SocketChannel channel; private ByteBuffer buffer; public NIOConnection(SelectionKey key) { this.buffer = ByteBuffer.allocate(1024); this.key = key; this.channel = (SocketChannel) key.channel(); } /** * 读操作 * @throws IOException */ public void handleRead() throws IOException { int byteRead = channel.read(buffer); if (byteRead == -1) { //对方已关闭socket，服务器就将通道关闭 channel.close(); }else { //有数据可读，此时设置感兴趣的I/O事件为读或写， //读出了一部分数据，就说嘛有空间可以写了，当然还有可能有其他数据可读 key.interestOps(SelectionKey.OP_READ | SelectionKey.OP_WRITE); } } public void handleWrite() throws IOException { //要开始读数据了，就得先把数据的开始索引，也就是当前索引position，改为数据的开始位置， //在此之前，得先把limit的改为position,position所在位置是数据的下一个写入位置， // 把限制limit设置为position // 结合上面position ~ limit就是当前的全部数据的位置 buffer.flip(); //开始写出 System.out.println(\"收到的数据：\" + Charset.forName(\"UTF-8\").decode(buffer).toString()); channel.write(buffer); //之后判断是否还有数据存在，如果没数据了，就可以将其设置为只可以读了 if (!buffer.hasRemaining()) { key.interestOps(SelectionKey.OP_READ); } //写完后，有可能还有数据剩余，就将数据移到buffer的最前面 buffer.compact(); } 注意：NIO是没有专门的客户端的，你可以使用Socket进行连接，也可以使用telnet进行连接 Selector的使用步骤 创建一个Selector实例: 将该实例注册到各种通道，指定每个通道上感兴趣的I/O操作; 重复执行(选择器循环): 调用一种select()方法; 获取已选键集; 对于已选键集中的每-一个键: 将已选键从键集中移除; 获取信道，并从键中获取附件(如果需要); 确定准备就绪的操作并执行;对于accept操作获得的SocketChannel对象，需将信道设置为非阻塞模式，并将其注册到选择器中; 根据需要，修改键的兴趣操作集。 错误总结 这里的connkey用错会报错Exception in thread &quot;main&quot; java.lang.ClassCastException: sun.nio.ch.ServerSocketChannelImpl cannot be cast to java.nio.channels.SocketChannel","link":"/post/93c86522.html"},{"title":"python可视化","text":"","link":"/post/c0163c11.html"},{"title":"Python/python数据可视化/散点图","text":"","link":"/post/1620326b.html"},{"title":"MapReduce介绍","text":"MapReduce的定义 &emsp;&emsp;Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。&emsp;&emsp;Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 MapReduce优缺点优点 MapReduce 易于编程。它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 良好的扩展性。当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 高容错性。MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 适合PB级以上海量数据的离线处理。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。缺点 MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 MapReduce核心思想 分布式的运算程序往往需要分成至少2个阶段。第一个阶段的maptask并发实例，完全并行运行，互不相干。 第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 MapReduce进程 一个完整的mapreduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调。 MapTask：负责map阶段的整个数据处理流程。 ReduceTask：负责reduce阶段的整个数据处理流程。 MapReduce编程规范 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper阶段 12345（1）用户自定义的Mapper要继承自己的父类 （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） （3）Mapper中的业务逻辑写在map()方法中 （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） （5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 Reduce阶段 1234（1）用户自定义的Reducer要继承自己的父类 （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV （3）Reducer的业务逻辑写在reduce()方法中 （4）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 第一代和第二代MapReduce的区别","link":"/post/5429.html"},{"title":"Hadoop 压缩","text":"压缩 压缩是指将数据转换为比原来的格式占用空间更小的格式来存储，以达到减小存储空间解压是压缩的反过程 Hadoop文件切片 Hadoop MapReduce是通过划分切片来处理得，这样就使得支持分割的压缩格式更适合Hadoop 针对ss.txt文件大小为300M 计算公式computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M（Hadoop 1.x中块大小为64M） 默认情况下，切片大小=blocksize 开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） 注意：切片主要由这几个值来运算决定 12mapreduce.input.fileinputformat.split.minsize=1 默认值为1mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认Long.MAXValue 因此，默认情况下，切片大小=blocksize。 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 Hadoop压缩 Hadoop作为一个叫通用的海量数据处理平台，在压缩方面主要考虑压缩速度和压缩的可分割性 小提示： 使用gzip压缩文件时 -9表示空间优先，也就是先考虑压缩空间的减小 -1表示时间优先，也就是压缩速度要快 Hadoop支持的压缩格式 压缩格式 工具 算法 扩展名 多文件 可分割性 换成压缩格式后，原来的程序是否需要修改 DEFLATE 无 DEFLATE .deflate 不 不 和文本处理一样，不需要修改 GZIP gzip DEFLATE .gzp 不 不 和文本处理一样，不需要修改 ZIP zip DEFLATE .zip 是 是，在文件范围内 BZIP2 bzip2 BZIP2 .bz2 不 是 和文本处理一样，不需要修改 LZO lzop LZO .lzo 不 是 需要建索引，还需要指定输入格式 压缩算法及其编码/解码器 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 性能压缩比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 压缩格式选择Gzip压缩 优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 Bzip2压缩 优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native(java和c互操作的API接口)；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 Lzo压缩 优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。 Snappy压缩 优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。 Hadoop 压缩案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Demo { public static void main(String[] args) throws IOException, ClassNotFoundException { //压缩为bz2格式 compress(\"org.apache.hadoop.io.compress.BZip2Codec\"); } private static void compress(String method) throws ClassNotFoundException, IOException { //1. 获取resources下的资源文件流 InputStream in = Demo.class.getClassLoader().getResourceAsStream(\"properties.xml\"); //2. 通过Java反射机制创建对应得编码名称 Class&lt;?&gt; codeClass = Class.forName(method); //3. 通过编码名称找对应得编码/解码器 Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codeClass, conf); //4. 指定压缩后的文件,codec.getDefaultExtension()获得相应的扩展名 File fileOut = new File(System.currentTimeMillis() + codec.getDefaultExtension()); //如果文件存在，删除，否则什么也不做 fileOut.delete(); //5. 创建输出流 FileOutputStream out = new FileOutputStream(fileOut); //6. 通过编码/解码器创建对应得输出流 CompressionOutputStream cout = codec.createOutputStream(out); //7. 压缩输出 /** * in 输入流 * cout 压缩输出流 * 1024 缓冲大小 * false 不关闭相应流，true则关闭 */ IOUtils.copyBytes(in,cout,4096,false); in.close(); cout.close(); } /** * 解压缩 * @param file * @throws IOException */ private static void decompress(File file) throws IOException { Configuration conf = new Configuration(); CompressionCodecFactory factory = new CompressionCodecFactory(conf); //通过扩展名获得编码/解码器 CompressionCodec codec = factory.getCodec(new Path(file.getName())); //通过编码解码器获得输入流 CompressionInputStream in = codec.createInputStream(new FileInputStream(file)); IOUtils.copyBytes(in, System.out, 4096, true); }} 说明：snappy压缩格式在Windows运行失败，不过打包放到集群里面是可以的，前提是你的hadoop集群支持snappy压缩格式","link":"/post/420bf138.html"},{"title":"Hadoop通信机制和内部协议之RPC","text":"Hadoop RPCRPC简介 简要地说，RPC就是允许程序调用位于其他机器上的过程(也可以是同一台机器的不同进程)。RPC调用过程是透明的 传统过程调用：传统的过程调用中，主程序将参数压人栈内并调用过程，这时候主程序停止执行并开始执行相应的过程。被调用的过程从栈中获取参数，然后执行过程函数;执行完毕后，将返回参数入栈(或者保存在寄存器里)，并将控制权交还给调用方。调用方获取返回参数，并继续执行。 而RPC调用是进程间的过程调用 RPC模型 通行模块： 请求-响应 Stub程序： 用于保证RPC的透明性。在客户端，不在本地调用，而是将请求信息通过网络模块发送给法服务器端，服务器接收后进行解码。服务器中，Stub程序依次进行 解码（请求的参数）、调用相应的服务过程、编码返回结果等处理 调度程序： 调度来自通行模块的请求信息，根据其中标识选一个Stub程序运行 客户程序： 请求发出者 服务过程： 请求接收者 一个RPC的旅游： 客户端以本地调用方式产生本地Stub程序 该Stub程序将函数调用信息按照网络通信模块的要求封装成消息包，并交给通信模块发送到远程服务器端。 远程服务器端接收此消息后，将此消息发送给相应的Stub程序 Stub程序拆封消息，形成被调过程要求的形式，并调用对应函数 服务端执行被调用函数，并将结果返回给Stub程序 Stub程序将此结果封装成消息，通过网络通信模块逐级地传送给客户程序。 RPC特性 透明性 调用过程就像本地调用，察觉不到它的经历 高性能 ：Hadoop各个系统（如HDFS、MapReduce、YARN等）均采用了Master/Slave结构，其中，Master实际上是一个RPC server，它负责响应集群中所有Slave发送的服务请求。RPC Server性能要求高，为的是能够让多个客户端并发方位 易用性/可控性 Hadoop系统不采用Java内嵌的RPC（RMI,Remote Method Invocation）框架的主要原因是RPC是Hadoop底层核心模块之一，需要满足易用性、高性能、轻量级等特性 RPC例子执行过程： CalculateClient对象本地调用产生Stub程序 经通信模块上传至服务器CalculateServer对象，在创建Server时设置了协议和业务逻辑（服务过程），处理过后根据上述RPC过程返回 客户端接收后打印到日志中 先定义一些常量 这里不需要太多的在意，直接使用在代码里面也行，在大的项目中为了使程序易于修改而这样设置 1234567891011/** * 静态变量声明类 */public interface Constants { public interface VersionID { public static final long RPC_VERSION = 7788L; } public static final String RPC_HOST = &quot;127.0.0.1&quot;; public static final int RPC_PORT = 8888;} 定义一个Service接口，协议类12345678910111213import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.ipc.ProtocolInfo;@ProtocolInfo(protocolName = &quot;&quot;, protocolVersion = Constants.VersionID.RPC_VERSION)public interface CalculateService { //真实业务逻辑，加减法， public IntWritable add(IntWritable a, IntWritable b); public IntWritable sub(IntWritable a, IntWritable b); public Text echo(Text mt);} @ProtocolInfo(protocolName = “”, protocolVersion = Constants.VersionID.RPC_VERSION) 没有这句就不能将该类设置为协议，不过也可以通过继承VersionProtocol接口 Service接口的实现类1234567891011121314151617181920212223242526272829303132333435import java.io.IOException;public class CalculateServiceImpl implements CalculateService { /** * 该方法没有也行 * */ public ProtocolSignature getProtocolSignature(String arg0, long arg1, int arg2) throws IOException{ return this.getProtocolSignature(arg0, arg1, arg2); } /** * 校验hadoop RFC版本号 * @param arg0 * @param arg1 * @return */ public long getProtocolVersion(String arg0, long arg1) throws IOException { return Constants.VersionID.RPC_VERSION; } @Override public IntWritable add(IntWritable a, IntWritable b) { return new IntWritable(a.get() + b.get()); } @Override public IntWritable sub(IntWritable a, IntWritable b) { return new IntWritable(a.get() - b.get()); } @Override public Text echo(Text mt) { return mt; }} Server和Client类123456789101112131415161718192021222324252627282930import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;public class CalculateServer { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { try { //构造Server,并设置协议接口，主机、端口，真实业务逻辑 RPC.Server server = new RPC.Builder(new Configuration()) .setProtocol(CalculateService.class) .setBindAddress(Constants.RPC_HOST) .setPort(Constants.RPC_PORT) .setInstance(new CalculateServiceImpl()) .build(); //启动Server server.start(); LOG.info(&quot;Server has Started!&quot;); } catch (IOException e) { LOG.error(&quot;Server has Error&quot;); } }} 123456789101112131415161718192021222324252627282930313233343536import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.net.InetSocketAddress;public class CalculateClient { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { //格式化IP和端口 InetSocketAddress addr = new InetSocketAddress(Constants.RPC_HOST, Constants.RPC_PORT); //校验Hadoop RPC版本号 long protocolVersion = RPC.getProtocolVersion(CalculateService.class); try { //获取Server连接 CalculateService proxy = RPC.getProxy(CalculateService.class, protocolVersion, addr, new Configuration()); IntWritable add = proxy.add(new IntWritable(1), new IntWritable(2)); IntWritable sub = proxy.add(new IntWritable(3), new IntWritable(2)); LOG.info(&quot;1+2 = &quot; + add); LOG.info(&quot;3-2 = &quot; + sub); } catch (IOException e) { LOG.error(&quot;Client has error!&quot;); } }} 注意： 查看本程序运行结果需要一个日志文件，如果不想加，把LOG的相关语句换为打印输出就行在resource文件夹下创建 log4j.properties 12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 客户端运行结果12342019-10-31 18:59:28,499 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-10-31 18:59:28,619 WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 1+2 = 3 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 3-2 = 5 其他开源RPC架构 Java RMI Apache Thrift Google Protocol Buffer","link":"/post/56192.html"},{"title":"Hadoop通信机制和内部协议之协议","text":"概述MapReduce核心协议 名称 描述 ClientProtocol 继承于Version基类，查看作业情况监控当前集群等 RefreshUserMappingProtocol 刷新用户到用户组映射关系到超级用户代理组列表 RefreshAuthorizationPolicyProtocol 刷新HDFS和MapReduce服务几倍访问控制列表 ResourceManagerAdministrationProtocol 继承于GetUserMappingProtocol基类，刷新队列列表，节点列表 ## ClientProtocol通信协议 ClientProtocol协议是JobClient和JobTracker之间进行交流的枢纽。JobClient 可以使用该协议中的函数来提交-一个作业(Job) 并执行，以了解当前系统的状态 提交作业协议中JobClient通过Hadoop RPC的submitjob()函数提交作业(Job)，函数所包含的参数有作业ID (JobID)，然后JobClient通过getNewJoblD0函数为作业(Job) 获得一个唯一的ID。 操作作业当用户提交作业(Job) 后，可以通过调用函数来控制该作业的执行流程，如设置提交作业的优先级(setlobPriority()函数) 、停止一个作业(killJob()函数) 、停止一个任务(illTask()函数)。 查看状态从实现源代码来看，该通信协议还提供了一系列函数来 查看状态，如查看集群当前状态(getClusterMetrics()函数)、查看当前任务状态(getJobTrackerStatus()函数) 、获取所有任务(getllobs()函数)等。 RefreshUserMappingProtocol RefreshU serMappingsProtocol 协议用于更新 HDFS 和 MapReduce 级别的用户到用户组映射关系及超级用户代理组列表 refreshUserToGroupsMappings() 函数和refreshSuperUserGroupsConfiguration()函数来实现，这两个函数均是通过调用Hadoop RPC来完成具体的逻辑。 RefreshAuthorizationPolicyProtocol RefreshAuthorizationPol icyProtocol 协议用于刷新当前使用的授权策略 通过调用 Hadoop RPC 远程调用 refreshServiceAcl（）函数，实现基于 HDFS 和MapReduce 级别的授权策略 ResourceManagerAdministrationProtocol ResourceManagerAdministrationProtocol 协议用于更新队列列表、节点 列表 、节点资源等 该协议继承于 GetUserMappingsProtocol 基类 ，通过 Hadoop RPC 远程调用来实现节点更新、资源更新 、添加标签等操作 说明：在IDE中导入hadoop源码加载进去后，按Ctrl+鼠标左键进入即可查看源码","link":"/post/13550.html"},{"title":"hadoop序列化","text":"序列化介绍 序列化是一种将对象的状态信息转化成可以存储或者传输的过程，与之相反的为反序列化不是某一种编程语言所独有的特性序列化的用途 作为一种持久化格式。对象序列化后存盘 作为一种通信的数据格式。如虚拟机之间通信 作为一种拷贝、克隆机制。放缓存 Java序列化 Java通过实现Serializable接口Java序列化后放入对象，通过对象流进行IO操作，ObjectInputStream/ObjectOutputStream 1234567import java. io.Serializable ;／＊＊定义一个可以序列化的 App 信息类. */public class Appinfo implements Serializable{ ／／序列化标识 private static final long serialVersionUID = 11 ;} Hadoop 不使用Java序列化原因 Java 自带的序列化机制占用内存空间大，额外的开销会导致速度降低，Hadoop对序列化的要求较高，需要保证序列化速度快、体积小、占用带宽低等特性 Hadoop 序列化机制是将对象序列化到流中，而 Java 序列化机制是不断创建新对象，对于MapReduce应用来说，不能重用对象 Java序列化在反序列化时，有可能需要访问前一个数据，这将导致数据无法分割来通过MapReduce来处理 Hadoop 序列化 在 Hadoop 序列化机制中，org.apache.hadoop.io包中定义了大量的可序列化对象 Hadoop 序列化机制通过调用write方法（它带有一个类型为DataOutput的参数），将对象序列化到流中 Hadoop 反序列化通过对象的readFields从流中读取数据 Hadoop序列化机制的特征 对于处理大数据的Hadoop平台，其序列化需要具备以下特征 紧凑。这样可以充分利用Hadoop集群的资源，hadoop集群中最稀缺的是资源 快速。进程通信时会大量使用序列化机制，因此需要减少序列化开销 可扩展性。为适应发展，序列化机制也需要支持这些升级和变化 互操作。支持不同语言开发 Hadoop Writable机制 Hadoop序列化都必须实现该接口 均实现Wriable接口的两个函数， 12(1) write：将对象写入字节流：(2) readFields：从字节流中解析出对象。例子 123456789101112131415161718192021222324252627282930313233343536373839/** * BlockWritable有三个对象， * write方法将三个对象写到流中 * readFields从流中读出三个对象 */public class BlockWritable implements Writable { private long blockId; private long numBytes; private long generationStamp; /** * 输出序列化对象到流中 * @param out * @throws IOException */ @Override public void write(DataOutput out) throws IOException { out.writeLong(this.blockId); out.writeLong(this.numBytes); out.writeLong(this.generationStamp); } /** * 从流中读取序列化对象 * 为了效率，尽可能复用现有对象 * @param in 从该流中读取数据 * @throws IOException */ @Override public void readFields(DataInput in) throws IOException { this.blockId = in.readLong(); this.numBytes = in.readLong(); this.generationStamp = in.readLong(); if (this.numBytes &lt; 0L) { throw new IOException(&quot;Unexpected block size: &quot; + this.numBytes); } }} Hadoop序列化的其它几个接口 WritableComparable RawComparator RawComparator允许执行者 比较 流中读取的未被反序列化为对象的 记录，从而省去创建对象所带来的开销 1234567891011/** * * @param var1 字节数组1 * @param var2 字节数组1的开始位置 * @param var3 字节数组1的记录长度 * @param var4 字节数组2 * @param var5 字节数组2的开始位置 * @param var6 字节数组2的记录长度 * @return */int compare(byte[] var1, int var2, int var3, byte[] var4, int var5, int var6); WritableComparator 在RawComparator中WritableComparator是个辅助类，实现了RawComparator接口 以DoubleWritable为例 1234567891011public static class Comparator extends WritableComparator { public Comparator() { super(DoubleWritable.class); } public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) { double thisValue = readDouble(b1, s1); double thatValue = readDouble(b2, s2); return thisValue &lt; thatValue ? -1 : (thisValue == thatValue ? 0 : 1); } } WritableComparator是RawComparator对WritableComparable类的一一个通用实现。提供两个主要功能。首先，提供了一个RawComparator的compare()默认实现，该实现从数据流中反序列化要进行比较的对象，然后调用对象的compare()方法进行比较(这些对象都是Comparable的)。其次，它充当了RawComparator实例的一个工厂方法,通过DoubleWritable获得RawComparator的代码如下 1RawComparator&lt;DoubleWritable&gt; comparator = WritableComparator.get(DoubleWritable.class); RawComparator和WritableComparable的类图 Hadoop 序列化的类java基本类型的封装 说明： 这些类实现了WritableComparable接口 VIntWritable和VLongWritable是只可变长 可变长的格式更空间 VIntWritable可用VLongWritable读入 变长整型分析 writeVLong ()方法实现了对整型数值的变长编码，它的编码规则如下:&emsp;&emsp;如果输入的整数大于或等于-112同时小于或等于127，那么编码需要1字节:否则，序列化结果的第一个字节，保存了输入整数的符号和后续编码的字节数。符号和后续字节数依据下面的编码规则(又一个规则): 如果是正数，则编码值范围落在-113和-120间(闭区间)，后续字节数可以通过-(v+112)计算。 如果是负数，则编码值范围落在-121和-128间(闭区间)，后续字节数可以通过-(v+120)计算。 后续编码将高位在前，写入输入的整数(除去前面全0字节)。代码如下: 1234567891011121314151617181920212223242526272829public static void writeVInt(DataOutput stream, int i) throws IOException { writeVLong(stream, (long)i); } public static void writeVLong(DataOutput stream, long i) throws IOException { if (i &gt;= -112L &amp;&amp; i &lt;= 127L) { stream.writeByte((byte)((int)i)); } else { int len = -112; if (i &lt; 0L) { i = ~i; len = -120; } for(long tmp = i; tmp != 0L; --len) { tmp &gt;&gt;= 8; } stream.writeByte((byte)len); len = len &lt; -120 ? -(len + 120) : -(len + 112); //后续编码 for(int idx = len; idx != 0; --idx) { int shiftbits = (idx - 1) * 8; long mask = 255L &lt;&lt; shiftbits; stream.writeByte((byte)((int)((i &amp; mask) &gt;&gt; shiftbits))); } } } ObjectWritable 针对Java基本类型、字符串、枚举、Writable、空值、Writable的其 他子类,ObjectWritable提供了一个封装，适用于字段需要使用多种类型。ObjectWritable 可应用于Hadoop远程过程调用中参数的序列化和反序列化; ObjectWritable 的另一个典型应用是在需要序列化不同类型的对象到某-个字段，如在一个SequenceFile 的值中保存不同类型的对象( 如LongWritable值或Text值)时，可以将该值声明为ObjectWritable。 ObjectWritable的实现比较冗长，需要根据可能被封装在ObjectWritable中的各种对象进行不同的处理。ObjectWritable 有三个成员变量，包括被封装的对象实例instance、该对象运行时类的Class对象和Configuration对象。 123private Class declaredClass;private Object instance;private Configuration conf; ObjectWritable的write 方法调用的是静态方法ObjectWritable.writeObject()，该方法可以往DataOutput接口中写入各种Java对象。 writeObject()方法先输出对象的类名(通过对象对应的Class对象的getName()方法获得)， 1UTF8.writeString(out, declaredClass.getName()); 然后根据传入对象的类型，分情况系列化对象到输出流中，也就是说，对象通过该方法输出对象的类名，对象序列化结果对到输出流中。在ObjectWritable. writeObject(的逻辑中，需要分别处理null Java 数组、字符串String、Java 基本类型、枚举和Writable的子类6种情况，由于类的继承，处理Writable时，序列化的结果包含对象类名，对象实际类名和对象序列化结果三部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public void write(DataOutput out) throws IOException { writeObject(out, this.instance, this.declaredClass, this.conf); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf) throws IOException { writeObject(out, instance, declaredClass, conf, false); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf, boolean allowCompactArrays) throws IOException { if (instance == null) { instance = new ObjectWritable.NullInstance(declaredClass, conf); declaredClass = Writable.class; } if (allowCompactArrays &amp;&amp; declaredClass.isArray() &amp;&amp; instance.getClass().getName().equals(declaredClass.getName()) &amp;&amp; instance.getClass().getComponentType().isPrimitive()) { instance = new Internal(instance); declaredClass = Internal.class; } UTF8.writeString(out, declaredClass.getName()); /****************此处****************/ if (declaredClass.isArray()) { int length = Array.getLength(instance); out.writeInt(length); for(int i = 0; i &lt; length; ++i) { writeObject(out, Array.get(instance, i), declaredClass.getComponentType(), conf, allowCompactArrays); } } else if (declaredClass == Internal.class) { ((Internal)instance).write(out); } else if (declaredClass == String.class) { UTF8.writeString(out, (String)instance); } else if (declaredClass.isPrimitive()) { if (declaredClass == Boolean.TYPE) { out.writeBoolean((Boolean)instance); } else if (declaredClass == Character.TYPE) { out.writeChar((Character)instance); } else if (declaredClass == Byte.TYPE) { out.writeByte((Byte)instance); } else if (declaredClass == Short.TYPE) { out.writeShort((Short)instance); } else if (declaredClass == Integer.TYPE) { out.writeInt((Integer)instance); } else if (declaredClass == Long.TYPE) { out.writeLong((Long)instance); } else if (declaredClass == Float.TYPE) { out.writeFloat((Float)instance); } else if (declaredClass == Double.TYPE) { out.writeDouble((Double)instance); } else if (declaredClass != Void.TYPE) { throw new IllegalArgumentException(\"Not a primitive: \" + declaredClass); } } else if (declaredClass.isEnum()) { UTF8.writeString(out, ((Enum)instance).name()); } else if (Writable.class.isAssignableFrom(declaredClass)) { UTF8.writeString(out, instance.getClass().getName()); ((Writable)instance).write(out); } else { if (!Message.class.isAssignableFrom(declaredClass)) { throw new IOException(\"Can't write: \" + instance + \" as \" + declaredClass); } ((Message)instance).writeDelimitedTo(DataOutputOutputStream.constructOutputStream(out)); } } 和输出对应，ObjectWritable 的readFields()方法调用的是静态方法ObjectWritable.readObject()，该方法的实现和writeObject()类似，唯一值得研究的是Writable对象处理部分，readObject ()方法依赖于WritableFactories类。WritableFactories 类允许非公有的Writable子类定义一一个对象工厂，由该工厂创建Writable对象，如在上面的readObject()代码中，通过WritableFactories的静态方法newInstance()，可以创建类型为instanceClass的Writable子对象。具体查看org.apache.hadoop.io.WritableFactories类 注：ObjectWritable它比较浪费资源，可以使用静态数组来记录数据类型以提高效率 Hadoop序列化优势 1231. 减少垃圾回收：从流中反序列化数据到当前对象，重复使用当前对象，减少了垃圾回收GC ;2. 减少网络流量 ： 序列化和反序列化对象类型不变 ，因此可以只保存必要的数据来减少网络流量；3. 提升 I/O 效率 ： 由于序列化和反序列化的数据量减少了，配合Hadoop压缩机制，可以提升I/O效率。","link":"/post/20979.html"},{"title":"配置文件","text":"Windows操作系统配置文件 配置设置文件（INI）文件是windows操作系统中的一种特殊的ASCII文件，以ini为文件扩展名,作为它的主要文件配置文件标准。该文件也被称为初始化文件initialization file和概要文件profile。应用程序可以拥有自己的配置文件，存储应用设置信息，也可以访问windows的基本系统配置文件win.ini中存储的配置信息 INI配置信息分为两部分 节，节标题放在方括号中, [section] 项，一个等式，key=value 1234567;注释;节 [section] ;参数（键=值） name=value INI文件片段 1234[0x0419]1100=Ошибка инициализации программы установки1101=%s1102=%1 Идет подготовка к запуску мастера %2, выполняющего установку программы. Ждите. Windows 提供的API 12345678910111213141516DWORD GetPrivateProfileString( LPCTSTR lpAppName, // If this parameter is NULL, the GetPrivateProfileString function copies all section names in the file to the supplied buffer. LPCTSTR lpKeyName, // If this parameter is NULL, all key names in the section specified by the lpAppNameparameter are copied to the buffer specified by the lpReturnedString parameter. LPCTSTR lpDefault, // If the lpKeyName key cannot be found in the initialization file, GetPrivateProfileString copies the default string to the lpReturnedString buffer. LPTSTR lpReturnedString, // destination buffer DWORD nSize, // size of destination buffer LPCTSTR lpFileName // The name of the initialization file);UINT GetPrivateProfileInt( LPCTSTR lpAppName, //节 LPCTSTR lpKeyName,//项 INT nDefault, //The default value to return if the key name cannot be found in the initialization file. LPCTSTR lpFileName //INI文件名); Java配置文件 JDK提供了java.util.Properties类，用于处理简单的配置文件。Properties继承自Hashtable相对于INI文件，Properties处理得配合文件格式非常简单 Properties的使用 非XML文件格式12345678//通过指定的键搜索属性public String getProperty(String key)//功能同上，参数defaultValue提供了默认值public String getProperty(String key, String defaultValue)//最终调用Hashtable 的方法putpublic synchronized object setProperty (String key, String value) Properties中的属性通过load)方法加载，该方法从输入流中读取键-值对，而store()方法法则将Properties表中的属性列表写入输出流。使用输入流和输出流，Properties对象但可以保存在文件中，而且还可以保存在其他支持流的系统中，如Web服务器。 123456789101112131415161718192021222324/** * log4j.properties内容如下 * log4j.rootLogger=INFO, stdout * log4j.appender.stdout=org.apache.log4j.ConsoleAppender * log4j.appender.stdout.layout=org.apache.log4j.PatternLayout * log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n * log4j.appender.logfile=org.apache.log4j.FileAppender * log4j.appender.logfile.File=target/spring.log * log4j.appender.logfile.layout=org.apache.log4j.PatternLayout * log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n * @param args * @throws IOException */ public static void main(String[] args) throws IOException { Properties properties = new Properties(); //获取resources目录下的文件流 InputStream stream = MyConfiguration.class.getClassLoader().getResourceAsStream(\"log4j.properties\"); //加载文件获取并获取配合信息 properties.load(stream); String property = properties.getProperty(\"log4j.rootLogger\"); System.out.println(property); /*输出： INFO, stdout */ } Java 1.5之后支持XML配置文件,Properties中的数据也可以以XML格式保存，对应的加载和写出方法是loadFromXML()和storeToXML() storeToXML() 12345Properties props = new Properties();props.setProperty(\"Length\", \"100\");props.setProperty(\"Width\", \"50\")FileOutputStream fos = new FileOutputStream(\"properties.xml\");props.storeToXML(fos, null); loadFromXML() 1234567Properties props = new Properties();InputStream in = MyConfiguaration2.class.getClassLoader().getResourceAsStream(\"properties.xml\");props.loadFromXML(in)String length = props.getProperty(\"Length\");System.out.println(length); xml有指定格式 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;entry key=\"Width\"&gt;50&lt;/entry&gt; &lt;entry key=\"Length\"&gt;100&lt;/entry&gt;&lt;/properties&gt; java.util.Properties提供的能力有限，其他配置信息读写方法，如Apache Jakarta Commons工具集提供的Commons Configuration 【说明】：上面的MyConfiguration都是自定义的类,并且这些配置文件都在resources文件夹下 Hadoop Configuration详解 Hadoop没有使用java.util.Properties管理配置文件，也没有使用Apache JakartaCommons Configuration 管理配置文件，而是使用了一套独有的配置文件管理系统，并提供自己的API，即使用org.apache.hadoop.conf.Configuration处理配置信息。 hadoop 配置文件格式12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.9.2/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hadoop 配置文件参数说明配置参数类型说明 参数名 String 参数值 boolean int long float，也可以是其他类型 参数说明 根元素 configuration configuration下的property元素 property下 name 参数名 value 参数值 description 参数描述 final 相当于java的final关键字，在资源合并时可以防止配置项被覆盖 合并资源 合并资源是是指将多个配置合并，产生一个配置文件，如core-site.xml和core-defualt.xml通过addResources()方法合并 1234567Configuration conf = new Configuration();//加载resources文件夹内容ClassLoader classLoader = HadoopConfiguaration.class.getClassLoader//添加合并资源conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-site.xml\")));conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-default.xml\")System.out.println(conf.get(\"fs.defaultFS\")); 【注意】如果第一个配置中存在final，则会以下出现警告,并且值不发生更改 1232019-11-12 17:31:34,548 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@cac736f:an attempt to override final parameter: fs.defaultFS; Ignoring. 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@1d7acb34:an attempt to override final parameter: fs.defaultFS; Ignoring. 测试用的core-default.xml文件 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://vmaster:8200&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.3.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;123&lt;/name&gt; &lt;value&gt;wer&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Configuation类的一般过程 构造Configuration对象 添加需要加载的资源 addResource()方法 然后通过set/get 方法访问/设置配置项 【说明】资源会在第一次使用时自动加载到对象中 Configuration类介绍类图 说明类图中，Configuration有7个非静态成员 布尔变量quietmode，用来设置加载配置的模式。如果quietmode为true (默认值)，则在加载解析配置文件的过程中，不输出日志信息。quietmode只是-一个方便开发人员调试的变量。 数组resources保存了所有通过addResource()方法添加Configuration对象的资源 Configuration.addResource()有如下4种形式: 1234public void addResource (InputStream in) //已打开的输入流public void addResource (Path file) //Hadoop文件路径public void addResource (String name) //CLASSPATH 资源 String形式public void addResource (URL url) //URL,统一资源定位符，如https://lyhcc.github.io 布尔变量loadDefaults用于确定是否加载默认资源，这些默认资源保存在defaultResources中。注意，defaultResources 是个静态成员变量，通过方法addDefaultResource()可以添加系统的默认资源。在HDFS中，会把hdfs-default.xml和hdfs-site.xml作为默认资源，并通过addDefaultResource()保存在成员变量defaultResources中;在MapReduce中，默认资源是mapred-default.xml和mapred-site.xml。1234567//下面的代码来自hadoop-1.x 的org.apache.hadoop.hdfs.server.datanode.DataNode static{ Configuration . addDe faultResource (\"hdfs-default . xml\") ; Conf igurat ion. addDe faultResource (\"hdfs-site. xml\") ;}//在hadoop2.x以后这代码被移到了Configuration类里面//hadoop-2.8.4中的1809行 properties 存放Hadoop配置文件解析后的键-值对，为java.util.Properties类型 finalParameters 类型是Set， 用来保存所有在配置文件中已经被声明为final的键-值对的键 overlay用于记录通过set()方式改变的配置项。也就是说，出现在overlay中的键-值对是应用设置的，而不是通过对配置资源解析得到的，为java.util.Properties类型 Configuration 是一一个类加载器变量，可以通过它来加载指定类，也可以通过它加载相关的资源。 上面提到addResource()可以通过字符串方式加载CLASSPATH资源，它其实通过Configuration中的getResource()将字符串转换成URL资源 123public URL getResource (String name){ return classLoader.getResource(name) ;} 2.8.4版本的Configuration类的1188行 Configuration类的过程构造Configuration对象资源加载添加资源到Configuration对象的方法有两种 对象的addResource()方法 类的静态addDefaultResource()方法(设置了loadDefaults标志) 添加的资源并不会立即被加载，只是通过reloadConfiguration()方法清空properties和finalParameters。相关代码如下: 123456789101112131415 //以URL资源为例public void addResource(URL url) { this.addResourceObject(new Configuration.Resource(url)); } //添加资源 private synchronized void addResourceObject(Configuration.Resource resource){ this.resources.add(resource); this.restrictSystemProps |= resource.isParserRestricted(); this.reloadConfiguration(); } //资源重新加载触发函数 public synchronized void reloadConfiguration() { this.properties = null; this.finalParameters.clear(); } 以上是类的成员方法addResource()方法的调用。 静态方法dDefaultResource()也可以清空Configuration对象中的数据（非静态成员），只不过是通过需要通过REGISTRY作为媒介进行。能够调用是因为REGISTRY记录了系统所有的Configuration对象REGISTRY的定义以及为其添加参数的过程 12345678private static final WeakHashMap&lt;Configuration, Object&gt; REGISTRY = new WeakHashMap();public Configuration(boolean loadDefaults) { ... synchronized(Configuration.class) { REGISTRY.put(this, (Object)null); }} 成员变量properties中的数据只在被调用的时候才会被加载进来。在getProps方法中，properties为空时，会触发loadResources()执行 123456789101112131415161718192021222324protected synchronized Properties getProps() { if (this.properties == null) { this.properties = new Properties(); Map&lt;String, String[]&gt; backup = new ConcurrentHashMap(this.updatingResource); this.loadResources(this.properties, this.resources, this.quietmode); ... } return this.properties;}//加载默认资源private void loadResources(Properties properties, ArrayList&lt;Configuration.Resource&gt; resources, boolean quiet) { if (this.loadDefaults) { Iterator i$ = defaultResources.iterator(); while(i$.hasNext()) { String resource = (String)i$.next(); this.loadResource(properties, new Configuration.Resource(resource, false), quiet); } if (this.getResource(\"hadoop-site.xml\") != null) { this.loadResource(properties, new Configuration.Resource(\"hadoop-site.xml\", false), quiet); }} hadoop配置文件解析 hadoop 的配置文件都是XML文件，JAXP(JAVA API for XML processing)是一种稳定的、可靠的XML处理API，支持两种XML处理方法 SAX解析(Simple API for XML) DOM解析(Documnet Object Model) hadoop使用的DOM解析 两种解析的区别 SAX 提供了一种流式的、事件驱动的XML处理方式 缺点：编写处理逻辑比较复杂 优势：适合处理大的XML文件 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档 Configurable接口 Configurable是一个很简单的接口，位于org.apache.hadoop.conf包中 类图 hadoop 代码中存在大量实现了该接口的类，可以通过setConf方法设置配置参数简化创建和setConf的两个步骤java反射机制实现，利用org.apache.hadoop.util.ReflectionUtils的newInstance方法 1public static &lt;T&gt; T newInstance(Class&lt;T&gt; theClass, Configuration conf) 该方法调用了ReflectionUtils中的setConf方法 12345678910public static void setConf(Object theObject, Configuration conf) { if (conf != null) { if (theObject instanceof Configurable) { ((Configurable)theObject).setConf(conf); } setJobConf(theObject, conf); } }","link":"/post/fa571a7.html"},{"title":"HDFSAPI","text":"HDFS文件上传 Java代码12345678910111213141516171819202122232425262728public void putFileToHDFS() throws URISyntaxException, IOException, InterruptedException { //1、创建配置信息对象 Configuration conf = new Configuration();//hjhj //2、设置部分参数 conf.set(\"dfs.replication\",\"2\"); //3、找到HDFS地址 //final URI uri, final Configuration conf, String user FileSystem fs = FileSystem.get(new URI(\"hdfs://vmaster:9000\"), conf, \"root\"); //4、上传本地文件路径 Path src = new Path(\"F:\\\\Course\\\\aliyun\\\\transaction_details.csv\"); //5、要上传的HDFS的路径 Path dest = new Path(\"hdfs://vmaster:9000/\"); //6、以拷贝的方式上传，从src -&gt; dest fs.copyFromLocalFile(src,dest); //7、关闭 fs.close(); // System.out.println(\"OK了\");} 命令 12345678910111213141516hadoop fs -put ./p /./p 源文件 localsrc/ 上传到对应目录，这里表示HDFS的根目录 dst-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt; : Copy files from the local file system into fs. Copying fails if the file already exists, unless the -f flag is given. Flags: -p Preserves access and modification times, ownership and the mode. -f Overwrites the destination if it already exists. -l Allow DataNode to lazily persist the file to disk. Forces replication factor of 1. This flag will result in reduced durability. Use with care. -d Skip creation of temporary file(&lt;dst&gt;._COPYING_). 使用RESTFUL API上传 使用PUT方法发送一个请求，这里-X 后面加PUT就是以PUT发起请求 然后使用返回的Location，在发送一个PUT请求 123456789101112131415161718192021222324252627Step1: curl -i -X PUT &quot;http://vmaster:50070/webhdfs/v1/1234.txt?op=create&amp;noredirect=true&quot;HTTP/1.1 100 ContinueHTTP/1.1 200 OKCache-Control: no-cacheExpires: Mon, 11 Jan 2021 02:20:57 GMTDate: Mon, 11 Jan 2021 02:20:57 GMTPragma: no-cacheExpires: Mon, 11 Jan 2021 02:20:57 GMTDate: Mon, 11 Jan 2021 02:20:57 GMTPragma: no-cacheX-FRAME-OPTIONS: SAMEORIGINContent-Type: application/jsonTransfer-Encoding: chunked{&quot;Location&quot;:&quot;http://vslave2:50075/webhdfs/v1/1234.txt?op=CREATE&amp;namenoderpcaddress=vmaster:9000&amp;createflag=&amp;createparent=true&amp;overwrite=false&quot;}Step2:curl -i -X PUT -T &quot;F:\\\\Course\\\\BigData\\\\H.txt&quot; &quot;http://vslave2:50075/webhdfs/v1/1234.txt?op=CREATE&amp;namenoderpcaddress=vmaster:9000&amp;createflag=&amp;createparent=true&amp;overwrite=false&quot;HTTP/1.1 100 ContinueHTTP/1.1 201 CreatedLocation: hdfs://vmaster:9000/1234.txtContent-Length: 0Access-Control-Allow-Origin: *Connection: close 如何将curl的命令转为直接发送请求，比如在程序中如何发起，https://zhuanlan.zhihu.com/p/33481273 HDFS目录创建 Java代码 12345678910public void mkdirHDFS() throws URISyntaxException, IOException, InterruptedException { //1、创建配置对象 Configuration conf = new Configuration(); //2、获取文件系统 FileSystem fs = FileSystem.get(new URI(\"hdfs://vmaster:9000\"), conf, \"root\"); //3、创建目录 fs.mkdirs(new Path(\"hdfs://vmaster:9000/Good\")); //4、关闭文件系统 fs.close(); } 命令 123hadoop fs -mkdir /dir递归创建hadoop fs -mkdir -p /dir/1/2/3 RESTful API 1curl -i -X PUT &quot;http://vmaster:50070/webhdfs/v1/dir?op=mkdirs&quot; HDFS目录，文件删除 Java代码 12345678910111213public void delHDFS() throws URISyntaxException, IOException, InterruptedException { //1、创建配置对象 Configuration conf = new Configuration(); //2、获取文件系统 FileSystem fs = FileSystem.get(new URI(\"hdfs://vmaster:9000\"), conf, \"root\"); //3、删除文件 //Path var1,删除的路径 // boolean var2//是否递归删除 fs.delete(new Path(\"hdfs://vmaster:9000/Good\"),true); //4、关闭系统 fs.close(); System.out.println(\"del\");} 命令 1hadoop fs -rm /LICENSE.txt RESTful 1curl -i -X DELETE &quot;http://vmaster:50070/webhdfs/v1/12.txt?op=DELETE&quot; HDFS文件下载 Java代码 12345678910111213141516171819public void getFileFromHDFS() throws URISyntaxException, IOException, InterruptedException { //1、创建配置信息对象 Configuration conf = new Configuration(); //2、找到文件系统 FileSystem fs = FileSystem.get(new URI(\"hdfs://vmaster:9000\"),conf,\"root\"); //3、下载文件 //boolean delSrc,是否将源文件删除 // Path src,要下载的路径 // Path dst, 要下载到那 // boolean useRawLocalFileSystem：是否校验文件 // fs.copyToLocalFile(false,new Path(\"hdfs://vmaster:9000/transaction_details.csv\"),new Path(\"F:\\\\Course\\\\BigData\"),true); //4、关闭文件系统 fs.close(); System.out.println(\"OKl\");} 命令 1hadoop fs -get /NOTICE.txt 查看文件信息 Java代码 12345678910111213141516171819202122232425262728293031public void readLsitFile() throws URISyntaxException, IOException, InterruptedException { //1、创建配置对象 Configuration conf = new Configuration(); //2、获取文件系统 FileSystem fs = FileSystem.get(new URI(\"hdfs://vmaster:9000\"), conf, \"root\"); //3、迭代器 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(\"hdfs://vmaster:9000/\"), true); //4、遍历迭代器 while(listFiles.hasNext()){ LocatedFileStatus fileStatus = listFiles.next(); //路径名 System.out.println(\"文件名\" + fileStatus.getPath().getName()); //块的大小 System.out.println(\"大小\" + fileStatus.getBlockSize()); //权限 System.out.println(\"权限\" + fileStatus.getPermission()); // System.out.println(fileStatus.getLen()); BlockLocation[] locations = fileStatus.getBlockLocations(); for(BlockLocation bl:locations){ System.out.println(\"block-offset\" + bl.getOffset()); String[] hosts = bl.getHosts(); for(String host:hosts){ System.out.println(host); } } System.out.println(\"----------------------------------------\"); } } 文件，目录判断12345678910111213141516171819public void checkFile() throws URISyntaxException, IOException, InterruptedException { //1、创建配置对象 Configuration conf = new Configuration(); //2、获取文件系统 FileSystem fs = FileSystem.get(new URI(\"hdfs://nna:9000\"), conf, \"root\"); //3、遍历所有文件 FileStatus[] status = fs.listStatus(new Path(\"/\")); for(FileStatus status1:status){ //判断是否为文件 if(status1.isFile()){ System.out.println(\"文件：\" + status1.getPath().getName()); }else { System.out.println(\"目录：\" + status1.getPath().getName()); } } //4、关闭文件系统 fs.close();} RESTful 更多查看 https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/WebHDFS.htmlJava 文档 https://hadoop.apache.org/docs/r2.9.2/api/index.html命令使用hadoop fs -help 查看","link":"/post/ad365e02.html"},{"title":"HDFS快照管理","text":"快照管理 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 快照影响 快照创建瞬间完成，所耗时间成本为O(1) 快照修改时才会使用额外的额外的内存空间，内存成本O(M),M表示修改过的文件或目录数 快照记录块和文件大小，不对DataNode中的块进行复制 说明：* 快照可以在HDFS任何目录下设置，一个目录最多容纳65536个并发快照 基本语法12345678（1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能）（2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）（3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照）（4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照）（5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）（6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有已快照目录）（7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处）（8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 案例实操 开启/禁用指定目录的快照功能 指定创建目录的位置为 /tmp/snapshot(即快照的存储目录)，在指定目录之前必须创建目录，不然会报错 12hdfs dfsadmin -allowSnapshot /tmp/snapshot hdfs dfsadmin -disallowSnapshot /tmp/snapshot //禁用时，对应的目录不允许存在快照 对目录创建快照 只有被开启快照功能的目录才能创建快照 123hdfs dfs -createSnapshot /tmp/snapshot // 对目录创建快照hdfs dfs -createSnapshot /tmp/snapshot snapshot //重命名快照（注：快照是只读的，无法修改名）通过web访问hdfs://Master:9000/tmp/snapshot/.snapshot/s…..// 快照和源文件使用相同数据块 查看快照 12hdfs dfs -lsr /tmp/snapshot/.snapshot/ //查看快照目录的详细信息hdfs lsSnapshottableDir //查看所有允许快照的目录 更改快照名字 12345hdfs dfs -renameSnapshot /tmp/snapshot/ snapshot snapshot1 注：路径只是你创建得名字/tmp/snapshot，不要带后边得/tmp/snapshot/.snapshot/，不然会出现hdfs dfs -renameSnapshot /tmp/snapshot/.snapshot/ snapshot1 snapshotrenameSnapshot: Modification on a read-only snapshot is disallowed 比较两个快照目录的不同之处 1234[root@vmaster opt]# hdfs snapshotDiff /tmp/snapshot s1 s2Difference between snapshot s1 and snapshot s2 under directory /tmp/snapshot:M .+ ./p 符号的意义： 符号 含义 + 文件或者目录被创建 - 文件或目录被删除 M 文件或目录被修改 R 文件或目录被重命名 恢复快照 123451.自定义创建一个快照名：hdfs dfs -createSnapshot /HAHA1 snapshot12.展示原文件包含内容：Hadoop fs -ls /HAHA13.里面有五个文件、删除其中1~2个/HAHA1/.snapshot/snapshot14.回复快照：hdfs dfs -cp /HAHA1/.snapshot/snapshot1 /snapshot 删除快照 12dfs dfs -deleteSnapshot 快照目录 快照名称dfs dfs -deleteSnapshot /tmp/snapshot snapshot1","link":"/post/21534.html"},{"title":"Hadoop IPC 连接的建立","text":"Hadoop 2.x 相关参数说明 connections 用于保存ConnectionId到Connection的映射，位于org.apache.hadoop.ipc.Client中 1private ConcurrentMap&lt;Client.ConnectionId, Client.Connection&gt; connections; calls 当前正在处理的远程调用，位于org.apache.hadoop.ipc.Client.Connection中 1private Hashtable&lt;Integer, Client.Call&gt; calls = new Hashtable(); shouldCloseConnection 连接关闭标志 1private AtomicBoolean shouldCloseConnection = new AtomicBoolean(); 相关方法说明 getConnection Client需要获取连接的时候，调用getConnection方法，该方法先检查connections中是否存在满足条件的IPC连接。有，则 复用 ，否则，创建新的连接复用是指connection相等（connection里面的三个参数相等则说明两个connection相等），就使用同一个connection 相关代码如下 123456789101112131415161718192021222324252627 private Client.Connection getConnection(Client.ConnectionId remoteId, Client.Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth) throws IOException { //首先，看看客户端是否还在运行 if (!this.running.get()) { throw new IOException(\"The client is stopped\"); } else { while(true) { //2. 查一下是否存在remoteId对应的连接connection Client.Connection connection = (Client.Connection)this.connections.get(remoteId); //connection==null，表明不存在，就需要创建一个新的IPC连接 if (connection == null) { //创建连接 connection = new Client.Connection(remoteId, serviceClass); Client.Connection existing = (Client.Connection)this.connections.putIfAbsent(remoteId, connection); if (existing != null) { connection = existing; } } //将IPC调用放入IPC连接中 if (connection.addCall(call)) { connection.setupIOstreams(fallbackToSimpleAuth); return connection; } this.connections.remove(remoteId, connection); } }} addCall 作用是将一个IPC调用放入IPC连接中&emsp;如果成员变量shouldCloseConnection为true, 返回false，这样可以防止将一个IPC调用放入一个已经关闭的IPC连接中。&emsp;否则，将调用放入IPC连接calls中再说一下，为什么会有这种情况？IPC来呢及可以在多个地方被触发,进入关闭过程，但知道Connection.close方法被调用，对应的connection才会在connections中删除。删除后的连接只有新建后才能将IPC调用传入连接中相关代码 12345678910 private synchronized boolean addCall(Client.Call call) { if (this.shouldCloseConnection.get()) { return false; } else { this.calls.put(call.id, call); //notify是唤醒等待的线程，因为这个方法会有多个地方调用，但进来的只能有一个 this.notify(); return true; }} setupIOstreams 该方法是使客户端和服务器通过Socket连接起来连接失败的话，会重传，最多maxRetries，可以设置${ipc.client.connect.max.retries} 12345678910111213141516171819202122232425262728293031323334353637 private synchronized void setupIOstreams(AtomicBoolean fallbackToSimpleAuth) { if (this.socket == null &amp;&amp; !this.shouldCloseConnection.get()) { ... if (Client.LOG.isDebugEnabled()) { Client.LOG.debug(\"Connecting to \" + this.server); } Span span = Tracer.getCurrentSpan(); if (span != null) { span.addTimelineAnnotation(\"IPC client connecting to \" + this.server); } short numRetries = 0; Random rand = null; while(true) { /** * 建立Socket连接，具体可以查看源代码Ctrl + 鼠标左键查看 * connection使用Socket连接设置了tcpNoDelay标志，禁用Nagle算法,无需等待直接发送 * 配置项${ipc.client.tcpnodelay} */ this.setupConnection(ticket); ... //与IPC服务器进行握手 this.writeConnectionContext(this.remoteId, this.authMethod); //更改最后访问时间lastActivity,该变量也是Client成员变量 this.touch(); ... //启动接受进程 this.start(); ... this.close(); } }} 以上是客户端，下面为IPC连接的另一端 服务器端 服务器建立IPC连接的代码分散在Listener和Server.Connection中Listener基于Java NIO开发的，是一个标准的NIO应用，其构造函数中打开服务器端口，创建Selector监听注意参数backlogLength，它由ipc.server.listen.queue.size参数指定，backlogLength是调用ServerSocket.bind()时可以额外提供的一个参数，用于指定在监听端口上排队请求的最大长度，队列满了以后的客户端请求，会被拒绝 run方法123456789101112131415161718192021222324252627282930public void run() { ... while(Server.this.running) { SelectionKey key = null; try { //select调用 this.getSelector().select(); //获取相关键 for(Iterator iter = this.getSelector().selectedKeys().iterator(); iter.hasNext(); key = null) { key = (SelectionKey)iter.next(); iter.remove(); try { //判断是不是可接受事件 if (key.isValid() &amp;&amp; key.isAcceptable()) { //doAccept方法,接受客户端请求、注册Socket到选择器上， //并且创建Reader，Reader里面的run方法处理 OP_READ //该方法中最主要的是ReadAndProcess方法 this.doAccept(key); } } catch (IOException var8) { } ... } }","link":"/post/28a780a5.html"},{"title":"Hadoop IPC 数据分帧和读写","text":"数据通信中定界的方法 定长消息:通信双方发送的消息长度是固定的，接收者只需要简单地将数据读入对应的缓冲区中，就可以获得消息。 基于定界符:消息的结束由唯一标记指出，消息发送者在传输完数据后，添加一-个特殊的字节序列。这个特殊的标记不能在传输的数据中出现，接收者简单地扫描输入信息并查找定界符，并将定位符前面的数据形成消息交给上层应用。 显式长度:在具体消息前面附加一一个固定大小的字段，指示该消息包含多少字节。接收者首先以定长消息的方式接受长度字段，然后根据这个长度接收消息。 Hadoop IPC通信的定界方法 客户端-&gt;服务器端：显式长度 服务器-&gt;客户端：定长消息，通过Writable序列化","link":"/post/a9423fcb.html"}],"tags":[{"name":"分布式存储管理","slug":"分布式存储管理","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"name":"NoSSQL特点","slug":"NoSSQL特点","link":"/tags/NoSSQL%E7%89%B9%E7%82%B9/"},{"name":"什么是大数据","slug":"什么是大数据","link":"/tags/%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"十大经典算法优缺点","slug":"十大经典算法优缺点","link":"/tags/%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9/"},{"name":"Tomcat 的安装与配置","slug":"Tomcat-的安装与配置","link":"/tags/Tomcat-%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"},{"name":"Java Web过滤器和监听器","slug":"Java-Web过滤器和监听器","link":"/tags/Java-Web%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E7%9B%91%E5%90%AC%E5%99%A8/"},{"name":"AJAX 和JSON的使用","slug":"AJAX-和JSON的使用","link":"/tags/AJAX-%E5%92%8CJSON%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"name":"Java设计模式面试题","slug":"Java设计模式面试题","link":"/tags/Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"Java设计模式之设计模式七大原则","slug":"Java设计模式之设计模式七大原则","link":"/tags/Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%83%E5%A4%A7%E5%8E%9F%E5%88%99/"},{"name":"创建型设计模式","slug":"创建型设计模式","link":"/tags/%E5%88%9B%E5%BB%BA%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"软件测试之软件测试方法","slug":"软件测试之软件测试方法","link":"/tags/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E4%B9%8B%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95/"},{"name":"Spring AOP","slug":"Spring-AOP","link":"/tags/Spring-AOP/"},{"name":"Spring AOP引入之Java动态代理打印日志","slug":"Spring-AOP引入之Java动态代理打印日志","link":"/tags/Spring-AOP%E5%BC%95%E5%85%A5%E4%B9%8BJava%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E6%89%93%E5%8D%B0%E6%97%A5%E5%BF%97/"},{"name":"Spring初始之HelloWorld","slug":"Spring初始之HelloWorld","link":"/tags/Spring%E5%88%9D%E5%A7%8B%E4%B9%8BHelloWorld/"},{"name":"Spring JDBCTemplate","slug":"Spring-JDBCTemplate","link":"/tags/Spring-JDBCTemplate/"},{"name":"Spring初识","slug":"Spring初识","link":"/tags/Spring%E5%88%9D%E8%AF%86/"},{"name":"Spring 声明式事务","slug":"Spring-声明式事务","link":"/tags/Spring-%E5%A3%B0%E6%98%8E%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"Spring IOC实验","slug":"Spring-IOC实验","link":"/tags/Spring-IOC%E5%AE%9E%E9%AA%8C/"},{"name":"Sping与Java Web","slug":"Sping与Java-Web","link":"/tags/Sping%E4%B8%8EJava-Web/"},{"name":"SpringIOC总结","slug":"SpringIOC总结","link":"/tags/SpringIOC%E6%80%BB%E7%BB%93/"},{"name":"Spring的单元测试","slug":"Spring的单元测试","link":"/tags/Spring%E7%9A%84%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"},{"name":"Shell 之Bash数学运算","slug":"Shell-之Bash数学运算","link":"/tags/Shell-%E4%B9%8BBash%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/"},{"name":"Spring AOP 之基于注解的AOP","slug":"Spring-AOP-之基于注解的AOP","link":"/tags/Spring-AOP-%E4%B9%8B%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E7%9A%84AOP/"},{"name":"MySQL数据备份","slug":"MySQL数据备份","link":"/tags/MySQL%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD/"},{"name":"Spring 源码","slug":"Spring-源码","link":"/tags/Spring-%E6%BA%90%E7%A0%81/"},{"name":"Shell编程 awk","slug":"Shell编程-awk","link":"/tags/Shell%E7%BC%96%E7%A8%8B-awk/"},{"name":"Shell 的函数的定义和使用","slug":"Shell-的函数的定义和使用","link":"/tags/Shell-%E7%9A%84%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E5%92%8C%E4%BD%BF%E7%94%A8/"},{"name":"Shell 之find、which、locate、whereis命令总结","slug":"Shell-之find、which、locate、whereis命令总结","link":"/tags/Shell-%E4%B9%8Bfind%E3%80%81which%E3%80%81locate%E3%80%81whereis%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{"name":"mysql命令详情","slug":"mysql命令详情","link":"/tags/mysql%E5%91%BD%E4%BB%A4%E8%AF%A6%E6%83%85/"},{"name":"Shell全局变量和局部变量","slug":"Shell全局变量和局部变量","link":"/tags/Shell%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%92%8C%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F/"},{"name":"Shell命令的替换","slug":"Shell命令的替换","link":"/tags/Shell%E5%91%BD%E4%BB%A4%E7%9A%84%E6%9B%BF%E6%8D%A2/"},{"name":"Shell变量的替换和测试","slug":"Shell变量的替换和测试","link":"/tags/Shell%E5%8F%98%E9%87%8F%E7%9A%84%E6%9B%BF%E6%8D%A2%E5%92%8C%E6%B5%8B%E8%AF%95/"},{"name":"Shell 的字符串处理","slug":"Shell-的字符串处理","link":"/tags/Shell-%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86/"},{"name":"Shell文件查找find命令","slug":"Shell文件查找find命令","link":"/tags/Shell%E6%96%87%E4%BB%B6%E6%9F%A5%E6%89%BEfind%E5%91%BD%E4%BB%A4/"},{"name":"Shell有类型变量","slug":"Shell有类型变量","link":"/tags/Shell%E6%9C%89%E7%B1%BB%E5%9E%8B%E5%8F%98%E9%87%8F/"},{"name":"文本处理三剑客(grep/sed/awk)","slug":"文本处理三剑客-grep-sed-awk","link":"/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2-grep-sed-awk/"},{"name":"Shell之脚本功能概述","slug":"Shell之脚本功能概述","link":"/tags/Shell%E4%B9%8B%E8%84%9A%E6%9C%AC%E5%8A%9F%E8%83%BD%E6%A6%82%E8%BF%B0/"},{"name":"SpringMVC之HelloWorld","slug":"SpringMVC之HelloWorld","link":"/tags/SpringMVC%E4%B9%8BHelloWorld/"},{"name":"HiddenHttpMethodFilter (REST)","slug":"HiddenHttpMethodFilter-REST","link":"/tags/HiddenHttpMethodFilter-REST/"},{"name":"SpringMVC之@RequestMapping","slug":"SpringMVC之-RequestMapping","link":"/tags/SpringMVC%E4%B9%8B-RequestMapping/"},{"name":"SpringMVC之响应数据传出","slug":"SpringMVC之响应数据传出","link":"/tags/SpringMVC%E4%B9%8B%E5%93%8D%E5%BA%94%E6%95%B0%E6%8D%AE%E4%BC%A0%E5%87%BA/"},{"name":"SpringMVC 请求数据传入","slug":"SpringMVC-请求数据传入","link":"/tags/SpringMVC-%E8%AF%B7%E6%B1%82%E6%95%B0%E6%8D%AE%E4%BC%A0%E5%85%A5/"},{"name":"Java NIO","slug":"Java-NIO","link":"/tags/Java-NIO/"},{"name":"SpringMVC之视图解析器","slug":"SpringMVC之视图解析器","link":"/tags/SpringMVC%E4%B9%8B%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90%E5%99%A8/"},{"name":"hadoop配置文件解析","slug":"hadoop配置文件解析","link":"/tags/hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/"},{"name":"DOM解析","slug":"DOM解析","link":"/tags/DOM%E8%A7%A3%E6%9E%90/"},{"name":"SpringMVC之RESTful实现CRUD","slug":"SpringMVC之RESTful实现CRUD","link":"/tags/SpringMVC%E4%B9%8BRESTful%E5%AE%9E%E7%8E%B0CRUD/"},{"name":"SpringMVC源码分析","slug":"SpringMVC源码分析","link":"/tags/SpringMVC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Java代理机制","slug":"Java代理机制","link":"/tags/Java%E4%BB%A3%E7%90%86%E6%9C%BA%E5%88%B6/"},{"name":"Java Proxy","slug":"Java-Proxy","link":"/tags/Java-Proxy/"},{"name":"Linux 工具包","slug":"Linux-工具包","link":"/tags/Linux-%E5%B7%A5%E5%85%B7%E5%8C%85/"},{"name":"SpringMVC之ModelAttribute","slug":"SpringMVC之ModelAttribute","link":"/tags/SpringMVC%E4%B9%8BModelAttribute/"},{"name":"Vim多行注释","slug":"Vim多行注释","link":"/tags/Vim%E5%A4%9A%E8%A1%8C%E6%B3%A8%E9%87%8A/"},{"name":"Java远程调用","slug":"Java远程调用","link":"/tags/Java%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"},{"name":"RMI","slug":"RMI","link":"/tags/RMI/"},{"name":"Java代理","slug":"Java代理","link":"/tags/Java%E4%BB%A3%E7%90%86/"},{"name":"Kafka之broker","slug":"Kafka之broker","link":"/tags/Kafka%E4%B9%8Bbroker/"},{"name":"kafka通信","slug":"kafka通信","link":"/tags/kafka%E9%80%9A%E4%BF%A1/"},{"name":"Kafka生产与消费","slug":"Kafka生产与消费","link":"/tags/Kafka%E7%94%9F%E4%BA%A7%E4%B8%8E%E6%B6%88%E8%B4%B9/"},{"name":"Hadooop介绍","slug":"Hadooop介绍","link":"/tags/Hadooop%E4%BB%8B%E7%BB%8D/"},{"name":"Google三宝","slug":"Google三宝","link":"/tags/Google%E4%B8%89%E5%AE%9D/"},{"name":"Kafka初识","slug":"Kafka初识","link":"/tags/Kafka%E5%88%9D%E8%AF%86/"},{"name":"Java NIO实例：回显服务器","slug":"Java-NIO实例：回显服务器","link":"/tags/Java-NIO%E5%AE%9E%E4%BE%8B%EF%BC%9A%E5%9B%9E%E6%98%BE%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"MapReduce介绍","slug":"MapReduce介绍","link":"/tags/MapReduce%E4%BB%8B%E7%BB%8D/"},{"name":"Hadoop 压缩","slug":"Hadoop-压缩","link":"/tags/Hadoop-%E5%8E%8B%E7%BC%A9/"},{"name":"Hadoop 文件分片","slug":"Hadoop-文件分片","link":"/tags/Hadoop-%E6%96%87%E4%BB%B6%E5%88%86%E7%89%87/"},{"name":"Hadoop IPC","slug":"Hadoop-IPC","link":"/tags/Hadoop-IPC/"},{"name":"Hadoop RPC","slug":"Hadoop-RPC","link":"/tags/Hadoop-RPC/"},{"name":"Hadoop 通信机制","slug":"Hadoop-通信机制","link":"/tags/Hadoop-%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/"},{"name":"MapReduce 通信协议","slug":"MapReduce-通信协议","link":"/tags/MapReduce-%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"},{"name":"Hadoop 协议","slug":"Hadoop-协议","link":"/tags/Hadoop-%E5%8D%8F%E8%AE%AE/"},{"name":"hadoop序列化","slug":"hadoop序列化","link":"/tags/hadoop%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"Windows操作系统配置文件","slug":"Windows操作系统配置文件","link":"/tags/Windows%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Java配置文件","slug":"Java配置文件","link":"/tags/Java%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Hadoop配置文件","slug":"Hadoop配置文件","link":"/tags/Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"HDFS快照","slug":"HDFS快照","link":"/tags/HDFS%E5%BF%AB%E7%85%A7/"},{"name":"Hadoop 如何建立 IPC连接","slug":"Hadoop-如何建立-IPC连接","link":"/tags/Hadoop-%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B-IPC%E8%BF%9E%E6%8E%A5/"},{"name":"Hadoop IPC通信中是如何知道数据长度的","slug":"Hadoop-IPC通信中是如何知道数据长度的","link":"/tags/Hadoop-IPC%E9%80%9A%E4%BF%A1%E4%B8%AD%E6%98%AF%E5%A6%82%E4%BD%95%E7%9F%A5%E9%81%93%E6%95%B0%E6%8D%AE%E9%95%BF%E5%BA%A6%E7%9A%84/"},{"name":"Hadoop IPC 数据分帧和读写","slug":"Hadoop-IPC-数据分帧和读写","link":"/tags/Hadoop-IPC-%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%A7%E5%92%8C%E8%AF%BB%E5%86%99/"}],"categories":[{"name":"BigData","slug":"BigData","link":"/categories/BigData/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"DataMining","slug":"DataMining","link":"/categories/DataMining/"},{"name":"Java Web","slug":"Java-Web","link":"/categories/Java-Web/"},{"name":"Java设计模式","slug":"Java设计模式","link":"/categories/Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Leetcode","slug":"Leetcode","link":"/categories/Leetcode/"},{"name":"Leecode","slug":"Leecode","link":"/categories/Leecode/"},{"name":"SoftwareTesting","slug":"SoftwareTesting","link":"/categories/SoftwareTesting/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"Shell","slug":"Shell","link":"/categories/Shell/"},{"name":"Spring MVC","slug":"Spring-MVC","link":"/categories/Spring-MVC/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"Sprig MVC","slug":"Sprig-MVC","link":"/categories/Sprig-MVC/"},{"name":"Kafka","slug":"BigData/Kafka","link":"/categories/BigData/Kafka/"},{"name":"Hadoop","slug":"BigData/Hadoop","link":"/categories/BigData/Hadoop/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Python可视化","slug":"Python/Python可视化","link":"/categories/Python/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"MapReduce","slug":"BigData/Hadoop/MapReduce","link":"/categories/BigData/Hadoop/MapReduce/"},{"name":"Hadoop common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"Hadoop_common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"HDFS","slug":"BigData/Hadoop/HDFS","link":"/categories/BigData/Hadoop/HDFS/"},{"name":"IPC","slug":"BigData/Hadoop/Hadoop-common/IPC","link":"/categories/BigData/Hadoop/Hadoop-common/IPC/"}]}