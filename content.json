{"pages":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start$Create a new post$1$ hexo new \"My New Post\" More info: Writing Run server$1$ hexo server More info: Server Generate static files$1$ hexo generate More info: Generating Deploy to remote sites$1$ hexo deploy More info: Deployment","link":"/about/about.html"},{"title":"gallery","text":"","link":"/album/index.html"},{"title":"标签","text":"","link":"/tags/index.html"},{"title":"分类","text":"","link":"/categories/index.html"}],"posts":[{"title":"大数据介绍","text":"什么是大数据 数据单位 大数据的特征 大数据相关技术 大数据带来的变革 大数据小故事 其他故事 大数据初步学习路线 大数据处理流程 大数据处理模型 科普 什么是大数据$ 百度百科的定义,大数据（BIG DATA），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 在目前的业界尚未对大数据由清晰明确的定义, 它的第一次出现是在麦肯锡公司的报告中出现的, 在维基百科上的较为模糊的定义是很难运用软件的手段获取大量的内容信息, 对其处理后整理得出的数据集合。其他计算机学科的学者给出的定义是数据的尺度极为巨大, 常规的数据处理软件无法对数据识别、存储和应用的海量数据信息 维基百科的定义，大数据是指无法在可承受的时间范围内用常规软件工具进行捕捉、管理和处理的数据集合。 研究机构Gartner定义，“大数据”是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 数据单位$1MB = 1024KB、1GB = 1024MB1TB = 1024GB、1PB = 1024TB 大数据的特征$ 容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息； 种类（Variety）：数据类型的多样性； 速度（Velocity）：指获得数据的速度； 可变性（Variability）：妨碍了处理和有效地管理数据的过程。 真实性（Veracity）：数据的质量 复杂性（Complexity）：数据量巨大，来源多渠道 价值（value）：合理运用大数据，以低成本创造高价值 大数据相关技术$ 数据采集： OLAP(联机分析处理)和数据挖掘的基础。ETL工具负责将分布的、异构的数据源进行抽取，抽取到中间层，进行清洗、转换、集成，（不过对于负责的逻辑处理不会这么干，用Spark或者其他的进行处理），最后放到数据仓库中存储，如Hive 数据存取： 关系型数据库、NoSQL(Not Only SQL,泛指非关系型数据库)，SQL等 基础架构： 云存储、分布式文件存储等 数据处理： 自然语言处理(Natural Language Processing, NLP) 数据分析： 假设检验、显著性检验、差异检验、差异分析、相关性分析、T检验、方差分析、卡方分析、偏相关性分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、预测和残差分析、岭回归、Logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、判别分析、对应分析、快速聚类和聚类法、对应分析、多元对应分析等 数据挖掘： 分类（Classification）、估计（Estimation）、预测（Prediction)、相关性分析或关联规则（Association）、复杂数据类型挖掘（Text、Web、图形图像、视频、音频等） 模型预测： 预测模型、机器学习、建模仿真 结果呈现： 云计算、标签云、关系图等 大数据带来的变革$大数据小故事$ &emsp;&emsp;最早关于大数据的故事发生在美国第二大的超市塔吉特百货（Target）。孕妇对于零售商来说是个含金量很高的顾客群体。但是他们一般会去专门的孕妇商店而不是在Target购买孕期用品。人们一提起Target，往往想到的都是清洁用品、袜子和手纸之类的日常生活用品，却忽视了Target有孕妇需要的一切。那么Target有什么办法可以把这部分细分顾客从孕妇产品专卖店的手里截留下来呢？ &emsp;&emsp;为此，Target的市场营销人员求助于Target的顾客数据分析部的高级经理Andrew Pole，要求他建立一个模型，在孕妇第2个妊娠期就把她们给确认出来。在美国出生记录是公开的，等孩子出生了，新生儿母亲就会被铺天盖地的产品优惠广告包围，那时候Target再行动就晚了，因此必须赶在孕妇第2个妊娠期行动起来。如果Target能够赶在所有零售商之前知道哪位顾客怀孕了，市场营销部门就可以早早的给他们发出量身定制的孕妇优惠广告，早早圈定宝贵的顾客资源。 &emsp;&emsp;可是怀孕是很私密的信息，如何能够准确地判断哪位顾客怀孕了呢？Andrew Pole想到了Target有一个迎婴聚会（baby shower）的登记表。Andrew Pole开始对这些登记表里的顾客的消费数据进行建模分析，不久就发现了许多非常有用的数据模式。比如模型发现，许多孕妇在第2个妊娠期的开始会买许多大包装的无香味护手霜；在怀孕的最初20周大量购买补充钙、镁、锌的善存片之类的保健品。最后Andrew Pole选出了25种典型商品的消费数据构建了“怀孕预测指数”，通过这个指数，Target能够在很小的误差范围内预测到顾客的怀孕情况，因此Target就能早早地把孕妇优惠广告寄发给顾客。 &emsp;&emsp;那么，顾客收到这样的广告会不会吓坏了呢？Target很聪明地避免了这种情况，它把孕妇用品的优惠广告夹杂在其他一大堆与怀孕不相关的商品优惠广告当中，这样顾客就不知道Target知道她怀孕了。百密一疏的是，Target的这种优惠广告间接地令一个蒙在鼓里的父亲意外发现他高中生的女儿怀孕了，此事甚至被《纽约时报》报道了，结果Target大数据的巨大威力轰动了全美。 &emsp;&emsp;根据Andrew Pole的大数据模型,Target制订了全新的广告营销方案，结果Target的孕期用品销售呈现了爆炸性的增长。Andrew Pole的大数据分析技术从孕妇这个细分顾客群开始向其他各种细分客户群推广，从Andrew Pole加入Target的2002年到2010年间，Target的销售额从440亿美元增长到了670亿美元。 &emsp;&emsp;我们可以想象的是，许多孕妇在浑然不觉的情况下成了Target常年的忠实拥泵，许多孕妇产品专卖店也在浑然不知的情况下破产。浑然不觉的背景里，大数据正在推动一股强劲的商业革命暗涌，商家们早晚要面对的一个问题就是：究竟是在浑然不觉中崛起，还是在浑然不觉中灭亡 其他故事$ Google根据搜索关键字分析流感病毒H1N1 2008年金融危机 大数据初步学习路线$ 技术 工具 JAVA 面向对象的编程语言 Linux 类Unix操作系统 Hadoop生态圈 1、HDFS 解决存储问题存储极大数目的信息（terabytes or petabytes），将数据保存到大量的节点当中。支持很大单个文件。提供高可靠性，是指一个或多个节点故障，系统仍然可以继续工作提供数据快速访问 2、MapReduce 解决计算问题它有个特点就是不管多大的数据只要给它时间它就能把数据跑完，但是时间可能不是很快所以它叫数据的批处理 3、Yarn 资源调度器 4、ZooKeeper 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 5、Flume 数据采集工具 6、Hive 基于Hadoop的数据仓库工具 7、Hbase 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 8、Sqoop 数据传递工具，如将数据从关系型数据库导入Hive Scala 多范式编程语言、面向对象和函数式编程的特性 Spark 目前企业常用的批处理离线数据/实时计算引擎它是用来弥补基于MapReduce处理数据速度上的缺点，它很是流氓，直接将数据存在内存中 【注意】 MapReduce运行时也是需要将代码数据加载到内存中的，只不过Spark都是基于内存操作 Flink 目前最火的流式处理框架、既支持流处理、也支持批处理 Elasticsearch 大数据分布式弹性搜索引擎 大数据处理流程$ 大数据处理模型$按照数据的三状态定义 水库里一平如镜的水—&gt;静止数据(data at rest) 水处理系统中上下翻滚的水—&gt;正在使用的数据(data in use) 汹涌而来的新水流—&gt;动态的水(data in motion) “快”说的是两层面 “动态数据” 来得快 “正在使用的数据” 处理得快 批处理 MapReduce 流处理 Spark Streaming 科普$ 根据国际数据公司（IDC）的《数据宇宙》报告显示：2008年全球数量为0.5ZB，2010年为1.2ZB，人类正式进入ZB时代。更为惊人的是，2020年以前全球数据量仍将保持每年40%多的高速增长，大约每两年就翻一倍，这与IT界的摩尔定律极为相似，姑且称之为“大数据爆炸定律”。","link":"/post/2260.html"},{"title":"分布式存储管理","text":"为什么直接采用关系模型的分布式数据库并不能适应大数据时代的？ NoSQL 特点 为什么直接采用关系模型的分布式数据库并不能适应大数据时代的？$ 规模效应所带来的压力 传统数据库倾向于纵向扩展(Scale-Up)，即增加单台计算机的性能 适应大数据的数据库系统的应该具有良好的横向扩展(Scale-Out)，即为集群增加一台计算机 数据类型的多样化 传统数据类型： 结构化数据大数据时代的数据类型： 结构化数据 半结构化数据 非结构化数据 设计理念所带来的冲突 关系型数据库 One size fits all ,即面对不同问题不需要重新考虑数据管理问题 简单来说就是，单一模式可以适应所有变化 新理念 “One size fits one” 和 “One size fits domain” 数据库的事务特性 传统数据库ACID特性 A(Atom，原子性)，C(Consistency，一致性)，I(Isolation,隔离性)，D(Durability,持久性) 大数据时代的数据库BASE。 Basically Available(基本可用)，Soft State(柔性状态)，Eventually Consistency(最终一致性)根据分布式领域著名的CAP理论来看，ACID追求一致性C,而BASE更加关注A一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）新型数据库Spanner NoSQL 特点$ 模式自由（Schema-free） 支持简易备份（Easy Replication Support) 简单应用程序接口（Simple API） 最终一致性（或说支持BASE特性，不支持ACID特性） 支持海量数据（Huge Amount of Data）","link":"/post/4faba951.html"},{"title":"十大经典算法的优缺点","text":"KNN Apriori 决策树 CART EM （Expectation Maximization） 朴素贝叶斯 K-means PageRank KNN$优点： 理论成熟，实现简单 缺点： 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数 计算量较大 Apriori$优点： 适合稀疏数据集。 算法原理简单，易实现。 适合事务数据库的关联规则挖掘 缺点： 多次扫描事务数据库，需要很大的I/O负载 对每次k循环，侯选集Ck中的每个元素都必须通过扫描数据库一次来验证其是否加入Lk。假如有一个最大频繁项目集包含10个项的话，那么就至少需要扫描事务数据库10遍。 可能产生庞大的侯选集 由Lk-1产生k-侯选集Ck是指数增长的，例如104个1-频繁项目集就有可能产生接近107个元素的2-侯选集。如此大的侯选集对时间和主存空间都是一种挑战。 决策树$ID3 优点： 理论清晰，方法简单 学习能力强 ID3算法在搜索的每一步都使用当前的所有训练样例，大大降低了对个别训练样例错误的敏感性 缺点： 只对比较小的数据集有效，且对噪声比较敏感，当训练数据集加大时，决策树可能会随之改变。 ID3算法在搜索过程中不进行回溯。收敛到局部最优而不是全局最优 ID3算法只能处理离散值的属性。 信息增益度量存在一个内在偏置，它偏袒具有较多值的属性。如日期属性。 ID3算法增长树的每一个分支的深度，直到恰好能对训练样例完美地分类。当数据中有噪声或训练样例的数量太少时，产生的树会过渡拟合训练样例。 ID3的改进 C4.5 优点： 通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，避免出现过拟合的特性，提升决策树的泛化能力。 CART$优点： 可以处理连续值 缺点： EM （Expectation Maximization）$优点： 简单稳定 缺点： 在缺失数据较多的情形,收敛的速度较慢，次数多，容易陷入局部最优 对于某些情况下,要计算算法中的M步,即完成对似然函数的估计是非常困难的 在某些情况下是要获得EM算法中的E步的期望显式是非常困难或者不可能的 朴素贝叶斯$优点： 生成式模型，通过计算概率来进行分类，可以用来处理多分类问题， 对小规模的数据表现很好，适合多分类任务，适合增量式训练，算法也比较简单。 缺点： 对输入数据的表达形式很敏感， 由于朴素贝叶斯的“朴素”特点，所以会带来一些准确率上的损失。 需要计算先验概率，分类决策存在错误率。 K-means$优点： 可解释性比较强。 调参的参数仅为簇数k。 相对于高斯混合模型而言收敛速度快，因而常用于高斯混合模型的初始值选择。K-means 的时间复杂度为 O(N⋅K⋅I) ，簇数 K 和 迭代次数 I 通常远小于N，所以可优化为 O(N) ，效率较高。 缺点： 对离群点敏感。 K值难以事先选取，交叉验证不大适合 PageRank$优点： PageRank算法通过网页间的链接来评价网页的重要性，在一定程度上避免和减少了人为因素对排序结果的影响； 采用与查询无关的离线计算方式，使其具有较高的响应速度； 一个网页只能通过别的网页对其引用来增加自身的PR值，且算法的均分策略使得一个网页的引用越多，被引用网页所获得的PR值就越少。 因此，算法可以有效避免那些为了提高网站的搜索排名而故意使用链接的行为。 缺点： 主题漂移问题 PageRank 算法仅利用网络的链接结构，无法判断网页内容上的相似性；且算法根据向外链接平均分配权值使得主题不相关的网页获得与主题相关的网页同样的重视度，出现主题漂移。 偏重旧网页问题 决定网页 P R 值的主要因素是指向它的链接个数的多少。一个含有重要价值的新网页，可能因为链接数目的限制很难出现在搜索结果的前面，而不能获得与实际价值相符的排名。 算法并不一定能反映网页的重要性，存在偏重旧网页现象。 忽视用户个性化问题 PageRank算法在设计之初，没有考虑用户的个性化需要。个性化搜索引擎的兴起，对 PageRank排序算法提出新的挑战。 参考： 朴素贝叶斯 https://www.jianshu.com/p/6309e084ce64 K-means https://www.cnblogs.com/massquantity/p/9416109.html","link":"/post/85ed9b6d.html"},{"title":"Java Web过滤器和监听器","text":"过滤器 过滤器的使用步骤 Filter配置 监听器 使用 Java Web 三大基本组件（Servlet、Filter、Listener）Servlet： 处理请求Filter: 过滤拦截请求Listener：监听器 三大组件中基本都需要在web.xml中进行注册：除了Listener的两个（活化钝化监听器、绑定监听器）需要JavaBean实现不需要注册外 过滤器$过滤器的使用步骤$ 实现Filter接口 去web.xml注册 Filter配置$12345678&lt;filter&gt; &lt;filter-name&gt;MyFilter&lt;/filter-name&gt; &lt;filter-class&gt;&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;&lt;/filter-name&gt; &lt;url-pattern&gt;&lt;/url-pattern&gt;&lt;/filter-mapping&gt; url-pattern的三种写法$ 精确匹配 /pics/hello.jsp /hello/login: 直接拦截指定路径 路径匹配(模糊匹配) /pics/* ： 拦截pics下的所有请求 后缀匹配(模糊匹配) *.jsp:拦截所有以jsp结尾的请求 【注】 不能使用/pics/*.jsp Filter原理$1234doFilter(){ //放行请求 chain.doFilter(request, response);} 多个Filter的访问顺序 监听器$ 八个：ServletRequest(2)、HttpSession(4)、ServletContext(2)2:生命周期监听器、属性变化监听器4：HttpSession额外监听器，还有两个(活化钝化监听器、绑定监听器)掌握监听器：ServletContextListener(生命周期监听器)：监听Servlet的创建和销毁(监听服务器的启动停止): 服务器启动为当前项目创建ServletContext对象，服务器停止销毁创建的ServletServletContext 一个Web项目对应一个ServletContext，它代表当前web项目的信息 还可以作为最大的域对象在整个项目运行期间共享数据 使用$ 实现对应的监听接口 去web.xml中进行配置 注意：有两个Listener是JavaBean需要实现的接口（HttpSessionActivitionListener, HttpSeesionBindingListener）","link":"/post/4585d600.html"},{"title":"AJAX和JSON使用","text":"JSON 什么是JSON JSON格式 AJAX 原来的交互 现在的交互: ( XmlHttpRequest对象) 上面两种交互图解 XHR原生编程 JQuery-AJAX 国际化、文件上传与下载 国际化 两种方式进行国际化 JSON$什么是JSON$ JSON (JavaScript Object Notation)是(与xml对比)一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。它 基于JavaScript Programing Language, Standard ECMA -262 3rd Edition - December 1999 的一个子集。JSON 采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C, C++，C#,Java,JavaScript, Perl, Python 等)。这些特性 使JSON成为理想的数据交换语言。 JSON格式${key:value,key:value}value的类型 基本类型（字符串、数字、布尔值） 数组 {lastName:”李四”,books:[“西游记”,”红楼梦”,{}]} 对象 {} 123456{ \"lastname\":\"张三\", \"age\":19, \"car\":{\"pp\":\"宝马\",\"price\":\"10000$\"}, \"books\":[\"西游记\",\"红楼梦\"]} 浏览器处理方便，js解析方便JSON：js进行传输，(HTTP只能传输文本)js定义对象加不加双引号都可以JSON是利于传输的数据js中将对象转为JSON ： 1JSON.stringify(student) JSON转js对象： 1JSON.parse(str) AJAX$ AJAX ( Asynchronous Javascript And XML (景步JavaScrietXML) ) : AJAX：是一种无刚新页面与服务器的交互技术: (页面不剧新顿可以收到服务器响应的数据) 原来的交互$ 发送请求 服务器收到请求。调用对应的Servlet进行处理; servlet处理完感会有响应信息生成; 浏览器收到了服务器响应的数据，把之前的数据页面的清除，展示新的数据（效果是页面刷新） 现在的交互: ( XmlHttpRequest对象)$ XMLHttpRequest对象帮我们发送请求 服务器收到请求。调用对应的servlet进行处理; servlet处理完成会有响应信息成; XMLHttpRequest对象收数据;(浏览器就感受不到这个数据；xmlhttprequest对象收到这个数据) 上面两种交互图解$ XHR原生编程$ XMLHttpRequest对象 所有现代浏览器均支持XMLHttpRequest对象 xhr原生编程: 12345678910var xhr=new XMLHttpRequest0;//创建xhr对象;xhr.open(\"GET\",\"test1.txt\",true);//建立连接xhr.send() //发送数据//监听xhr的状态xhr.onreadystatechange=function()if (xhr.readyState==4 &amp;&amp; xhr.status= =200){ //获取数据 document.getElementByld(\"myDiv\").innerHTML =xhr.responseText;} 改变了我们传统的交互方式; 发请求; 服务器收到请求,处理请求经常要给页面携带数据。reguestsettribute(“map”map);转发到页面 浏览器收到页面数据,在页面使用e!表达式获取数据;导致页面整个刷新;造成很大的服务器负担; 只让服务器返回我们需要的部分数据即可;不用返回整个页面; xhr营代浏览器来接受响应发送请求;利用dom增删改的方式来改变页面效果; 异步：不会堵塞服务器同步：浏览器在接收请求后继续执行，也就是会堵塞到数据返回 什么是ajax :xhr对象向服务器发送请求,并收到响应数据,利用dom增删改的方式改变页面效果 JQuery-AJAX$ $.get()123456789101112131415 // $get(url,[data],[callback],[type])函数说明// data 传递的数据，可以是k=b&amp;k=v 也可以是js对象// callback ，定义一个回调函数，随便定义一个参数，这个参数就封装了服务器的返回数据// type:返回内容格式，xml, html, script, json, text, _default。 //type指定为json，jquery自动转换为json$.get(\"${ctp}/getinfo\",\"key=value\",function(d){ alert(d);}) //点击事件如不想更新整个页面，就直接在点击后function中返回false $(\"#btn\").click(function(){ return false }) 服务器可以使用gson工具生成json字符串给浏览器123456789 //使用前需要导包，包下载路径 //https://lyhcc.github.io/resourses/files/gson-2.2.4.jarMap&lt;String, Object&gt;map = new HashMap&lt;&gt;();map.put(\"lastname\", \"admin\");map.put(\"age\", 10);//解析Gson gson = new Gson();response.getWriter().write(gson.toJson(map)); $.post() $.ajax(默认异步请求) 123456789101112$.ajax({type: \"POST\",async: false,url: \"getinfo\",data: \"name=John&amp;location=Boston\",success: function(msg){ alert( \"Data Saved: \" + msg );},error: function(xhr, textStatus){ //xhr XMLHttpRequest}}); 国际化、文件上传与下载$国际化$ 国际化：根据Locale代表的区域信息可以进行国际化 得到需要的国际化区域信息：locale cn = Locale.CHINA 需要使用ResourceBundle绑定写好的国际化资源文件(基础名_语言代码_国家代码.properties) 1ResourceBundle bundle = ResoourceBundle.getBundle(&quot;bookstore&quot;, cn) 从bundle中获取配置文件中的值 1String ussername = bundle.getString(&quot;username&quot;); 更多的国际化功能 12345678java.lang.Objectjava.text.FormatAll Implemented Interfaces: Serializable, CloneableDirect Known Subclasses: DateFormat MessageFormat, NumberFormat Q 两种方式进行国际化$ 根据浏览器的请求头带来的信息国际化页面 12345 Locale locale = request.getLocale(); ``2. 点击超链接切换国际化 - 超链接上戴上区域信息；Locale就根据带上的区域信息来new - 推荐国际化取值，格式化日期 &lt;fmt:message key=&quot;k&quot;&gt; 1234### 文件上传下载#### 上传1. 上传准备： &lt;form method=&quot;post&quot; entype=&quot;multipart/from-data&quot;&gt; 123452. 文件上传请求体，多部件形式3. 需要导包处理#### 下载&gt;把文件交给浏览器，一定告诉浏览器，这个流不要打开请下载 response.setHeader(“Content-Disposition”, “attachment;filename=img.png”) 要求$ Jquery(精通) XML了解 Tomcat(掌握) Servlet(掌握思想) JSP/JSTL/EL(熟悉) HTML/CSS/JS(掌握) Session、Cookie(掌握) Filter(掌握) Listener(掌握ServletContext) AJAX和JSON(精通) 国际化和文件上传(了解)","link":"/post/3450b53b.html"},{"title":"Tomcat 的安装配置","text":"Tomcat下载 继续下一步安装即可 环境变量配置 新建JavaWeb工程遇到的问题 Tomcat$继续下一步安装即可$ 在浏览器输入localhost:8080查看是否安装成功，当然8080端口是默认端口，更改后的使用更改后的端口。 环境变量配置$123456789新建两个变量名：CATALINA_HOME变量值：D:\\ApacheTomcat\\bin变量名：CATALINA_BASE变量值：D:\\ApacheTomcat\\bin在PATH中添加%CATALINA_HOME%\\bin 详细参考 新建JavaWeb工程遇到的问题$ 新建jsp文件后报错 原因是没有导入Tomcat包 在buildpath-&gt;add Libarary-&gt;server runtime找不到Tomcat 在preference中找Server 再选runtime environment选择Tomcat安装路径后再回到第一步，就可以解决这个问题","link":"/post/9f166c45.html"},{"title":"Shell文本三剑客之awk","text":"简介 工作模式 语法格式 awk的内置变量 printf输出 模式匹配 动作表达式 awk条件语句 awk循环语句 awk字符函数 awk常用选项 awk数组与Shell数组 简介$ awk是一个文本处理工具,通常用于处理数据并生成结果报告awk的命名是它的创始人Alfred Aho、Peter Weinberger和Brian Kernighan姓氏的首个字母组成的 工作模式$ 类似sed，首先读取第一行，处理完后处理下一行，它可以在处理前面加一些数据，处理后面也可以加一些数据 语法格式$ 第一种形式:awk ‘BEGIN{}pattern{commands}END{}’ file_name BEGIN: 正式处理之前执行pattern: 匹配模式commands: 对匹配出来的数据的处理命令END: 处理完所有匹配数据后执行file_name: 文件名 第二种形式: standard output |awk ‘BEGIN{}pattern{commands}END{}’ standard output表示对标准输出数据进行处理其他同上 awk的内置变量$ 内置变量 含义 $0 整行内容 $1-$n 当前行的第1-n个字段 NF 当前行的字段个数，也就是有多少列 NR 当前行的行号，从1开始计数 FNR 多文件处理时,每个文件行号单独计数,都是从0开始 FS 输入字段分隔符。不指定默认以空格或tab键分割 RS 输入行分隔符。默认回车换行\\n OFS 输出字段分隔符。默认为空格 ORS 输出行分隔符。默认为回车换行 FIL ENAME 当前输入的文件名字 ARGC 命令行参数个数 ARGV 命令行参数数组 123456789101112131415161718192021222324251. /etc/passwd 的所有内容显示 awk '{print $0}' /etc/passwd2. 指定分隔符分割后的一列，/etc/passwd awk 'BEGIN{FS=\":\"}{print $7}' /etc/passwd3. 输出当前行某个文件分割后的数据的个数,NF(Number Field) awk 'BEGIN{FS=\":\"}{print NF}' /etc/passwd4. NR(Number Row),处理行号 awk 'BEGIN{FS=\":\"}{print NR}' /etc/passwd 当有多个文件时，直接加，如果是FNR(File Number Row)就会每个文件的行号从1开始5. FS(Field Separator) 字段分隔符，不指定是以空格或tab键分割 awk 'BEGIN{FS=\":\"}{print $7}' /etc/passwd6. RS(Row Separator) 行分割符，默认换行符 awk 'BEGIN{RS=\"---\"}{print $0}' rs.txt # rs.txt 内容aaaaaaaaaaa---bbbbbbbbbbbbbbbb---cccccccccccccc7. OFS(Output Field Separator) 字段输出分隔符，默认空格 awk 'BEGIN{FS=\":\";OFS=\"##\"}{print $1,$2}' /etc/passwd # 如果$1 $2不以逗号分割OFS不起作用8. ORS(Output Row Separator) 输出行分隔符，默认换行 awk 'BEGIN{ORS=\"-_+\";FS=\":\"}{print $1,$2}' /etc/passwd9. FILENAME 文件名 awk '{print FILENAME}' /etc/passwd # 多少行数据，输出多少个10. ARGC 变量个数 awk '{print ARGC}' /etc/passwd # 2个参数， awk和/etc/passwd printf输出$ 格式符 含义 %s 打印字符串 %d 打印十进制数 %f 打印一一个浮点数 %X 打印十六进制数 %o 打印八进制数 %e 打印数字的科学计数法形式 %c 打印单个字符的ASCII码 修饰符 含义 - 左对齐 + 右对齐 # 显示8进制在前面加0 ,显示16进制在前面加0x 123456789101112131415161718192021格式符示例: 1、以字符串格式打印/etc/passwd中的第7个字段，以\":\"作 为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%s\\n\",$7}' /etc/passwd 2、以10进制格式打印/etc/passwd中的第3个字段，以\":\"作 为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%d\\n\",$3}' /etc/passwd 3、以浮点数格式打印/etc/passwd中的第3个字段，以\":\"作为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%f\\n\",$3}' /etc/passwd 4、以16进制数格式打印/etc/passwd中的第3个字段，以\":\"作 为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%x\\n\",$3}' /etc/passwd 5、以8进制数格式打印/etc/passwd中的第3个字段，以\":\"作为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%o\\n\",$3}' /etc/passwd 6、以科学计数法格式打印/etc/passwd中的第3个字段，以\":\"作为分隔符 awk 'BEGIN{FS=\":\"}{printf \"%e\\n\",$3}' /etc/passwd修饰符示例: 1、左对齐格式 awk 'BEGIN{FS=\":\"}{printf \"%-20s %-20s\\n\",$1,$7}' /etc/passwd 2、右对齐格式 awk 'BEGIN{FS=\":\"}{printf \"%20s %20s\\n\",$1,$7}' /etc/passwd 3、打印8进制或16进制数字是在前面加# awk 'BEGIN{FS=\":\"}{printf \"%#o\\n\",$3}' /etc/passwd awk 'BEGIN{FS=\":\"}{printf \"%#x\\n\",$3}' /etc/passwd 模式匹配$ RegExp(表示可以是一个敞亮字符串或者是一个正则表达式写法) 1234匹配/etc/passwd文件行中含有root字符串的所有行 awk 'BEGIN{}/^root/{print $0}' /etc/passwd匹配/etc/passwd文件行中以yarn开头的所有行 awk 'BEGIN{}/^yarn/{print $0}' /etc/passwd 关系运算匹配 123456789101112131415161718192021222324252627282930关系运算符匹配:&lt; 小于 &gt; 大于 &lt;= 小于等于 &gt;= 大于等于== 等于 != 不等于 ~ 匹配正则表达式 !~ 不匹配正则表达式 不匹配正则表达式(1)、以:为分隔符， 匹配/etc/passwd文件中第3个字段小于50的所有行信息 awk 'BEGIN{FS=\":\"}$3&lt;50' /etc/passwd(2)、以:为分隔符，匹配/etc/passwd文件中 第3个字段大于50的所有行信息 awk 'BEGIN{FS=\":\"}$3&gt;50' /etc/passwd(3)、以:为分隔符，匹配/etc/passwd文 件中第7个字段为/bin/bash的所有行信息 awk 'BEGIN{FS=\":\"}$7==\"\\/bin\\/bash\"' /etc/passwd(4)、以:为分隔符，匹配/etc/passwd文件中 第7个字段不为/bin/bash的所有行信息 awk 'BEGIN{FS=\":\"}$7==\"/bin/bash\"' /etc/passwd(5)、以:为分隔符，匹配/etc/passwd中第3个字段包含3个以上数字的所有行 awk 'BEGIN{FS=\":\"}$3~/[0,9]{3,}/' /etc/passwd布尔运算符匹配:|| 或&amp;&amp; 与! 非(1)、以:为分隔符，匹配/etc/passwd文 件中包含root或mysql的所有行信息 awk 'BEGIN{FS=\":\"}$1==\"root\" || $1==\"mysql\"' /etc/passwd(2)、以:为分隔符，匹配/etc/passwd文件中第3个字段小于50并且第4个字段大于50的所有行信息 awk 'BEGIN{FS=\":\"}$3&lt;50 &amp;&amp; $4&gt;50' /etc/passwd 动作表达式$ 运算符 含义 + 加 - 减 * 乘 / 除 % 模 ^或** 乘方 ++x 在返回x变量之前, x变量加1 x++ 在返回变量之后，x变量加1 12345678910111213141、使用awk计算/etc/services中的空白行数量) # 空白行的表示^$ awk '/^$/{sum++}END{print sum}' /etc/services2、计算学生课程分数平均值，学生课程文件内容如下: Allen 80 90 96 98 Mike 93 98 92 91 Zhang 78 76 87 92 Jerry 86 89 68 92 Han 85 95 75 90 Li 78 88 98 100 awk '{sum=$2+$3+$4+$5; AVG=sum/4; printf \"%-8s%-8d%-8d%-8d%-8d%0.2f\\n\",$1,$2,$3,$4,$5,AVG}' stu.txt 3、使用awk输出/etc/passwd文件的行数。分两种方式显示行数，一种是正序如1.2.3.4.. awk '{line++; printf \"%d:%s\\n\", line,$0}' /etc/passwd awk条件语句$1234567891011121、以:为分隔符，只打印/etc/passwd中第3个字段的数值在50-100范围内的行信息 awk 'BEGIN{FS=\":\"}{if(50&lt;$3 &amp;&amp; $3&lt;100) print $0}' /etc/passwd2、计算下列每个同学的平均分数，并且只打印平均分数大于90的同学姓名和分数信息 Allen 80 90 96 98 Mike 93 98 92 91 Zhang 78 76 87 92 Jerry 86 89 68 92 Han 85 95 75 90 Li 78 88 98 100 awk '{sum=$2+$3+$4+$5; AVG=sum/4; if(AVG&gt;90)printf \"%-8s%-8d%-8d%-8d%-8d%0.2f\\n\",$1,$2,$3,$4,$5,AVG}' stu.txt awk循环语句$1234# 三种循环awk 'BEGIN {i = 1; while (i &lt; 6) { print i; ++i } }'awk 'BEGIN { for (i = 1; i &lt;= 5; ++i) print i }'awk 'BEGIN {i = 1; do { print i; ++i } while (i &lt; 6) }' awk字符函数$ 函数名 解释 函数返回值 length(str) 计算字符串长度 整数长度值 index(str1 ,str2) 在str1中查找str2的位置 返回值为位置索引,从1计数 tolower(str) 转换为小写 转换后的小写字符串 toupper(str) 转换为大写 转换后的大写字符串 substr(str,m,n) 从str的m个字符开始，截取n位 截取后的子串 split(str,arr,fs) 按fs切割字符串,结果保存arr 切割后的子串的个数 match(str,RE) 在str中按照RE查找，返回位置 返回索引位置 sub(RE,RepStr,str) 在str中搜索符合RE的字串,将其替换为RepStr ;只替换第一个 替换的个数 gsub(RE,RepStr,str) 在str中搜索符合RE的字串,将其替换为RepStr;替换所有 替换的个数 12345678910111213141516171819202122232425262728291、以:为分隔符，返回/etc/passwd中每行中每个字段的长度 awk 'BEGIN{ FS=\":\" }{ i=0; while(i&lt;=NF){ printf \"%d\", length($i); if(i!=NF)printf \":\"; i++; } printf \"\\n\" }' /etc/passwd2、搜索字符串\"I have a dream\"中出现\"ea\"字符串的位置 awk 'BEGIN{str=\"I have a dream\"; print index(str,\"ea\")}'3、将字符串\"Hadoop is a bigdata Framawork\"全部转换为小写 awk 'BEGIN{str=\"Hadoop is a bigdata Framawork\"; print tolower(str)}'4、将字符串\"Hadoop is a bigdata Framawork\" 全部转换为大写 awk 'BEGIN{str=\"Hadoop is a bigdata Framawork\"; print toupper(str)}'5、将字符串\"Hadoop Kafka Spark Storm HDFS YARN Zookeeper\"， 按照空格为分隔符 # 从1开始 awk 'BEGIN{str=\"Hadoop Kafka Spark Storm HDFS YARN Zookeeper\"; split(str, arr, \" \"); print arr[1]}'6、搜索字符串\"Tranction 2345 Start:Select * from master\"第一个数字出现的位置 awk 'BEGIN{str=\"Tranction 2345 Start:Select * from master\"; print match(str, /[0-9]+/)}'7、截取字符串\" transaction start\"的子串，截取条件从第4个字符开始，截取5位 awk 'BEGIN{str=\" transaction start\"; print substr(str, 4,5)}'8、替换字符串\"Tranction 243 start, Event ID:9002\"中第一个匹配到的数字串为$符号 awk 'BEGIN{str=\"Tranction 243 start, Event ID:9002\"; sub(/[0-9]+/, \"$\", str); print str}' awk常用选项$ 选项 解释 -v 参数传递 -f 指定脚本文件 -F 指定分隔符 -V 查看awk的版本号 1234567891011121314151617181920212223# -v[root@master ~]# num1=100[root@master ~]# var=\"You\"[root@master ~]# awk -v num2=$num1 -v var1=$var 'BEGIN{print num2, var1}'100 You# 字符串有空格需要引号[root@master ~]# var=\"Hello Shell\"[root@master ~]# awk -v num2=$num1 -v var1=$var 'BEGIN{print num2, var1}'awk: fatal: cannot open file `BEGIN{print num2, var1}' for reading (No such file or directory)[root@master ~]# awk -v num2=$num1 -v var1=\"$var\" 'BEGIN{print num2, var1}'100 Hello Shell# -f awk后面的BEGIN放入test.awk后执行[root@master datas]# awk -v num2=$num1 -v var1=\"$var\" -f test.awk 100 Hello Shell# -Fawk -F : 'BEGIN{}{print $1}' /etc/passwd# -V[root@master ~]# awk -VGNU Awk 4.0.2 awk数组与Shell数组$awk数组$Shell数组$123456789101112131415array=(\"If\" \"we\" \"can\" \"only\" \"encounter\" \"each\" \"other\" \"rather\" \"than\" \"stay\" \"with\" \"each\" \"other,then\" \"I\" \"wish\" \"we\" \"had never encountered\")打印元素: echo ${array[2] } 打印元素个数: echo ${#array[@] } 打印元素长度: echo ${#array[3] } 给元素赋值: array[3]=\"Li\" 删除元素: unset array[2] ;unset array 分片访问: echo ${array[@] :1:3} 元素内容替换: ${array[@]/e/E}只替换第-一个e;${array[@]//e/E}替换所有的e 数组的遍历：for a in arraydo echo $adone","link":"/post/209b93b1.html"},{"title":"find、which、locate、whereis总结","text":"locate命令介绍 whereis命令 which 命令 总结 locate命令介绍$ 文件查找命令，所属软件包mlocate 不同于find命令是在整块磁盘中搜索, locate命令在数据库文件中查找 find是默认全部匹配, locate则是默认部分匹配 updatedb命令更新 对应数据库文件，就可以通过locate命令查找，默认定时执行 用户更新/var/lib/mlocate/mlocate.db 所使用配置文件/etc/updatedb.conf，如排除某些目录 该命令在后台cron计划任务中定期执行 whereis命令$ 选项 含义 -b 只返回二进制文件 -m 只返回帮助文档文件 -S 只返回源代码文件 which 命令$ 作用： 仅查找二进制文件 选项 含义 -b 只返回二进制文件 总结$ 命令 适用场景 优缺点 find 查找某一类文件,比如文件名部分一致 功能强大，速度慢 locate 只能查找单个文件 功能单一, 速度快 whereis 查找程序的可执行文件、帮助文档等 不常用 which 只查找程序的可执行文件 常用于查找程序的绝对路径","link":"/post/f7db85fb.html"},{"title":"Shell 之Bash数学运算","text":"expr 求和案例 bc ## expr **语法格式** expr 操作符对照表 操作符 含义 num1 | num2 num1不为空且非0 ,返回num1 ;否则返回num2 num1 &amp; num2 num1不为空且非0，返回num1 ;否则返回0 hum1 &lt; num2 num1小于num2 ,返回1 ;否则返回0 num1 &lt;= num2 num1小于等于num2 ,返回1 ;否则返回0 num1 = num2 num1等于num2 ,返回1 ;否则返回0 num1 != num2 num1不等于num2 ,返回1 ;否则返回0 num1 &gt; num2 num1大于num2 ,返回1 ;否则返回0 num1 &gt;= num2 num1大于等于num2 ,返回1 ;否则返回0 num1 + num2 求和 num1 - num2 求差 num1 * num2 求积 num1 / num2 求商 num1 % num2 求余 =号判断使用$(($num1 = $num2))会出错，但使用==时可以运行成功时 echo $?返回0 12345678910111213141516num1=20num2=100expr $num1 \\| $num2expr $num1 \\&amp; $num2expr $numl \\&lt; $num2expr $num1 \\&lt; $num2expr $num1 \\&lt;= $num2expr $num1 \\&gt; $num2expr $num1 \\&gt;= $num2expr $num1 = $num2expr $num1 != $num2expr $num1 + $num2expr $num1 - $num2expr $num1 \\* $num2expr $num1 / $num2expr $num1 % $num2 求和案例$12345678910111213141516171819202122#!/bin/bashwhile truedo read -p \"Please input a postive number: \" num # 不需要expr $num + 1的返回值 expr $num + 1 &amp;&gt; /dev/null #判断是不是整形 if [ $? -eq 0 ];then if [ `expr $num \\&gt; 0` -eq 1 ];then for((i=1;i&lt;=$num;i++)) do sum=`expr $sum + $i` done echo \"The ans is $sum\" exit fi fi echo \"illegal input\" continuedone bc$ bc是bash内建的运算器, 支持浮点数运算 内建变量scale可以设置,默认为0 bc操作符对照表 操作符 含义 num1 + num2 求和 num1 - num2 求差 num1 * num2 求积 num1 / num2 求商 num1 % num2 求余 案例 12345[root@master datas]# echo &quot;23+35&quot; | bc58[root@master datas]# echo &quot;scale=5;23/35&quot; | bc.65714","link":"/post/7b00cacb.html"},{"title":"Shell 函数的定义和使用","text":"函数的介绍 语法格式 如何使用函数 案例–MySQL守护进程 函数的传递参数 Shell函数传参 Shell 函数调用 函数返回值 返回值方式 使用return返回值 使用echo返回值 函数的介绍$ Linux Shell中的函数和大多数编程语言中的函数一样 将相似的任务或代码封装到函数中,供其他地方调用 语法格式$ 格式1 格式2 1234567function name{ command1 command2 ... conmandn} 如何使用函数$ 直接使用函数名调用,可以将其想象成Shell中的一条命令 函数内部可以直接使用参数$1、$…$n. 调用函数: function_ name $1 $2 案例–MySQL守护进程$ 需求描述：写一个监控mysql的桥本，如果mysql服务宕掉，则该脚本可以检测到并将进程启动,(守护进程) 123456789101112131415161718192021#!/bin/bash# 获取进程号this_pid=$$while truedo ps -ef | grep mysqld | grep -v grep | grep -v $this_pid &amp;&gt; /dev/null # 判断进程是否存在 if [ $? -eq 0 ];then echo \"Mysql is runing well\" else systemctl start mysqld echo \"Mysql is down, start it...\" fidone 函数的传递参数$高级语言函数的定义 1234567int example_1 (int arg1,int arg2){ arg1 = arg2 ........ ........ return null} 高级语言函数的调用 Shell函数传参$12345function name{ echo \"Hello $1\" echo \"Hello $2\"} Shell 函数调用$123456name Lily lyhcc[root@master datas]# function greeting { echo \"Hello $1, $2\"; }[root@master datas]# greeting Allen lyhccHello Allen, lyhcc 简单计算器 123456789101112131415161718192021222324function cal{ echo \"$2\" case $2 in +) echo $(($1 + $3)) ;; -) echo $(($1 - $3)) ;; \\*) echo \"$1 $2 $3\" echo \"`expr $1 \\* $3`\" ;; /) echo $(($1 / $3)) ;; esac}cal 2 \\* 3cal 2 + 3 注意\\t 函数返回值$返回值方式$ 方法 格式 方法一 return 方法二 echo 使用return返回值$ 使用return返回值，只能返回1-255的整数 函数使用return返回值, 通常只是用来供其他地方调用获取状态,因此通常仅返回0或1 ; 0表示成功, 1表示失败 案例–判断MySQL是否在运行 1234567891011121314151617181920212223242526272829#!/bin/bashthis_pid=$$function mysql_isRunning{ ps -ef | grep mysqld | grep -v grep | grep -v $this_pid &amp;&gt; /dev/null if [ $? -eq 0 ];then return else return 1 fi}mysql_isRunning &amp;&amp; echo \"Mysql Server is running\" || echo \"Mysql Server is stoped\"执行过程[root@master datas]# sh -x mysqld_check.sh + this_pid=14205+ mysql_isRunning+ grep -v 14205+ ps -ef+ grep mysqld+ grep -v grep+ '[' 0 -eq 0 ']'+ return+ echo 'Mysql Server is running'Mysql Server is running 使用echo返回值$ 使用echo可以返回任何字符串结果 通常用于返回数据,比如一个字符串值或者列表值 案例–获取系统用户名 12345678#!/bin/bashfunction get_users{ users=`cat /etc/passwd | cut -d \":\" -f 1` echo $users}get_users","link":"/post/dc0c80f8.html"},{"title":"Shell全局变量和局部变量&函数库","text":"全局变量 局部变量 函数库 为什么要定义函数库 案例–定义函数库 经验之谈 全局变量$ 不做特殊声明, Shell中变量都是全局变量 Tips: 大型脚本程序中函数中慎用全局变量 局部变量$ 定义变量时,使用local关键字 函数内和外若存在同名变量,则函数内部变量覆盖外部变量 函数库$为什么要定义函数库$ 经常使用的重复代码封装成函数文件 一般不直接执行,而是由其他脚本调用 案例–定义函数库$ 定义一个函数库，该函数库实现以下几个函数: 加法函数add&emsp;add 12 89 减法函数reduce&emsp;reduce 80 20 乘法函数multip1e 除法函数divide 打印系统运行情况的函数sys_load，该函数可以显示内存运行情况,磁盘使用情况 函数库文件 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bash#function add{ echo \"`expr $1 + $2`\"}function reduce{ echo \"`expr $1 - $2`\"}function multiple{ echo \"`expr $1 \\* $2`\"}function divide{ echo \"`expr $1 / $2`\"}function sys_load{ echo \"Memory Info:\" echo \"`free -m`\" echo echo \"Disk Capacity:\" echo df -u echo } shell脚本 12345678#!/bin/bash#. /opt/datas/lib/base_functionadd 2 3reduce 3 4multiple 3 6divide 34 2 经验之谈$ 库文件名的后缀是任意的,但一般使用.lib 库文件通常没有可执行选项 库文件无需和脚本在同级目录,只需在脚本中引用时指定 第一行一般使用#!/bin/echo ,输出警告信息,避免用户执行","link":"/post/e8089af9.html"},{"title":"Shell命令的替换","text":"1 方法一 方法二 语法格式 `command` $(command) 例子 例子1:获取系统得所有用户并输出 123456[root@master datas]# cat /etc/passwdroot:x:0:0:root:/root:/bin/bash# cut 切分 -d 指定分隔符 -f 取段[root@master datas]# cat /etc/passwd | cut -d \":\" -f 1root 123456789#!/bin/shind=1for user in `cat /etc/passwd | cut -d \":\" -f 1`do echo \"This is $ind user: $user\" ind=$(($ind + 1))done 例子2:根据系统时间计算今年或明年 123456789[root@master datas]# date Sat Dec 7 00:01:44 CST 2019[root@master datas]# date +%Y2019[root@master datas]# echo &quot;This is $(date +%Y) year&quot;This is 2019 year[root@master datas]# echo &quot;This is $(($(date +%Y) + 1)) year&quot;This is 2020 year 例子3:根据系统时间获取今年还剩下多少星期，已经过了多少星期12345[root@slave2 datasets]# echo \"This yaer have passed $(date +%j) days\"This yaer have passed 341 days[root@slave2 datasets]# echo \"This yaer have passed $(($(date +%j)/7)) weeks\"This yaer have passed 48 weeks 例子4:判定nginx进程是否存在，若不存在则自动拉起该进程 说明：-v grep 去掉grep本身进程wc -l 统计结果行数V 1ps -ef | grep nginx -v grep |wc -l 1234567#!/bin/bashmysql_process_num=$(ps -ef | grep mysql | grep -v grep | wc -l)if [ $mysql_process_num -eq 0 ]; then systemctl start mysqldfi 总结: ``和$()两者是等价的，但推荐初学者使用$()，易于掌握;缺点是极少数UNIX可能不支持$(())主要用来进行整数运算，包括加减乘除，引用变量前面可以加$，也可以不加$","link":"/post/7297f3f0.html"},{"title":"Shell变量的替换和测试","text":"变量的替换 变量的测试 变量的替换$ 语法 说明 ${变量名#匹配规则} 从变量 开头 进行规则匹配，将符合 最短 的数据删除 ${变量名##匹配规则} 从变量开头进行规则匹配,将符合最长的数据删除 ${变量名%匹配规则} 从变量尾部进行规则匹配,将符合最短的数据删除 ${变量名%%匹配规则} 从变量尾部进行规则匹配,将符合最长的数据删除 ${变量名旧字符串/新字符串} 变量内容符合旧字符串则,则第一个旧字符串会被新字符串取代 ${变量名旧字符串/新字符串} 变量内容符合旧字符串则，则全部的旧字符串会被新字符串取代 案例 123456789101112131415161718192021222324252627[root@master opt]# var=&quot;Hello! I am Kiki. What&apos;s your name?&quot;[root@master opt]# echo $varHello! I am Kiki. What&apos;s your name?[root@master opt]# var1=${var#*am}[root@master opt]# echo $var1Kiki. What&apos;s your name?[root@master opt]# var2=${var##*am}[root@master opt]# echo $var2e?[root@master opt]# var3=${var%am*}[root@master opt]# echo $var3Hello! I am Kiki. What&apos;s your n[root@master opt]# var4=${var%%am*}[root@master opt]# echo $var4Hello! I[root@master opt]# var5=${var/I am/He is}[root@master opt]# echo $var5Hello! He is Kiki. What&apos;s your name?[root@master opt]# var6=${var//am/ma}[root@master opt]# echo $var6Hello! I ma Kiki. What&apos;s your nmae? 变量的测试$","link":"/post/31fca8c2.html"},{"title":"Shell的字符串处理","text":"计算字符串的长度 获取子串在字符串中的索引位置 计算子串长度 抽取子串 字符串处理小案例 计算字符串的长度$ a 语法 说明 方法1 ${ #string} 无 方法2 expr length “$string” string有空格必须加双引号 123456789[root@master opt]# var=&quot;Hello World&quot;[root@master opt]# len=${#var}[root@master opt]# echo ${len}11[root@master opt]# len=`expr length &quot;${var}&quot;`[root@master opt]# echo ${len}11 获取子串在字符串中的索引位置$语法 1234567891011121314151617expr index $string $substring# 例：[root@master opt]# ind=`expr index &quot;${var}&quot; start`[root@master opt]# echo $ind6# 说明：不是一个字符串的子串的索引，而是切分字符后查找，先找第一个字符，找不到再找第二个，以此类推[root@master opt]# ind=`expr index &quot;${var}&quot; abc`[root@master opt]# echo $ind4# 按照以上的方法仍然找不到，返回0[root@master opt]# ind=`expr index &quot;${var}&quot; z`[root@master opt]# echo $ind0 计算子串长度$语法 123456789101112expr match $string substr例：[root@master opt]# var=&quot;quickstart is app&quot;[root@master opt]# sub_len=`expr match &quot;${var}&quot; app`[root@master opt]# echo $sub_len0[root@master opt]# sub_len=`expr match &quot;${var}&quot; quic`[root@master opt]# echo $sub_len4说明：必须从头开始匹配 抽取子串$ a 语法 说明 方法一 ${string:position} 从string中的position开始 方法二 ${string:position:length} 从position开始，匹配长度为length 方法三 ${string: -position} 从右边开始匹配 方法四 ${string:(position)} 从左边开始匹配 方法五 expr substr $string $position $length 从position开始,匹配长度为length 123456789101112[root@master opt]# var=&quot;Hadoop Kafka Zookeeper HBase&quot;[root@master opt]# echo ${var:10}ka Zookeeper HBase[root@master opt]# echo ${var:10:2}ka[root@master opt]# echo ${var:(-1)}e[root@master opt]# echo ${var:(0)}Hadoop Kafka Zookeeper HBase[root@master opt]# echo `expr substr &quot;${var}&quot; 3 4`doop 字符串处理小案例$需求描述: 变量string=”Bigdata process framework is Hadoop , Hadoop is an open source project”执行脚本后，打印输出string字符串变量，并给出用户以下选项: 打印string长度 删除字符串中所有的Hadoop 替换第一个Hadoop为Mapreduce 替换全部Hadoop为Mapreduce用户输入数字1121314，可以执行对应项的功能;输入q1Q则退出交互模式 思路： 将不同功能模块划分，并编写函数12345function print_tipsfunction len_of_stringfunction del_hadoopfunction rep_hadoop_mapreduce_firstfunction rep_hadoop_mapreduce_all 实现以上所定义的功能函数 程序主流程设计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!/bin/shstring=\"Bigdata process framework is Hadoop , Hadoop is an open source project\"function print_tips{ echo \"****************\" echo \"1. 打印string长度\" echo \"2. 删除字符串中所有的hadoop\" echo \"3. 替换第一个Hadoop为MapReduce\" echo \"4. 替换全部Hadoop为MapReduce\" echo \"****************\"}function len_of_string{ echo \"${#string}\"}function del_hadoop{ echo \"${string//Hadoop/}\"}function rep_hadoop_mapreduce_first{ echo \"${string/Hadoop/MapReduce}\"}function rep_hadoop_mapreduce_all{ echo \"${string//Hadoop/MapReduce}\"}while truedo echo \"string=$string\" echo print_tips read -p \"Please input your choice(1|2|3|4|q|Q): \" op case $op in 1) len_of_string ;; 2) del_hadoop ;; 3) rep_hadoop_mapreduce_first ;; q|Q) rep_hadoop_mapreduce_all exit ;; *) echo \"Error, input on;y in {1|2|3|4|5|q|Q}\" ;; esacdone","link":"/post/8d5a03cf.html"},{"title":"Spring初始之HelloWorld","text":"概述 框架编写流程 总结 概述$ 通过各种方式给容器中注册对象以前是自己new对象，现在交给容器创建；给容器中注册主键组件 框架编写流程$ 导包 12spring-beans-4.0.0.RELEASE.jar、spring-context-4.0.0.RELEASE.jar、spring-core-4.0.0.RELEASE.jar、spring-expression-4.0.0.RELEASE.jarSpring 运行时依赖的日志包 commons-logging-1.1.3.jar 写配置文件 Spring 配置文件中，集合了spring对的ioc容器管理的所有组件（会员清单） 创建一个Spring Bean Configuration File(Spring bean配置文件) 1234567891011121314151617181920212223 创建一个Persion类，属性有LastName Name gender email，生成getter/setter方法 在配置文件中创建一个Person对象，Spring会自动创建Person对象 一个bean标签创建一个组件 &lt;!-- 一个bean标签可以注册一个组件（对象、类） class 写要注册的组件的全类名 id 这个对象的唯一标识 --&gt;&lt;bean id=\"person01\" class=\"xyz.lyhcc.bean.Person\"&gt; &lt;property name=\"lastName\" value=\"李四\"&gt;&lt;/property&gt; &lt;property name=\"name\" value=\"王五\"&gt;&lt;/property&gt; &lt;property name=\"gender\" value=\"男\"&gt;&lt;/property&gt; &lt;property name=\"email\" value=\"abc@123.com\"&gt;&lt;/property&gt;&lt;/bean&gt; //在JUnit中测试 //ApplicationContext,代表ioc容器 //当前应用的xml配置文件在ClassPath下，ClassPathXmlApplicationContext ApplicationContext ioc = new ClassPathXmlApplicationContext(\"ioc.xml\"); Person bean = (Person) ioc.getBean(\"person01\"); System.out.println(bean); 测试 总结$ 注意： src，源码包开始的路径，称为类路径的开始 所有源码包里面的东西都会合并放到类路径里面Java Wen类开始 /WEB-INF/classes/Java /bin/ 导包commons-logging-1.1.3.jar 先导包在创建配置文件 Spring的容器接管标志了s的类 几个细节 ApplicationContext（IOC容器接口） new ClassPathXmlApplicationContext(“ioc.xml”); ioc配置文件在类路径下 new FileSystemXmlApplicationContext(“D://ioc.xml”); ioc容器配置文件在磁盘路径下 给容器中注册一个组件，我们也从容器中按照id拿到了这个组件的对象 组件的创建工作，是容器完成的 Person对象是在容器创建完成时就已经完成 同一个组件在ioc容器中是单例的，容器启动时创建完成 容器中如果没有这个组件，报异常NoSuchBeanDefinitionException ioc容器正在创建这个对象的时候，会利用setter方法为javaBean的属性进行赋值 JavaBean的属性名由什么决定？getter/setter方法决定的","link":"/post/f431a4c4.html"},{"title":"Spring 初识","text":"框架介绍 Spring 容器 Spring 模块 IOC (Iversion Of Control):控制反转 HelloWorld 实现 框架介绍$ 框架是一种可重用代码的一种设计，具有高度可重用性（ 半成品 ）比如说，在设计一个书城项目：涉及到一些类，将它们打包为jar,再将其中一些jar抽取成高度可重用的；事务控制的，强大的Servlet，项目的一些工具框架：多个可重用模块的集合，形成能一个领域的整体解决方案 Spring 容器$ 可以用来管理所有 组件（类） 的框架核心:IOC AOP Spring是一个开源框架 Spring为简化企业级开发而生，使用Spring，JavaBean就可以实现很多以前要靠EJB（Enterprise JavaBean）才能实现的功能。同样的功能，在EJB中要通过繁琐的配置和复杂的代码才能够实现，而在Spring中却非常的优雅和简洁 Spring是一个IOC(DI)和AOP容器框架。 Spring的优良特性 非侵入式：基于Spring开发的应用中的对象可以不依赖于Spring的API 依赖注入：DI——Dependency Injection，反转控制(IOC)最经典的实现。 面向切面编程：Aspect Oriented Programming——AOP 容器：Spring是一个容器，因为它包含并且管理应用对象的生命周期 组件化：Spring实现了使用简单的组件配置组合成一个复杂的应用。在 Spring 中可以使用XML和Java注解组合这些对象。（可以导入部分包） 一站式：在IOC和AOP的基础上可以整合各种企业应用的开源框架和优秀的第三方类库（实际上Spring 自身也提供了表述层的SpringMVC和持久层的Spring JDBC）。 Spring 模块$ Test: Spring 的单元测试模块 Core Container: 核心容器（IOC）；黑色的那部分代表这部分的功能有哪些jar包组成 1spring-beans-4.0.0.RELEASE.jar、spring-context-4.0.0.RELEASE.jar、spring-core-4.0.0.RELEASE.jar、spring-expression-4.0.0.RELEASE.jar AOP+Aspects（面向切面编程模块） 1spring-aop-4.0.0.RELEASE、spring-aop-4.0.0. RELEASE 数据访问模块 123spring-jdbc-4.0.0.RELEASE、spring-orm(Object Relation Mapping)-4.0.O)RELEASEspring-ox(xml)m-4.0.0.RELEASE、spring-jms-4.0.0.RELEASE(integration)spring-tX-4.0.0.RELEASE(事务) Web: Spring 开发web应用模块 1234spring- websocket(新的技术)-4.0.日. RELEASE、spring-web-4. e.0. RELEASE、和原生的web相关(servlet )spring- webmvc-4.0.0. RELEASE、开发web项目的(web)spring- webmvc-portlet-4. e.日. RELEASE (开发web应用的组件集成) 使用时只需要导入对应的包即可 开发Spring框架的应用，经常要写框架的配置文件，写起来复杂，我们需要提示eclipse安装需要安装插件 IOC (Iversion Of Control):控制反转$控制：资源获取方式 主动式：(要什么资源自给new) 1234BookServlet{ BookService bs = new BookService() AirPlane ap = new AirPlane(); //复杂对象的创建是一个比较庞大的工程} 被动式：资源的获取不是我们自己创建，而是交给组件创建 123456BookServlet{ BookService bs; public void test01(){ bs.checkout(); }} 容器： 管理所有组件（有功能的类），假设，BookSevlet受容器管理，BookService也受容器管理；容器可以自动的探查除那些组件（类）需要用到另一些组件（类），容器帮我们创建BookService对象，并把BookService对象赋值过去 主动的new资源变为被动的接受资源 IOC是一种思想，DI是一种实现DI:（Dependency injection） 依赖注入容器能知道那个组件（类）运行的时候，需要另外一个类（组件）；容器通过反射的形式，容器通过反射的形式，将容器中准备的BookSevice对象注入（利用反射给属性赋值）到BookServlet中 只要是容器管理的组件，都能使用容器提供的强大功能 HelloWorld 实现$HelloWorld 实现","link":"/post/e866659f.html"},{"title":"Shell有类型变量","text":"declare和typeset命令◆ declare命令 和typeset命令两者等价◆ declare、 typeset命令 都是用来定义变量类型的 declare$ 参数 含义 -r 将变量设为只读 -i 将变量设为整数 -a 将变量定义为数组 -f 显示此脚本前定义过的所有函数及内容 -F 仅显示此脚本前定义过的函数名 -x 将变量声明为环境变量","link":"/post/96cd4f46.html"},{"title":"Shell文件查找find命令","text":"语法格式 选项参数 使用 选项 操作 语法格式$ - 格式 语法格式1 find [路径] [选项] [操作] 选项参数$ 选项 含义 -name 根据文件名查找 -perm 根据文件权限查找 -prune 该选项可以排除某些查找目录 -user 根据文件属主查找 -group 根据文件属组查找 -mtime- n | +n 根据文件更改时间查找 -nogroup 查找无有效属组的文件 -nouser 查找无有效属主的文件 -newer file1 ! file2 查找更改时间比file1新但比file2旧IDE文件 -type 按文件类型查找 -size- n +n 按文件大小查找 -mindepth n 从n级子目录开始搜索 -maxdepth n 最多搜索到n级子目录 使用$选项$常用选项 1234567891011121314151617181920212223242526272829303132333435- name 查找/etc目录下以conf结尾的文件 find /etc -name '*conf' ,- iname 查找当前目录下文件名为aa的文件，不区分大小写” find . -iname aa-user 查找文件属主为hdfs的所有文件 find . -user lyhcc-group 查找文件属组为yarn的所有文件 find . -group mysql-type f 文件 find . -type f d 目录 find . -type d c 字符设备文件 find . -type c b 块设备文件 find . -type b l 链接文件 find . -type 1 p 管道文件 find . -type P-size -n 大小大于n的文件 +n 大小小于n的文件 例子1:查找/etc目录下小于10000字节的文件 find /ete -size -10000c 例子2:香找/ete目录下大干1M的文件 find /etc -size +1M-mtime -n n天以内修改的文件 +n n天以外修改的文件 n 正好n天修改的文件 例子1:查找/etc目录下5天之内修改且以conf结尾的文件find /etc -mtime -5 -name '*.conf' 例子2:查找/etc目录下10天之前修改且属主为root的文件 find /etc -mtime +10 -user root-mmin -n n分钟以内修改的文件 +n n分钟以外修改的文件 例子1:查找/etc目录下30分钟之前修改的文件 find /etc -mmin +30 例子2:查找/etc目录下30分钟之内修改的目录 find /etc -mmin -30 -type d-mindepth n 表示从n级子目录开始搜索 例子:在/etc下的3级子目录开始搜索 find /etc -mindepth 3-maxdepth n 表示最多搜索到n级子目录 例子1:在/etc下搜索符合条件的文件，但最多搜索到2级子目录find /etc -maxdepth 3 -name \"文件条件\" 例子2: find ./etc/ -type f -name \"*.conf\" -size +10k -maxdepth 2 其他选项 1234567891011121314151617-nouser 查找没有属主的用户 例子: find . -type f -nouser-nogroup 查找没有属组的用户 例子: find . -type f -nogroup-perm 例子: find . -perm 664-prune 通常和-path-起使用，用于将特定目录排除在搜索条件之外 例子1:查找当前目录下所有普通文件，但排除test目录 find . -path ./test -prune -o -type f 例子2:查找当前目录下所有普通文件，但排除etc和opt 目录 find . -path ./etc -prune -o -path ./opt -prune -o -type f 例子3:查找当前目录下所有普通文件，但排除etc和opt 目录，但属主为hdfs find . -path ./etc -prune -o -path ./opt -prune -o -type f -a -user hdfs 例子4:查找当前目录下所有普通文件，但排除etc和opt目录，但属主为hdfs，且文件大小必须大于500字节 find . -path ./etc -prune -o -path ./opt -prune -o -type f -a -user hdfs -a -size +500c-newer file1 例子: find /etc -newer a 案例–找短命对应的cnf文件配置项个数 操作$12345678910111213-print 打印输出(默认)-exec 对搜索到的文件执行特定的操作，格式为-exec 'command' {} \\; {}表示前面搜索的结果 \\;固定格式 例子1:搜索/etc下的文件(非目录)，文件名以conf结尾，且大于10k， 然后将其删除 find ./etc/ -type f -name '*.conf' -size +10k -exec rm -f {} \\; 例子2:将/vax/1og/ 目录下以1og结尾的文件，且更改时间在7天以上的删除 find /var/1og/ -name '*.1og' -mtime +7 -exec rm -rf {} \\; . 例子3:搜索条件和例子1一样， 只是不删除，而是将其复制到/root/conf目录下 find ./etc/ -size +10k -type f -name '*.conf' -exec cp {} /root/conf/ \\;-ok 和exec功能一样，只是每次操作都会给用户提示 逻辑运算符 12345678910-a 与-o 或-not|! 非 例子1:查找当前目录下，属主不是hdfs的所有文件 find . -not -user hdfs I find . ! -user hdfs 例子2:查找当前目录下，属主属于hdfs，且大小大于300字节的文件 find . -type f -a -user hdfs -a -size +300c 例子3:查找当前目录下的属主为hdfs或者以xml结尾的普通文件 find . -type f -a \\( -user hdfs -o -name '*.xml' \\)","link":"/post/f449e7e2.html"},{"title":"文本处理三剑客(grep/sed/awk)","text":"grep和egrep过滤器 grep 选项option sed 流编辑器 sed选项 sed Pattern用法 sed 中的编辑命令详解 awk报告生成器 grep和egrep过滤器$grep$ 第一种形式: grep [option] [pattern] [file1,file..] 第二种形式: command| grep [option] [pattern] 选项option$常用选项 选项 含义 -V 不显示匹配行信息 -i 搜索时忽略大小写 -n 显示行号 -r 递归搜索 -E 支持扩展正则表达式 -F 不按正则表达式匹配,按照字符串字面意思匹配 不常用选项 选项 含义 -c 只显示匹配行总数 -w 匹配整词 -x 匹配整行 -l 只显示文件名,不显示内容 -s 不显示错误信息 grep和egrep: grep默认不支持扩展正则表达式，只支持基础正则表达式 使用grep-E可以支持扩展正则表达式 使用egreep可以支持扩展正则表达式，与grep -E等价 sed 流编辑器$ sed(Stream Editor)，流编辑器。对标准输出或文件逐行进行处理格式 第一种形式: stdout | sed [option] “pattern command” 第二种形式: sed [option] “pattern command” file sed选项$ 选项 含义 -n 只打印模式匹配行 -e 直接在命令行进行sed编辑,默认选项 -f 编辑动作保存在文件中,指定文件执行 -r 支持扩展正则表达式 -i 直接修改文件内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@master datas]# cat sed.txt I love shellI love SHELLaxczcaxa[root@master datas]# sed -n '/Shell/p' sed.txt I love shell# 多个comand时必须加-e[root@master datas]# sed -n '/shell/p' sed.txt I love shell[root@master datas]# sed -n -e '/shell/p' -e '/SHELL/p' sed.txt I love shellI love SHELL# -f# edit.sed中有 /SHELL/p[root@master datas]# sed -n '/SHELL/p' sed.txt I love SHELL[root@master datas]# sed -n -f edit.sed sed.txt I love SHELL# -r[root@master datas]# sed -n -r '/SHELL|shell/p' sed.txt I love shellI love SHELL# -i[root@master datas]# sed -n 's/love/like/g;p' sed.txt I like shellI like SHELLaxczcaxa[root@master datas]# cat sed.txt I love shellI love SHELLaxczcaxa# 加-i直接将原文件修改[root@master datas]# sed -n -i 's/love/like/g;p' sed.txt [root@master datas]# cat sed.txt I like shellI like SHELLaxczcaxa sed Pattern用法$ 匹配模式 含义 10command 匹配到第10行 10,20command 匹配从第10行开始,到第20行结束 10, +5command 匹配从第10行开始,到第16行结束 /pattern1/command 匹配到pattern1的行 /pattern1/,/pattern2/command 匹配到pattern1的行开始,到匹配到pattern2的行结束 10,/pattern1/command 匹配从第10行开始,到匹配到pettern1的行结束 /pattern1/,10command 匹配到pattern1的行开始,到第10行匹配结束 1234567891011121314151、LineNumber ---直接指定行号 sed -n \"17p\" file 打印file文件的第17行2、StartLine , EndLine --指定起始行号和结束行号 sed -n \"10,20p\" file 打印file文件的10到20行3、Star tLine, +N 指定起始行号，然后后面N行 sed -n \"10,+5p\" file 打印file文件中从第10行开始，往后面加5行的内容4、/pattern1/ ----------------正则表达式匹配的行 sed -n \"/^root/p\" file 打印file文件中以root开头的行5、/pattern1/,/pattern2/ ---从匹配到pattern1的行，到匹配到pattern2 ... 案例–查找配置文件中配置项段名与段的个数 12345678910111213141516171819202122#!/bin/bashFILE_NAME=/opt/datas/openssl.cnffunction get_all_segment{ segments=\"`sed -n '/\\[ .* \\]/p' $FILE_NAME | sed -n 's/\\[ \\(.*\\) \\]/\\1/g;p'`\" echo $segments}function count_segment_num{ echo \"`sed -n '/\\[ '$1' \\]/,/\\[/p' $FILE_NAME | sed -n -e '/=/p' | grep -v ^\\# -c`\"}number=1for segment in `get_all_segment`do echo \"$number: $segment `count_segment_num $segment`\" number=`expr $number + 1`done 案例 sed 中的编辑命令详解$ 查询 p 打印 增加 a 行后追加 i 行前追加 r 外部文件读入，行后追加 w 匹配行写入外部文件 追加内容 1234567891011121314151617181920212223241、a (1)、passwd文件第10行后面追加\"Add Line Behind\" sed -i '10a Add line behind' passwd (2)、passwd文件第10行到第20行，每一-行后面都追加\"Test Line Behind\" sed -i '10,20a Test Line Behind' passwd (3)、passwd文 件匹配到/bin/bash的行后面追加\"Insert Line For /bin/bash Behind\" sed -i '/\\/bin\\/bash/a Insert Line For /bin/bash Behind' passwd2、i (1)、passwd文件匹配到以tss开头的行，在匹配航前面追加\"AddLineBefore\" sed -i '/^tss/i Add a line before it' passwd (2)、passwd文件每一行前面都追加\"Insert Line Before Every Line\" sed -i 'i Insert a line before every line' passwd3、r (1)、将/opt/datas/anotation文 件的内容追加到passwd文件的第20行后面 sed -i '20r /opt/datas/anotation' passwd (2)、将/etc/inittab文件内容追加到passwd文件匹配/sbin/nologin行的后面 sed -i '/\\/sbin\\/nologin/r /opt/datas/anotation' passwd (3)、将/opt/datas/anotation文件内容追加到passwd文件中特定行后面，匹配以ftp开头的行,到第18行的所有行 sed -i '/ftp/,18r /opt/datas/anotation' passwd4、w (1)、将passwd文件匹配到/bin/bash的行追加到/opt/datas/sed.txt文件中 sed -i '/\\/bin\\/bash/w /opt/datas/sed.txt' passwd (2)、 将passwd文件从第10行开始，到匹配到tss开头的所有行内容追加到/opt/datas/sed-1.txt sed -i '10,/^tss/w /opt/datas/sed-1.txt' passwd 删除 d 删除12345678910111213141516171819202122231、删除/etc/passwd中的第15行 sed -i '15d' /etc/passwd2、删除/etc/passwd中的第8行到第14行的所有内容 sed -i '8,14d' /etc/passwd3、删除/etc/passwd中的不能登录的用户(筛选条件: /sbin/nologin) sed -i '/\\/sbin\\/nologin/d' /etc/passwd4、删除/etc/passwd中以mai1开头的行，到以yarn开头的行的所有内容 sed -i '/^mai1/,/^yarn/d' /etc/passwd5、删除/etc/passwd中第一个不能登录的用户，到第13行的所有内容(少用) sed -i '/\\/sbin\\/nologin/,13d' /etc/passwd6、删除/etc/passwd中 第5行到以ftp开头的所有行的内容 sed -i '5,/^ftp/d' /etc/passwd7、删除/etc/passwd中以yarn开头的行到最后行的所有内容 sed -i '/^yarn/,$d' /etc/passwd8、删除/etc/passwd中不能登录的所有用户 sed -i '/\\/sbin\\/nologin/d' /etc/passwd典型需求 1、删除配置文件中的所有注释行和空行 sed -i '/^#/d;/^$/d' openssl.cnf sed -i '/[:blank:]*#/d' openssl.cnf # 删除空格行的注释，但对openssl.cnf无效 2、在配置文件中所有不以#开头的行前面添加*符号，注意:以#开头的行不添加 sed -i 's/^[^#]/\\*&amp;/g' openssl.cnf 修改 s/old/new 将行内第一个old替换为new s/old/new/g 将行内全部的old替换为new s/old/new/2g 将行内前2个old开始替换所有为new s/old/new/ig 将行内olg全部替换为new ,忽略大小写修改命令对照表 | 编辑命令 | 含义 || —————————— | ———————————————————— | || 1s/old/new/ | 替换第1行内容old为new || 1,10s/old/new/ | 替换1行到10行的内容old为new || 1, +5s/old/newl | 替换1行到6行的内容old为new || /pattern1/s/old/new/ | 替换匹配到pattern1的行内容old为new || /pattrn1/,/pattern2/s/old/new/ | 替换匹配到pattern1的行直到匹配到pattern2的所有行内容old为new || /pattern1/, 10s/old/new/ | 替换匹配到pattern1的行到10行的所有行内容old为new || 10,/pattern1/s/old/new/ | 替换第10行直到匹配到pattern1的所有行内容old为new | 1234567891011121、修改/etc/passwd中第1行中第1个root为ROOT sed -i '1s/root/ROOT/' /etc/passwd2、修改/etc/passwd中第5行到第10行中所有的/sbin/nologin为/bin/bash sed -i '1,10s/\\/sbin\\/nologin/\\/bin\\/bash/' /etc/passwd3、修改/etc/passwd中匹配到/sbin/nologin的行， 将匹配到行中的login改为大写的LOGIN sed -i '/\\/sbin\\/nologin/s/login/LOGIN/g' /etc/passwd4、修改/etc/passwd中从匹配到以root开头的行，到匹配到行中包含mail的所有行。修改内为将这些所有匹配的行中的bin改为HADOOP sed -i '/^root/,/mail/s/bin/HADOOP/g' passwd5、修改/etc/passwd中从匹配到以root开头的行，到第15行中的所有行，修改内容为将这些行中的nologin修改为SAPRK sed -i '/^root/,15s/nologin/SPARK/g' passwd6、修改/etc/passwd中从第15行开始，到匹配到以tss开头的所有行，修改内容为将这些行中的bin换为BIN sed -i '15,/^tss/s/bin/BIN/g' passwd 其他 = 查看行号不显示内容 什么是反向引用？$ 什么是反向引用？ 使用&amp;或\\1可以表示前面匹配到的,使用\\1需要加括号，其中&amp;查找部分原封不动的取代 12345678910111213141516171819202122232425262728293031[root@master datas]# cat test hadoophadAAphadBBphadCCphadDDpsed -i 's/had..p/&amp;s/g' test[root@master datas]# cat test hadoopshadAApshadBBpshadCCpshadDDps[root@master datas]# sed -i 's/\\(had..p\\)/\\1S/g' test [root@master datas]# cat test hadoopSshadAApSshadBBpSshadCCpSshadDDpSs# 部分取代进行修改，用括号括住引用部分[root@master datas]# sed -i 's/\\(had\\)...../\\1oop/g' test [root@master datas]# cat test hadoophadoophadoophadoophadoop 有变量时使用双引号或将变量用单引号引起来 $","link":"/post/e4677a0d.html"},{"title":"Java动态代理","text":"概述 代理接口的创建 java.lang.eflct.Proxy比较重要的方法 调用转发 Java动态代理实例 概述$ Hadoop远程过程调用实现使用Java动态代理和新输入1输出系统(NewInput/Output,NIO) Java动态代理类位于java.lang.reflect包下，主要包括java.lang rlfct.Proxy和java.lang.reflect.InvocationHandler 代理对象两大任务 创建代理接口 实现由java.lang,reflect.Proxy完成 调用转发通过java.lang.reflect.InvocationHandler的实例完成 代理接口的创建$ 在Java中，代理对象往往实现和目标对象-致的接口，并作为目标对象的代替，接收对象用户(Client) 的调用，并将全部或部分调用转发给目标对象 代理也是拥有和目标对象一样的权利的‘人’， 简单的说， 代理可以越俎代庖。实际上他是调用转发，将这个任务交给有权利实施的人 代理时序图 java.lang.eflct.Proxy提供了用于创建动态代理类和对象的静态方法。也就是说，通过java.lang.reflect.Proxy可以动态地创建某个接口实现 java.lang.eflct.Proxy比较重要的方法$ public static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces) 获得代理类的java.lang.Class对象。该代理类将定义在指定的类加载器（参数loader）中，并将实现参数interfaces指定所有接口 注意： 这个类只创建一次，如果再次传入相同的loader和interfaces给newProxyInstance()方法，获得的也只是第一次调用创建的那个java.lang.Class对象 通过Proxy.getProxyClass()获得的代理类都包含一个构造函数,该构造函数需要一个java.lang.reflect.InvocationHandler 的实例 如何获取Proxy对象？，请看以下代码 123456Class clss = Proxy.getProxyClass(loader, interfaces);//获得构造函数Constructor constructor = clss.getConstructor(new Class[]{InvocationHandler.class});//创建代理对象Object proxy = constructor.newInstance(new Object[]{invocationHandler}); public static boolean isProxyClass(Class&lt;?&gt; cl) 判断java.lang.Class对象是否是代理类 public static InvocationHandler getInvocationHandler(Object proxy) throws IllegalArgumentException 获取代理实例对应的调用处理程序（即构建代理传入的InvocationHandler实例） 调用转发$ InvocationHandler调用实例也叫调用句柄实例 12345678910111213141516/** * * @param proxy 代理对象本身 * @param method 用户调用的代理对象上的方法 * @param args 传递给该方法的参数 * @return 代理对象方法调用结果 * @throws Throwable * * java.lang.reflect.Method * 它提供了关于类或接口上某个方法以及如何访问该方法的信息 * 其中的invoke方法可以在指定对象上调用对象的方法 */@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return null;} Method类中的invoke方法声明如下 123public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException 假设目标对象为target，实现转发代码如下 123public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return method.invoke(target, args);s} Java动态代理实例$实例类图 代码 12345678910111213/**DPStatus.java**/public class DPStatus { String name; public DPStatus(String name) { this.name = name; } @Override public String toString() { return \"Hello, \" + name + \"!\"; }} 12345678/** * PDQueryStatus.java * * 动态代理机制与java远程调用不同，不需要继承什么接口 */public interface PDQueryStatus { public DPStatus getStatus();} 1234567891011/** * DPQueryStatusImpl.java * * PDQueryStatus的简单实现 */public class DPQueryStatusImpl implements PDQueryStatus{ @Override public DPStatus getStatus() { return new DPStatus(\"Bitty\"); }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.text.MessageFormat;import java.util.Arrays;/** * DPInvocationHandler.java * * 转发由该类实现 * 该类中最重要的一部分是invoke方法， * * 如果代理对象调用某个方法时，DPInvocationHandler.invoke将会被调用， * 传入invoke的method中，也就是说，PDQueryStatus.getStatus()就是是method对象， * 而getStatus()没有参数 * * 在这里完成代理转发的是 * Object res = method.invoke(dpqs, args); * 当method是PDQueryStatus.getStatus()时，其效果就相当于 * res = pdqs.getStatus() * * * */public class DPInvocationHandler implements InvocationHandler { //目标对象 private DPQueryStatusImpl dpqs; public DPInvocationHandler(DPQueryStatusImpl dpqs) { this.dpqs = dpqs; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //实现附加功能，在控制台输出调用参数的String表示 String msg = MessageFormat.format(\"Calling method\", method.getName(), Arrays.toString(args)); System.out.println(msg); //调用转发 Object res = method.invoke(dpqs, args); //其他附加功能 return res; }} 1234567891011121314151617/** * DPMain.java * * 在这个类中使用create方法创建代理 */public class DPMain { public static PDQueryStatus create(DPQueryStatusImpl dpqs){ //newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) //参数准备 Class&lt;?&gt;[] interfaces = new Class[]{PDQueryStatus.class}; DPInvocationHandler handler = new DPInvocationHandler(dpqs); return (PDQueryStatus) Proxy.newProxyInstance(dpqs.getClass().getClassLoader(), interfaces, handler); }} 123456789public class Demo { public static void main(String[] args) throws Exception { PDQueryStatus pdqs = DPMain.create(new DPQueryStatusImpl()); System.out.println(pdqs.getStatus()); }}","link":"/post/e63a7813.html"},{"title":"DOM解析","text":"DOM 的工作方式是： 处理步骤 hadoop配置文件解析的特别说明 hadoop 配置文件解析完整代码 说明：这里主要分析hadoop的DOM解析 DOM 的工作方式是：$ 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档处理步骤$ 获得用于创建DOM解析器的工厂对象1DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); 可以设置一下参数[可选]12345678docBuilderFactory.setIgnoringComments(true);docBuilderFactory.setNamespaceAware(true);boolean useXInclude = !wrapper.isParserRestricted();try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28);} 获得解析XML的DocumentBuilder对象1DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); 获取根节点下的所有节点1NodeList props = root.getChildNodes(); 遍历节点123456789101112for(int i = 0; i &lt; props.getLength(); ++i) { //获取节点 Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; //prop.getTagName()获取节点内的值 if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { ... } hadoop配置文件解析的特别说明$ 对DocumentBuilderFactory做的处理 忽略XML文档中的注释1docBuilderFactory.setIgnoringComments(true); 支持XML命名空间1docBuilderFactory.setNamespaceAware(true); 支持XML包含机制12345try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \")for parser \" + docBuilderFactory, var28);} XInclude机制允许将XML文档分解为多个可管理的块，然后将-一个或多个较小的文档组装成一个大型文档。 hadoop 配置文件解析完整代码$123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157private Configuration.Resource loadResource(Properties properties, Configuration.Resource wrapper, boolean quiet) { String name = \"Unknown\"; try { Object resource = wrapper.getResource(); name = wrapper.getName(); DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); docBuilderFactory.setIgnoringComments(true); docBuilderFactory.setNamespaceAware(true); boolean useXInclude = !wrapper.isParserRestricted(); try { docBuilderFactory.setXIncludeAware(useXInclude); } catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28); } if (wrapper.isParserRestricted()) { docBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true); } DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); Document doc = null; Element root = null; boolean returnCachedProperties = false; if (resource instanceof URL) { doc = this.parse(builder, (URL)resource); } else if (resource instanceof String) { URL url = this.getResource((String)resource); doc = this.parse(builder, url); } else if (resource instanceof Path) { File file = (new File(((Path)resource).toUri().getPath())).getAbsoluteFile(); if (file.exists()) { if (!quiet) { LOG.debug(\"parsing File \" + file); } doc = this.parse(builder, new BufferedInputStream(new FileInputStream(file)), ((Path)resource).toString()); } } else if (resource instanceof InputStream) { doc = this.parse(builder, (InputStream)resource, (String)null); returnCachedProperties = true; } else if (resource instanceof Properties) { this.overlay(properties, (Properties)resource); } else if (resource instanceof Element) { root = (Element)resource; } if (root == null) { if (doc == null) { if (quiet) { return null; } throw new RuntimeException(resource + \" not found\"); } root = doc.getDocumentElement(); } Properties toAddTo = properties; if (returnCachedProperties) { toAddTo = new Properties(); } if (!\"configuration\".equals(root.getTagName())) { LOG.fatal(\"bad conf file: top-level element not &lt;configuration&gt;\"); } NodeList props = root.getChildNodes(); Configuration.DeprecationContext deprecations = (Configuration.DeprecationContext)deprecationContext.get(); for(int i = 0; i &lt; props.getLength(); ++i) { Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { if (wrapper.isParserRestricted() &amp;&amp; \"http://www.w3.org/2001/XInclude\".equals(prop.getNamespaceURI())) { throw new RuntimeException(\"Error parsing resource \" + wrapper + \": XInclude is not supported for restricted resources\"); } LOG.warn(\"Unexpected tag in conf file \" + wrapper + \": expected &lt;property&gt; but found &lt;\" + prop.getTagName() + \"&gt;\"); } NodeList fields = prop.getChildNodes(); String attr = null; String value = null; boolean finalParameter = false; LinkedList&lt;String&gt; source = new LinkedList(); //遍历所有节点，并根据情况设置对象的成员变量properties和finalParameters for(int j = 0; j &lt; fields.getLength(); ++j) { Node fieldNode = fields.item(j); if (fieldNode instanceof Element) { Element field = (Element)fieldNode; if (\"name\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { attr = StringInterner.weakIntern(((Text)field.getFirstChild()).getData().trim()); } if (\"value\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { value = StringInterner.weakIntern(((Text)field.getFirstChild()).getData()); } if (\"final\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData()); } if (\"source\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { source.add(StringInterner.weakIntern(((Text)field.getFirstChild()).getData())); } } } source.add(name); if (attr != null) { if (deprecations.getDeprecatedKeyMap().containsKey(attr)) { Configuration.DeprecatedKeyInfo keyInfo = (Configuration.DeprecatedKeyInfo)deprecations.getDeprecatedKeyMap().get(attr); keyInfo.clearAccessed(); String[] arr$ = keyInfo.newKeys; int len$ = arr$.length; for(int i$ = 0; i$ &lt; len$; ++i$) { String key = arr$[i$]; this.loadProperty(toAddTo, name, key, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } else { this.loadProperty(toAddTo, name, attr, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } } } } if (returnCachedProperties) { this.overlay(properties, toAddTo); return new Configuration.Resource(toAddTo, name, wrapper.isParserRestricted()); } else { return null; } } catch (IOException var29) { LOG.fatal(\"error parsing conf \" + name, var29); throw new RuntimeException(var29); } catch (DOMException var30) { LOG.fatal(\"error parsing conf \" + name, var30); throw new RuntimeException(var30); } catch (SAXException var31) { LOG.fatal(\"error parsing conf \" + name, var31); throw new RuntimeException(var31); } catch (ParserConfigurationException var32) { LOG.fatal(\"error parsing conf \" + name, var32); throw new RuntimeException(var32); }}","link":"/post/e9fcf702.html"},{"title":"Java NIO","text":"Java基本套接字 基本操作 Java NIO基础 NIO与IO的区别 缓冲区 通道 对象的创建 数据的读写 是否支持工作在非阻塞状态 选择器 选择器的打开关闭 获取/设置标志位 SeverSocketChannel/SocketChannel配合工作的API 注销/获取选择器 select方法 获取相关建的方法 判断通道上等待操作的方法 与附件有关的另外两个方法 Java NIO实例 Java基本套接字$基本操作$ 连接远程机器。 发送数据。 接收数据。 关闭连接。 绑定端口。 监听入站数据。 在所绑定端口.上接受来自远程机器的连接。 其中，前四项用于客户端，后六项用于服务器，最后三项只有服务器才需要，即等待客户端的连接，这些操作通过ServerSocket类实现。 Java NIO基础$概述 NIO解决一客户一线程所带来的开销大的问题所谓一客户一线程是指，多个客户端访问同一个服务器进程时，服务器需要为每个请求创建一个服务线程 并且非堵塞是NIO实现的重要功能之一，为了实现非堵塞，NIO引入了 选择器 Selector 通道 Channel通道表示到实体(如硬件设备、文件、网络套接字或者可以执行一个或多个不同的的IO操作)的程序组件开放连接 通道可以注册一个选择器实例，通过该实例的select方法，用户可以询问“在一个或一组通道中，哪一个是当前需要的服务（即被读、写或被接受）“在一个准备好的通道执行相应的I/O操作，就不需要等待，也就不会堵塞了 NIO与IO的区别$ I0 NIO 面向流(Stream Oriented) 面向缓冲区(Buffer Oriented) 阻塞I0(Blocking I0) 非阻塞I0(Non Blocking I0) (无) 选择器(Selectors) 缓冲区$ NIO中一个主要的特性是java.nio.Buffer。缓冲区(Buffer)提供了一个比流抽象的、更高效和可预测的I/O。Buffer 代表了一个有限容量的容器一其本质是一个数组，通道Channel使用Buffer实例来传输数据Buffer包含4个索引 capacity: 缓冲区总容量，可通过Buffer.capacity获取,并且是不可修改 position: 缓冲区位置，即下一个要写入或读取的索引，获取/设置通过position()/position(int) limit: 缓冲区限制，即第一个不应该读取或写入的位置，获取/设置通过limit()/limit(int) mark: 缓冲区位置标记，通过mark()设置一个位置，reset()方法被调用后，position被置为mark 遵循如下规则 0 ≤ mark ≤ position ≤ limit ≤ capacity ByteBuffer的创建$1234567//直接创建缓冲区public static ByteBuffer allocate (int capacity) //在某个字节数组上创建缓冲区public static ByteBuffer wrap (byte[] array)//上面的调用下面的//上一个方法的返回 wrap(array, 0, array.length)public static ByteBuffer wrap (byte[] array, int offset, int length) 这种方式与流的区别：流是单向的，而这种方式看读可写 ByteBuffer的读写$ put()/get()方法：基于相对位置和绝对位置的读写基于相对位置就是基于目前缓冲区位置position的当前值，从“下一个”位置读取或存放数据，并为position增加适当的值。 绝对位置的put)/get()方法，必须提供写入/读出的位置, 该方式的读写操作，不改变position的值 1234567891011//相对位置public byte get ()public ByteBuffer get (byte[] dst)public ByteBuffer get (byte[] dst, int offset，int length)public ByteBuffer put (byte b)public final ByteBuffer put (byte[] src)public ByteBuffer put (byte[] src, int offset, int length)public ByteBuffer put (ByteBuffer src)//绝对位置public byte get (int index)public ByteBuffer put (int index, byte b) 需要注意的是 ，部分数据的get()/put()是不允许的。以写人为例，如果要写入的数据量超过当前缓冲区允许写入的数据量(可通过Buffer.remaining()方法获得该值)，则所有的数据都不会写入缓冲区，position的位置不变，put()方法抛出BufferOverflowException异常。 工具方法$ clear() 通过clear()方法，缓冲区的poistion被设置为0, limit 设置为capacity,这样，缓冲区准备好接收新数据。后续的put()/readO调用，将数据从第-一个元素开始填入缓冲区，最多直到填满该缓冲区，达到limit位置(等于capacity)。 flip()方法用于将缓冲区准备为数据传出状态，该方法将limit设置为position后，将position 设置为0。后续的get()/write()方法将从缓冲区的第一个元素开始传出数据，直到limit位置。通过fip0方法和get(/write()方法配合，可以将前面利用put()/read)方法放入缓冲区的所有数据读出(一直读到limit)。 rewind)方法将position设置为0，但不改变limit的值，如果需要多次读取缓冲区里的数据，可以在两次读取间使用rewind()方法。 compact()方法将position和limit间的数据复制到缓冲区的开始位置，为后续的put()/read()调用让出空间。调用结束后，poistion的值被设置为数据的长度，也就是原来的limit减去position的值，而limit则设置为capacity。和clear()、fip() 等方法不同，compact() 不但改变了position 和limit的位置，还改变了缓冲区中的数据。 compact()主要用于在缓冲区中还有未写出的数据时，为读入数据准备空间:即在write()方法调用后和添加新数据的read)方法前调用compact()方法，将未写出的“剩余”数据移动到缓冲区前面，为后面read()方法提供释放空间。在图中，假设write()方法调用后，缓冲区处于“某工作状态”，这时，position 到limit间的数据为未写出的“剩余”数据，而limit到capacity的空间则是read)方法可以使用的空间，通过compact()操作，position 到limit 间的数据被挪到缓冲区的前面，position 的位置也被设置为“剩余”数据长度。接下来，开发人员就可以直接调用read0/put0方法，从position位置开始放入数据。该数据和原来的“剩余”数据- -起，构成了连续的可用数据。 Buffer还支持一些其他功能，如直接缓冲区(directbuffer)、Java基本类型的put()/get()、缓冲区共享、复制、透视、字符编码转换等 通道$ 一个Channel的实例代表一个 和设备的 连接 对象的创建$ ServerSocketChannel/SocketChannel通过工厂方法创建 1234public static SocketChannel open() throws IOExceptionpublic static SocketChannel open(SocketAddress remote) throws IOExceptionpublic static ServerSocketChannel open() throws IOException SocketChannel创建后，可以通过connect()连接到远程机器，通过close()关闭连接，这些操作和Socket的没有什么差别。 数据的读写$123456public int read (ByteBuffer dst)public long read (ByteBuffer[] dsts, int offset, int length)public final long read (ByteBuffer[] dsts)public int write (ByteBuffer src)public long write (ByteBuffer[] srcs, int offset, int length)public final long write (ByteBuffer[] srcs) 和socket类似的操作，可以通过socket方法获取ServerSocket对象 12public SocketChannel accept ()public ServerSocket socket () 是否支持工作在非阻塞状态$ Buffer使用阻塞方法相对于基本套接字没有什么优点 12public SelectableChannel configureBlocking (boolean block) //设置堵塞public boolean isBlocking() //是否堵 非阻塞的SocketChannel 的connect()方法会立即返回，用户必须通过isConnected()判断连接是否已经建立，或者通过finishConnect()方法在非阻塞套接字上阻塞等待连接成功:非阻塞的read(),在Socket上没有数据的时候，立即返回(返回值为0)，不会等待;非阻塞的acceptO,如果没有等待的连接，将返回null 12public boolean isConnected ()public boolean finishConnect () 选择器$ 选择器(Selector) 的使用方法:通过静态的工厂方法创建Selector实例，通过Channel的注册方法，将Selector实例注册到想要监控的Channel实例上，最后调用选择器的select()方法。该方法会阻塞等待，直到有一个或多个通道准备好I/O操作或超时。select() 方法将返回可进行I/O操作的通道数量。现在，在一个单独的线程中，就可以检查多个通道是否可以进行I/O操作，不需要为每一一个通道都准备一个线程了。 选择器的打开关闭$123public static Selector open ()public boolean isOpen ()public abstract void close () 获取/设置标志位$选择器注册标记SelectionKey维护联的信息保存在java.nio.channels.SelectionKey实例中 OP_ READ (通道上有数据可读) OP_ WRITE (通道已经可写) OP_ CONNECT (通道连接已建立) OP_ ACCEPT (通道上有连接请求) 12public int interestOps ()public SelectionKey interestops (int ops) SeverSocketChannel/SocketChannel配合工作的API$1234public SelectionKey register (Selector sel, int ops) // 注册public SelectionKey register (Selector sel, int ops, object att)//带附件注册public SelectionKey keyFor (Selector sel) // 根据选择器，查找对应的selectionKeypublic boolean isRegistered() // 判断通道是否已经注册 选择器实例selector.上注册了- -个SocketChannel对象，支持读操作 1SelectionKey readKey = channel . register (selector, SelectionKey.OP_READ) 注销/获取选择器$123Selector selector () //获取选择器selectableChannel channel () //获取相应的channelvoid cancel () //注销选择器 select方法$1234/**如果发现select()的返回值大于0，表明有需要处理的I/O事件发生**/public int select()// 阻塞等待，直到一个注册通道有感兴趣的操作就绪public int select (1ong timeout)// 等待一段时间，或一个注册通道有感兴趣的操作就绪public int selectNow()// 非阻塞版本 获取相关建的方法$12public Set&lt;SelectionKey&gt; keys () //selector上已注册的所有键public Set &lt;SelectionKey&gt; selectedKeys()// 已选键集 Hadoop IPC上的使用 判断通道上等待操作的方法$12345public int readyOps ()public boolean isReadable ()public boolean iswritable ()public boolean isConnectable ()public boolean isAcceptable () 与附件有关的另外两个方法$12public Object attach (object ob) //添加附件public object attachment () //获取附件 以上内容来自《Hadoop技术内幕 深入解析HADOOP COMMON和HDFS架构设计与实现原理》","link":"/post/f2d80d11.html"},{"title":"Linux的一些笔记","text":"coreutils nohub命令使用需要安装 mlocate locate命令使用需要安装 vim 下如何批量注释","link":"/post/fe24bc9e.html"},{"title":"Java远程调用","text":"概述 远程方法调用实例 Java远程调用实例类图 概述$ Java远程方法调用(Remote Method Invocation, RMI)是Java的一个核心API和类库,允许一个Java虚拟机上运行的Java程序调用不同虚拟机上运行的对象中的方法，即使这两个虚拟机运行于物理隔离的不同主机上。在某种程度上，RMI可以看成RPC的Java升级版。 和RPC一样,存在服务端和客户端典型服务器端应用程序 创建多个远程对象（Remote Object）,使这些对象能被客户端引用，并等待客户端调用远程对象的方法典型的客户端程序 从服务器获得一个或多个远程对象的引用，然后调用远程对象的方法 Java远程方法调用依赖于Java序列化，调用远程方法传的参数、返回值都是序列化对象 远程方法调用实例$123456789/**RMIQueryStatus.java**/import java.rmi.Remote;import java.rmi.RemoteException;public interface RMIQueryStatus extends Remote { String getStatus(String name) throws RemoteException;} RMIQueryStatus的定义要求 远程接口必须声明为public，否则客户端试着装载“实现远程接口”的远端对象时，会收到错误的消息。 远程接口必须继承自java.rmi.Remote。 远程接口中的每-一个方法，除了自定义的异常之外，必须将java.rmi.RemoteException声明于其throws子句中。 在远程方法声明中，作为参数或者返回值的远程对象，或者包含在其他非远程对象中的远程对象，必须声明为其对应的远程接口，而不是实际的实现类。(这点在String类中并没有体现) RMIQueryStatus的实现类 123456789101112/**RMIQueryStatusImp.java**/import java.rmi.RemoteException;import java.rmi.server.UnicastRemoteObject;public class RMIQueryStatusImp extends UnicastRemoteObject implements RMIQueryStatus { protected RMIQueryStatusImp() throws RemoteException { } public String getStatus(String name) throws RemoteException { return \"I'm \" + name + \".\"; }} 客户端和服务端的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/**RMIDemoServer**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.RemoteException;import java.rmi.registry.LocateRegistry;/** * @author lyhcc */public class RMIDemoServer { public static void main(String[] args) throws RemoteException, MalformedURLException { //1. 创建RMIQueryStatus对象 RMIQueryStatusImp queryService = new RMIQueryStatusImp(); //2. 设置服务端口 LocateRegistry.createRegistry(12090); //3. 绑定远端对象名 Naming.rebind(\"rmi://localhost:12090/queryTest\", queryService); System.out.println(\"Server is running!\"); }}/**客户端的代码**/import java.net.MalformedURLException;import java.rmi.Naming;import java.rmi.NotBoundException;import java.rmi.RemoteException;public class RMIDemoClient { public static void main(String[] args) throws RemoteException, NotBoundException, MalformedURLException { //1. 创建RMIQueryStatusImp对象 RMIQueryStatus queryStatus = (RMIQueryStatus) Naming.lookup(\"rmi://localhost:12090/queryTest\"); //2. 调用远程方法 String status = queryStatus.getStatus(\"KiKi\"); System.out.println(status); }} 先运行服务端然后运行客户端查看结果 Java远程调用实例类图$ 客户端RMIQueryStatusClient的工作依赖于RMI存根(Stub)，这个存根是通过Java的代理机制 java.lang.reflect.Proxy","link":"/post/55cde24e.html"},{"title":"Hadoop介绍","text":"起源 Google 在大数据方面的三大论文 （谷歌三宝） Hadoop 三大发行版本 硬件要求 起源 Google 在大数据方面的三大论文 （谷歌三宝）$在github大的当前目录下三宝的介绍 Hadoop 三大发行版本$ Apache、Cloudera、Hortonworks Apache版本最原始、最基础：适合零基础 大公司在用 Cloudera Cloudera’s DistributionIncluding Apache Hadoop 简称CDH中小型公司用、简单方便、自带可视化 Hortonworks 文档较好 注：Cloudera 和Hortonworks 在2018年10月，国庆期间宣布合并硬件要求$内存$ 最大支持内存查询：win + R输入 wmic memphysical get maxcapacity计算 MaxCapacity/1024/1024GB 硬盘:500G+$","link":"/post/8a9ad9ba.html"},{"title":"Kafka初始","text":"kafka的设计主要目标 为什么使用消息系统 Kafka架构 Kafka网络拓扑 Kafka的通信过程详解 kafka的设计主要目标$ 以复杂度O(1)的方式提供消息持久化能力，即使对TB级以上的数据也能保证常数的访问性能 高吞吐率，即使在非常廉价的商用机器也能做到单机支持秒100k条消息的传输 支持Kafka Server间消息分区，及分布式消费，同时保证分区内的消息顺序传输 支持离线的数据处理和实时数据处理 支持在线水平扩展 为什么使用消息系统$ 解耦 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可 灵活性和峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证性 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性） 缓冲 消息队列通过一个缓冲层来帮助任务最高效率地执行，写入队列的处理尽可能开始。有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 异步通信 很多时候，用户不想也不需要立即处理信息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但不立即处理，想要队列中放入多少数据就放多少，然后在需要的时候再去处理 Kafka架构$ Kafka集群中将消息以Topic命名的消息队列中，消费者订阅发往某个Topic命名的消息队列queue中的消息其中Kafka集群由若干个Broker组成，Topic中含有多个Partition，每个Partition中通过Offset来获取 Producer：消息生产者，即将消息发布到指定的Topic中，同时Producer也能绝对消息所属的Partition Consumer：消息消费者，即向指定的Topic获取消息，根据指定的Topic的分区索引及其对应分区上的的消息偏移量来获取消息 Consumer Group: 消费者组，每一个Consumer属于一个Consumer Group,每一个Consumer Group包含一个或多个Consumer。如果所有的Consumer都具有相同的Consumer Group，那么消息将会在Consumer之间进行负载均衡，这也是传统的消息系统 “队列”模型。也就是说一个Partition中的消息只会被具有相同groupID的Consumer消费，并且每个Consumer Group之间是相互独立的。如果要实现“发布-订阅”模型，则每个消费 者的消费者组名称都不相同，这样每条消息就会广播给所有的消费者。 Broker:一台Kafka服务器即使一个Broker，一个集群由多个Broker组成，一个Broker可以容纳多个Topic，Broker之间的关系基本是平等的，并不像Hadoop集群那样存在主从模式和为防止单点故障Standby节点 Topic:每条发送到Kafka集群的消息都属于某个Topic。物理上Topic是分开存储的，逻辑上，用户读写数据时并不需要关心他们是存储到哪里的 Partition：Kafka集群为了实现可扩展性，一个非常大的Topic可以分成多个Partition，从而分布到多台Broker中。Partition中的每条消息都会分配有一个自增ID(Offset)。Kafka保证一个Partition内的消息的有序，不保证一个Topic的Partition之间有序 Offset：消息Topic的partition中的位置，同一个Partition中，随着消息的写入，对应Offset自增 Replica：副本。Topic的Partition含有N个副本。其中一个是Replica的Leader，其他的都是Follower，Leader处理partition的读写请求，与此同时，Follower会定期的去同步Leader上的数据 Message：消息，是通信的结伴单位，每个Producer可以向一个Topic发布一些Message Zookeeper：存放Kafka集群的元数据组件。在Zookeeper集群中会保存Topic的状态信息，如分区个数，分区组成，分区分布情况等、保存Broker的状态信息、保存消费者的消费信息等。通过这些信息，Kafka很好地将消息生产、消息存储、消息消费的过程结合起来 Kafka网络拓扑$说明 Producer根据指定的路由方法（Round-Robin、Hash等），将消息Push到Topic中的某个Partition中 Kafka集群收到Producer发来的信息后，将其持久化到硬盘，并保留消息的指定时长（可配置），而不关注消息是否被消费 Consumer从Kafka集群中Pull数据，并控制获取消息的Offset Kafka的通信过程详解$ 先说一下KafkaController,它是Broker内部负责管理分区和副本状态以及异常情况下分区重新分配等功能的模块，每个Kafka集群只有一个KafkaController为Leader其他的为Standby，当一个Leader挂掉后，Zookeeper或选举一个新的KafkaController为Leader 过程详情","link":"/post/df89d65f.html"},{"title":"python可视化","text":"","link":"/post/c0163c11.html"},{"title":"Python/python数据可视化/散点图","text":"","link":"/post/1620326b.html"},{"title":"Kafka通信详情","text":"ProducerRequest:生产者发送消息的请求，生产者将消息发送至Kafka集群中的某个Broker, Broker 接收到此请求后持久化此消息并更新相关元数据信息。（发送消息） TopicMetadataRequest :获取Topic元数据信息的请求，无论是生产者还是消费者都需要通过此请求来获取感兴趣的Topic的元数据。（获取感兴趣Topic元数据） FetchRequest:消费者获取感兴趣Topic的某个分区的消息的请求，除此之外，分区状态为Follower的副本也需要利用此请求去同步分区状态为Leader的对应副本数据。 （获取感兴趣Topic分区消息） OffsetRequest:消费者发送至Kafka集群来获取感兴趣Topic的分区偏移量的请求，通过此请求可以获知当前Topic所有分区在不同时间段的偏移量详情。（获取消费者感兴趣的分区偏移量） OffsetCommitRequest:消费者提交Topic被消费的分区偏移量信息至Broker, Broker接收到此请求后持久化相关偏移量信息。（提交消费了的分区偏移量） OffsetFetchRequest:消费者发送 获取 提交至Kafka集群的相关Topic被消费的 详细信息，和OffsetCommitRequest相互对应。（获取被消费的消息） LeaderAndlsrRequest:当Topic的某个分区状态发生变化时，处于Leader状态的KafkaController发送此请求至相关的Broker,通知其做出相应的处理。 （检测Topic分区变化状态） StopReplicaRequest:当Topic的某个分区被删除或者下线的时候，处于Leader状态的KafkaController发送此请求至相关的Broker,通知其做出相应的处理。（检测分区是否在线） UpdateMetadataRequest:当Topic 的元数据信息发生变化时，处于Leader状态的KafkaContoller发送此请求至相关的Broker,通知其做出相应的处理。（Topic元数据变化） BrokerContolledShutdownRequest:当Broker正常下线时，发生此请求至处于Leader状态的KafnRaCotoller。（Broker下线） ConsumerMetadataRequest: 获取保存特定Consumer Group消费详情的分区信息。（获取CG信息） 交互过程 Producer和Kafka集群: Producer需要利用ProducerRequest和TopicMetadataRequest米完成 Topic元数据的查询、消息的发送。 Consumer和Kafka集群: Consumer需要利用TopicMetadataRequest请求，FetchRequest请求。OffsetRequest 请求、OffsetCommitRequest 请求、fsetFechnRequest 请求和ConsumerMetadataRequest请求来完成Topic元数据的查询、消息的订阅、历史偏移量的查询、偏移量的提交、当前偏移量的查询。 Kafkaontroller状态为Leader的Broker和KafkaController状态为Standby的Broker:KafkaContoller 状态为Leader的Broker需要利用LeaderAndIsrRequest请求、Stop-ReplicaRequest请求，UpdateMetadataRequest 请求来完成对Topic的管理; Kafka-Controller状态为Standby 的Broker需要利用BrokerControlledShutdownRequest请求来通知KafkaContoller状态为Leader的Broker自己的下线动作。 Broker和Broker之间: Broker相互之间需要利用FetchRequest请求来同步Topic分区的副本数据，这样才能使Topic分区各副本数据实时保持一致。","link":"/post/bc0600c8.html"},{"title":"Java NIO实例","text":"概述 源码 Selector的使用步骤 错误总结 概述$ 回显服务器是指接收到客户端的数据，原封不动的返回给客户端 小提示 &nbsp;在阅读这里之前，希望先看一个视频和相关API的介绍，API的介绍不是很详细，当然也可以遇到不知道的API在百度/Google搜索视频：https://www.bilibili.com/video/av57390893?t=5655相关API: https://lyhcc.github.io/post/f2d80d11.html#more 源码$123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/**NIOServer.java**/import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;public class NIOServer { public static void main(String[] args) throws IOException { //1. 打开一个选择器 Selector selector = Selector.open(); //2. 打开一个ServerSocketChannel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //3. 配置异步模式 serverSocketChannel.configureBlocking(false); //4. 绑定到TCP端口上，注意ServerSocketChannel不提供bind方法 //需要使用ServerSocketChannel 内部的socket对象对应的bind方法 serverSocketChannel.socket().bind(new InetSocketAddress(\"127.0.0.1\",12122)); //5. 注册 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //6. 服务器循环，调用Selector.select()方法等待IO事件，如果返回值为0，表明没有事件发生 while (true) { //如果select()带参数，它将不会堵塞到等有感兴趣的数据过来 if (selector.select() == 0) { System.out.println(\"Nothing to do!\"); continue; } //获得连接已选键 Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); //如果是事件是”通道上有请求“ if (key.isAcceptable()) { //相应的处理是通过accept()操作获得SocketChannel对象，并配置对象的异步工作方式 SocketChannel channel = serverSocketChannel.accept(); //设置异步工作模式、注册到选择器中，注册事件为通道可读 SelectionKeys.OP_READ channel.configureBlocking(false); SelectionKey connkey = channel.register(selector, SelectionKey.OP_READ); //根据注册分到的SelectionKey 对象构造连接对象，并将对象作为SelectionKey对象附件 NIOConnection conn = new NIOConnection(connkey); connkey.attach(conn); } //key有效，即通道未关闭，并且为可读的OP_READ if (key.isValid() &amp;&amp; key.isReadable()) { NIOConnection conn = (NIOConnection) key.attachment(); conn.handleRead(); } //key有效，即通道未关闭，并且为可写的 OP_WRITE if (key.isValid() &amp;&amp; key.isWritable()) { NIOConnection conn = (NIOConnection) key.attachment(); conn.handleWrite(); } } } }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/**NIOConnection.java**/import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;public class NIOConnection { private SelectionKey key; private SocketChannel channel; private ByteBuffer buffer; public NIOConnection(SelectionKey key) { this.buffer = ByteBuffer.allocate(1024); this.key = key; this.channel = (SocketChannel) key.channel(); } /** * 读操作 * @throws IOException */ public void handleRead() throws IOException { int byteRead = channel.read(buffer); if (byteRead == -1) { //对方已关闭socket，服务器就将通道关闭 channel.close(); }else { //有数据可读，此时设置感兴趣的I/O事件为读或写， //读出了一部分数据，就说嘛有空间可以写了，当然还有可能有其他数据可读 key.interestOps(SelectionKey.OP_READ | SelectionKey.OP_WRITE); } } public void handleWrite() throws IOException { //要开始读数据了，就得先把数据的开始索引，也就是当前索引position，改为数据的开始位置， //在此之前，得先把limit的改为position,position所在位置是数据的下一个写入位置， // 把限制limit设置为position // 结合上面position ~ limit就是当前的全部数据的位置 buffer.flip(); //开始写出 System.out.println(\"收到的数据：\" + Charset.forName(\"UTF-8\").decode(buffer).toString()); channel.write(buffer); //之后判断是否还有数据存在，如果没数据了，就可以将其设置为只可以读了 if (!buffer.hasRemaining()) { key.interestOps(SelectionKey.OP_READ); } //写完后，有可能还有数据剩余，就将数据移到buffer的最前面 buffer.compact(); } 注意：NIO是没有专门的客户端的，你可以使用Socket进行连接，也可以使用telnet进行连接 Selector的使用步骤$ 创建一个Selector实例: 将该实例注册到各种通道，指定每个通道上感兴趣的I/O操作; 重复执行(选择器循环): 调用一种select()方法; 获取已选键集; 对于已选键集中的每-一个键: 将已选键从键集中移除; 获取信道，并从键中获取附件(如果需要); 确定准备就绪的操作并执行;对于accept操作获得的SocketChannel对象，需将信道设置为非阻塞模式，并将其注册到选择器中; 根据需要，修改键的兴趣操作集。 错误总结$ 这里的connkey用错会报错Exception in thread &quot;main&quot; java.lang.ClassCastException: sun.nio.ch.ServerSocketChannelImpl cannot be cast to java.nio.channels.SocketChannel","link":"/post/93c86522.html"},{"title":"Hadoop 压缩","text":"压缩 Hadoop文件切片 Hadoop压缩 压缩格式选择 Hadoop 压缩案例 压缩$ 压缩是指将数据转换为比原来的格式占用空间更小的格式来存储，以达到减小存储空间解压是压缩的反过程 Hadoop文件切片$ Hadoop MapReduce是通过划分切片来处理得，这样就使得支持分割的压缩格式更适合Hadoop 针对ss.txt文件大小为300M 计算公式computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M（Hadoop 1.x中块大小为64M） 默认情况下，切片大小=blocksize 开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） 注意：切片主要由这几个值来运算决定 12mapreduce.input.fileinputformat.split.minsize=1 默认值为1mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认Long.MAXValue 因此，默认情况下，切片大小=blocksize。 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 Hadoop压缩$ Hadoop作为一个叫通用的海量数据处理平台，在压缩方面主要考虑压缩速度和压缩的可分割性 小提示： 使用gzip压缩文件时 -9表示空间优先，也就是先考虑压缩空间的减小 -1表示时间优先，也就是压缩速度要快 Hadoop支持的压缩格式 压缩格式 工具 算法 扩展名 多文件 可分割性 换成压缩格式后，原来的程序是否需要修改 DEFLATE 无 DEFLATE .deflate 不 不 和文本处理一样，不需要修改 GZIP gzip DEFLATE .gzp 不 不 和文本处理一样，不需要修改 ZIP zip DEFLATE .zip 是 是，在文件范围内 BZIP2 bzip2 BZIP2 .bz2 不 是 和文本处理一样，不需要修改 LZO lzop LZO .lzo 不 是 需要建索引，还需要指定输入格式 压缩算法及其编码/解码器 压缩格式 对应的编码/解码器 DEFLATE org.apache.hadoop.io.compress.DefaultCodec gzip org.apache.hadoop.io.compress.GzipCodec bzip2 org.apache.hadoop.io.compress.BZip2Codec LZO com.hadoop.compression.lzo.LzopCodec Snappy org.apache.hadoop.io.compress.SnappyCodec 性能压缩比较 压缩算法 原始文件大小 压缩文件大小 压缩速度 解压速度 gzip 8.3GB 1.8GB 17.5MB/s 58MB/s bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s LZO 8.3GB 2.9GB 49.3MB/s 74.6MB/s http://google.github.io/snappy/ On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more. 压缩格式选择$Gzip压缩$ 优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。 缺点：不支持split。 应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。 Bzip2压缩$ 优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native(java和c互操作的API接口)；在linux系统下自带bzip2命令，使用方便。 缺点：压缩/解压速度慢；不支持native。 应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。 Lzo压缩$ 优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。 缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。 应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。 Snappy压缩$ 优点：高速压缩速度和合理的压缩率。 缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装； 应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。 Hadoop 压缩案例$12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Demo { public static void main(String[] args) throws IOException, ClassNotFoundException { //压缩为bz2格式 compress(\"org.apache.hadoop.io.compress.BZip2Codec\"); } private static void compress(String method) throws ClassNotFoundException, IOException { //1. 获取resources下的资源文件流 InputStream in = Demo.class.getClassLoader().getResourceAsStream(\"properties.xml\"); //2. 通过Java反射机制创建对应得编码名称 Class&lt;?&gt; codeClass = Class.forName(method); //3. 通过编码名称找对应得编码/解码器 Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codeClass, conf); //4. 指定压缩后的文件,codec.getDefaultExtension()获得相应的扩展名 File fileOut = new File(System.currentTimeMillis() + codec.getDefaultExtension()); //如果文件存在，删除，否则什么也不做 fileOut.delete(); //5. 创建输出流 FileOutputStream out = new FileOutputStream(fileOut); //6. 通过编码/解码器创建对应得输出流 CompressionOutputStream cout = codec.createOutputStream(out); //7. 压缩输出 /** * in 输入流 * cout 压缩输出流 * 1024 缓冲大小 * false 不关闭相应流，true则关闭 */ IOUtils.copyBytes(in,cout,4096,false); in.close(); cout.close(); } /** * 解压缩 * @param file * @throws IOException */ private static void decompress(File file) throws IOException { Configuration conf = new Configuration(); CompressionCodecFactory factory = new CompressionCodecFactory(conf); //通过扩展名获得编码/解码器 CompressionCodec codec = factory.getCodec(new Path(file.getName())); //通过编码解码器获得输入流 CompressionInputStream in = codec.createInputStream(new FileInputStream(file)); IOUtils.copyBytes(in, System.out, 4096, true); }} 说明：snappy压缩格式在Windows运行失败，不过打包放到集群里面是可以的，前提是你的hadoop集群支持snappy压缩格式","link":"/post/420bf138.html"},{"title":"MapReduce介绍","text":"MapReduce的定义 MapReduce优缺点 优点 缺点 MapReduce核心思想 MapReduce进程 MapReduce编程规范 第一代和第二代MapReduce的区别 MapReduce的定义$ &emsp;&emsp;Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。&emsp;&emsp;Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 MapReduce优缺点$优点$ MapReduce 易于编程。它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 良好的扩展性。当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 高容错性。MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 适合PB级以上海量数据的离线处理。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。缺点$ MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 MapReduce核心思想$ 分布式的运算程序往往需要分成至少2个阶段。第一个阶段的maptask并发实例，完全并行运行，互不相干。 第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 MapReduce进程$ 一个完整的mapreduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调。 MapTask：负责map阶段的整个数据处理流程。 ReduceTask：负责reduce阶段的整个数据处理流程。 MapReduce编程规范$ 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper阶段 12345（1）用户自定义的Mapper要继承自己的父类 （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） （3）Mapper中的业务逻辑写在map()方法中 （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） （5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 Reduce阶段 1234（1）用户自定义的Reducer要继承自己的父类 （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV （3）Reducer的业务逻辑写在reduce()方法中 （4）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 第一代和第二代MapReduce的区别$","link":"/post/5429.html"},{"title":"hadoop序列化","text":"序列化介绍 Java序列化 Hadoop 不使用Java序列化原因 Hadoop 序列化 Hadoop序列化机制的特征 Hadoop Writable机制 Hadoop序列化的其它几个接口 Hadoop 序列化的类 序列化介绍$ 序列化是一种将对象的状态信息转化成可以存储或者传输的过程，与之相反的为反序列化不是某一种编程语言所独有的特性序列化的用途 作为一种持久化格式。对象序列化后存盘 作为一种通信的数据格式。如虚拟机之间通信 作为一种拷贝、克隆机制。放缓存 Java序列化$ Java通过实现Serializable接口Java序列化后放入对象，通过对象流进行IO操作，ObjectInputStream/ObjectOutputStream 1234567import java. io.Serializable ;／＊＊定义一个可以序列化的 App 信息类. */public class Appinfo implements Serializable{ ／／序列化标识 private static final long serialVersionUID = 11 ;} Hadoop 不使用Java序列化原因$ Java 自带的序列化机制占用内存空间大，额外的开销会导致速度降低，Hadoop对序列化的要求较高，需要保证序列化速度快、体积小、占用带宽低等特性 Hadoop 序列化机制是将对象序列化到流中，而 Java 序列化机制是不断创建新对象，对于MapReduce应用来说，不能重用对象 Java序列化在反序列化时，有可能需要访问前一个数据，这将导致数据无法分割来通过MapReduce来处理 Hadoop 序列化$ 在 Hadoop 序列化机制中，org.apache.hadoop.io包中定义了大量的可序列化对象 Hadoop 序列化机制通过调用write方法（它带有一个类型为DataOutput的参数），将对象序列化到流中 Hadoop 反序列化通过对象的readFields从流中读取数据 Hadoop序列化机制的特征$ 对于处理大数据的Hadoop平台，其序列化需要具备以下特征 紧凑。这样可以充分利用Hadoop集群的资源，hadoop集群中最稀缺的是资源 快速。进程通信时会大量使用序列化机制，因此需要减少序列化开销 可扩展性。为适应发展，序列化机制也需要支持这些升级和变化 互操作。支持不同语言开发 Hadoop Writable机制$ Hadoop序列化都必须实现该接口 均实现Wriable接口的两个函数， 12(1) write：将对象写入字节流：(2) readFields：从字节流中解析出对象。例子 123456789101112131415161718192021222324252627282930313233343536373839/** * BlockWritable有三个对象， * write方法将三个对象写到流中 * readFields从流中读出三个对象 */public class BlockWritable implements Writable { private long blockId; private long numBytes; private long generationStamp; /** * 输出序列化对象到流中 * @param out * @throws IOException */ @Override public void write(DataOutput out) throws IOException { out.writeLong(this.blockId); out.writeLong(this.numBytes); out.writeLong(this.generationStamp); } /** * 从流中读取序列化对象 * 为了效率，尽可能复用现有对象 * @param in 从该流中读取数据 * @throws IOException */ @Override public void readFields(DataInput in) throws IOException { this.blockId = in.readLong(); this.numBytes = in.readLong(); this.generationStamp = in.readLong(); if (this.numBytes &lt; 0L) { throw new IOException(&quot;Unexpected block size: &quot; + this.numBytes); } }} Hadoop序列化的其它几个接口$ WritableComparable RawComparator RawComparator允许执行者 比较 流中读取的未被反序列化为对象的 记录，从而省去创建对象所带来的开销 1234567891011/** * * @param var1 字节数组1 * @param var2 字节数组1的开始位置 * @param var3 字节数组1的记录长度 * @param var4 字节数组2 * @param var5 字节数组2的开始位置 * @param var6 字节数组2的记录长度 * @return */int compare(byte[] var1, int var2, int var3, byte[] var4, int var5, int var6); WritableComparator 在RawComparator中WritableComparator是个辅助类，实现了RawComparator接口 以DoubleWritable为例 1234567891011public static class Comparator extends WritableComparator { public Comparator() { super(DoubleWritable.class); } public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) { double thisValue = readDouble(b1, s1); double thatValue = readDouble(b2, s2); return thisValue &lt; thatValue ? -1 : (thisValue == thatValue ? 0 : 1); } } WritableComparator是RawComparator对WritableComparable类的一一个通用实现。提供两个主要功能。首先，提供了一个RawComparator的compare()默认实现，该实现从数据流中反序列化要进行比较的对象，然后调用对象的compare()方法进行比较(这些对象都是Comparable的)。其次，它充当了RawComparator实例的一个工厂方法,通过DoubleWritable获得RawComparator的代码如下 1RawComparator&lt;DoubleWritable&gt; comparator = WritableComparator.get(DoubleWritable.class); RawComparator和WritableComparable的类图 Hadoop 序列化的类$java基本类型的封装$ 说明： 这些类实现了WritableComparable接口 VIntWritable和VLongWritable是只可变长 可变长的格式更空间 VIntWritable可用VLongWritable读入 变长整型分析 writeVLong ()方法实现了对整型数值的变长编码，它的编码规则如下:&emsp;&emsp;如果输入的整数大于或等于-112同时小于或等于127，那么编码需要1字节:否则，序列化结果的第一个字节，保存了输入整数的符号和后续编码的字节数。符号和后续字节数依据下面的编码规则(又一个规则): 如果是正数，则编码值范围落在-113和-120间(闭区间)，后续字节数可以通过-(v+112)计算。 如果是负数，则编码值范围落在-121和-128间(闭区间)，后续字节数可以通过-(v+120)计算。 后续编码将高位在前，写入输入的整数(除去前面全0字节)。代码如下: 1234567891011121314151617181920212223242526272829public static void writeVInt(DataOutput stream, int i) throws IOException { writeVLong(stream, (long)i); } public static void writeVLong(DataOutput stream, long i) throws IOException { if (i &gt;= -112L &amp;&amp; i &lt;= 127L) { stream.writeByte((byte)((int)i)); } else { int len = -112; if (i &lt; 0L) { i = ~i; len = -120; } for(long tmp = i; tmp != 0L; --len) { tmp &gt;&gt;= 8; } stream.writeByte((byte)len); len = len &lt; -120 ? -(len + 120) : -(len + 112); //后续编码 for(int idx = len; idx != 0; --idx) { int shiftbits = (idx - 1) * 8; long mask = 255L &lt;&lt; shiftbits; stream.writeByte((byte)((int)((i &amp; mask) &gt;&gt; shiftbits))); } } } ObjectWritable$ 针对Java基本类型、字符串、枚举、Writable、空值、Writable的其 他子类,ObjectWritable提供了一个封装，适用于字段需要使用多种类型。ObjectWritable 可应用于Hadoop远程过程调用中参数的序列化和反序列化; ObjectWritable 的另一个典型应用是在需要序列化不同类型的对象到某-个字段，如在一个SequenceFile 的值中保存不同类型的对象( 如LongWritable值或Text值)时，可以将该值声明为ObjectWritable。 ObjectWritable的实现比较冗长，需要根据可能被封装在ObjectWritable中的各种对象进行不同的处理。ObjectWritable 有三个成员变量，包括被封装的对象实例instance、该对象运行时类的Class对象和Configuration对象。 123private Class declaredClass;private Object instance;private Configuration conf; ObjectWritable的write 方法调用的是静态方法ObjectWritable.writeObject()，该方法可以往DataOutput接口中写入各种Java对象。 writeObject()方法先输出对象的类名(通过对象对应的Class对象的getName()方法获得)， 1UTF8.writeString(out, declaredClass.getName()); 然后根据传入对象的类型，分情况系列化对象到输出流中，也就是说，对象通过该方法输出对象的类名，对象序列化结果对到输出流中。在ObjectWritable. writeObject(的逻辑中，需要分别处理null Java 数组、字符串String、Java 基本类型、枚举和Writable的子类6种情况，由于类的继承，处理Writable时，序列化的结果包含对象类名，对象实际类名和对象序列化结果三部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public void write(DataOutput out) throws IOException { writeObject(out, this.instance, this.declaredClass, this.conf); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf) throws IOException { writeObject(out, instance, declaredClass, conf, false); } public static void writeObject(DataOutput out, Object instance, Class declaredClass, Configuration conf, boolean allowCompactArrays) throws IOException { if (instance == null) { instance = new ObjectWritable.NullInstance(declaredClass, conf); declaredClass = Writable.class; } if (allowCompactArrays &amp;&amp; declaredClass.isArray() &amp;&amp; instance.getClass().getName().equals(declaredClass.getName()) &amp;&amp; instance.getClass().getComponentType().isPrimitive()) { instance = new Internal(instance); declaredClass = Internal.class; } UTF8.writeString(out, declaredClass.getName()); /****************此处****************/ if (declaredClass.isArray()) { int length = Array.getLength(instance); out.writeInt(length); for(int i = 0; i &lt; length; ++i) { writeObject(out, Array.get(instance, i), declaredClass.getComponentType(), conf, allowCompactArrays); } } else if (declaredClass == Internal.class) { ((Internal)instance).write(out); } else if (declaredClass == String.class) { UTF8.writeString(out, (String)instance); } else if (declaredClass.isPrimitive()) { if (declaredClass == Boolean.TYPE) { out.writeBoolean((Boolean)instance); } else if (declaredClass == Character.TYPE) { out.writeChar((Character)instance); } else if (declaredClass == Byte.TYPE) { out.writeByte((Byte)instance); } else if (declaredClass == Short.TYPE) { out.writeShort((Short)instance); } else if (declaredClass == Integer.TYPE) { out.writeInt((Integer)instance); } else if (declaredClass == Long.TYPE) { out.writeLong((Long)instance); } else if (declaredClass == Float.TYPE) { out.writeFloat((Float)instance); } else if (declaredClass == Double.TYPE) { out.writeDouble((Double)instance); } else if (declaredClass != Void.TYPE) { throw new IllegalArgumentException(\"Not a primitive: \" + declaredClass); } } else if (declaredClass.isEnum()) { UTF8.writeString(out, ((Enum)instance).name()); } else if (Writable.class.isAssignableFrom(declaredClass)) { UTF8.writeString(out, instance.getClass().getName()); ((Writable)instance).write(out); } else { if (!Message.class.isAssignableFrom(declaredClass)) { throw new IOException(\"Can't write: \" + instance + \" as \" + declaredClass); } ((Message)instance).writeDelimitedTo(DataOutputOutputStream.constructOutputStream(out)); } } 和输出对应，ObjectWritable 的readFields()方法调用的是静态方法ObjectWritable.readObject()，该方法的实现和writeObject()类似，唯一值得研究的是Writable对象处理部分，readObject ()方法依赖于WritableFactories类。WritableFactories 类允许非公有的Writable子类定义一一个对象工厂，由该工厂创建Writable对象，如在上面的readObject()代码中，通过WritableFactories的静态方法newInstance()，可以创建类型为instanceClass的Writable子对象。具体查看org.apache.hadoop.io.WritableFactories类 注：ObjectWritable它比较浪费资源，可以使用静态数组来记录数据类型以提高效率 Hadoop序列化优势 1231. 减少垃圾回收：从流中反序列化数据到当前对象，重复使用当前对象，减少了垃圾回收GC ;2. 减少网络流量 ： 序列化和反序列化对象类型不变 ，因此可以只保存必要的数据来减少网络流量；3. 提升 I/O 效率 ： 由于序列化和反序列化的数据量减少了，配合Hadoop压缩机制，可以提升I/O效率。","link":"/post/20979.html"},{"title":"Hadoop通信机制和内部协议之协议","text":"概述 ClientProtocol通信协议 RefreshUserMappingProtocol RefreshAuthorizationPolicyProtocol ResourceManagerAdministrationProtocol 概述$MapReduce核心协议 名称 描述 ClientProtocol 继承于Version基类，查看作业情况监控当前集群等 RefreshUserMappingProtocol 刷新用户到用户组映射关系到超级用户代理组列表 RefreshAuthorizationPolicyProtocol 刷新HDFS和MapReduce服务几倍访问控制列表 ResourceManagerAdministrationProtocol 继承于GetUserMappingProtocol基类，刷新队列列表，节点列表 ## ClientProtocol通信协议 ClientProtocol协议是JobClient和JobTracker之间进行交流的枢纽。JobClient 可以使用该协议中的函数来提交-一个作业(Job) 并执行，以了解当前系统的状态 提交作业协议中JobClient通过Hadoop RPC的submitjob()函数提交作业(Job)，函数所包含的参数有作业ID (JobID)，然后JobClient通过getNewJoblD0函数为作业(Job) 获得一个唯一的ID。 操作作业当用户提交作业(Job) 后，可以通过调用函数来控制该作业的执行流程，如设置提交作业的优先级(setlobPriority()函数) 、停止一个作业(killJob()函数) 、停止一个任务(illTask()函数)。 查看状态从实现源代码来看，该通信协议还提供了一系列函数来 查看状态，如查看集群当前状态(getClusterMetrics()函数)、查看当前任务状态(getJobTrackerStatus()函数) 、获取所有任务(getllobs()函数)等。 RefreshUserMappingProtocol$ RefreshU serMappingsProtocol 协议用于更新 HDFS 和 MapReduce 级别的用户到用户组映射关系及超级用户代理组列表 refreshUserToGroupsMappings() 函数和refreshSuperUserGroupsConfiguration()函数来实现，这两个函数均是通过调用Hadoop RPC来完成具体的逻辑。 RefreshAuthorizationPolicyProtocol$ RefreshAuthorizationPol icyProtocol 协议用于刷新当前使用的授权策略 通过调用 Hadoop RPC 远程调用 refreshServiceAcl（）函数，实现基于 HDFS 和MapReduce 级别的授权策略 ResourceManagerAdministrationProtocol$ ResourceManagerAdministrationProtocol 协议用于更新队列列表、节点 列表 、节点资源等 该协议继承于 GetUserMappingsProtocol 基类 ，通过 Hadoop RPC 远程调用来实现节点更新、资源更新 、添加标签等操作 说明：在IDE中导入hadoop源码加载进去后，按Ctrl+鼠标左键进入即可查看源码","link":"/post/13550.html"},{"title":"配置文件","text":"Windows操作系统配置文件 Java配置文件 Hadoop Configuration详解 hadoop 配置文件格式 Configuration类介绍 Configuration类的过程 Configurable接口 Windows操作系统配置文件$ 配置设置文件（INI）文件是windows操作系统中的一种特殊的ASCII文件，以ini为文件扩展名,作为它的主要文件配置文件标准。该文件也被称为初始化文件initialization file和概要文件profile。应用程序可以拥有自己的配置文件，存储应用设置信息，也可以访问windows的基本系统配置文件win.ini中存储的配置信息 INI配置信息分为两部分 节，节标题放在方括号中, [section] 项，一个等式，key=value 1234567;注释;节 [section] ;参数（键=值） name=value INI文件片段 1234[0x0419]1100=Ошибка инициализации программы установки1101=%s1102=%1 Идет подготовка к запуску мастера %2, выполняющего установку программы. Ждите. Windows 提供的API 12345678910111213141516DWORD GetPrivateProfileString( LPCTSTR lpAppName, // If this parameter is NULL, the GetPrivateProfileString function copies all section names in the file to the supplied buffer. LPCTSTR lpKeyName, // If this parameter is NULL, all key names in the section specified by the lpAppNameparameter are copied to the buffer specified by the lpReturnedString parameter. LPCTSTR lpDefault, // If the lpKeyName key cannot be found in the initialization file, GetPrivateProfileString copies the default string to the lpReturnedString buffer. LPTSTR lpReturnedString, // destination buffer DWORD nSize, // size of destination buffer LPCTSTR lpFileName // The name of the initialization file);UINT GetPrivateProfileInt( LPCTSTR lpAppName, //节 LPCTSTR lpKeyName,//项 INT nDefault, //The default value to return if the key name cannot be found in the initialization file. LPCTSTR lpFileName //INI文件名); Java配置文件$ JDK提供了java.util.Properties类，用于处理简单的配置文件。Properties继承自Hashtable相对于INI文件，Properties处理得配合文件格式非常简单 Properties的使用 非XML文件格式12345678//通过指定的键搜索属性public String getProperty(String key)//功能同上，参数defaultValue提供了默认值public String getProperty(String key, String defaultValue)//最终调用Hashtable 的方法putpublic synchronized object setProperty (String key, String value) Properties中的属性通过load)方法加载，该方法从输入流中读取键-值对，而store()方法法则将Properties表中的属性列表写入输出流。使用输入流和输出流，Properties对象但可以保存在文件中，而且还可以保存在其他支持流的系统中，如Web服务器。 123456789101112131415161718192021222324/** * log4j.properties内容如下 * log4j.rootLogger=INFO, stdout * log4j.appender.stdout=org.apache.log4j.ConsoleAppender * log4j.appender.stdout.layout=org.apache.log4j.PatternLayout * log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n * log4j.appender.logfile=org.apache.log4j.FileAppender * log4j.appender.logfile.File=target/spring.log * log4j.appender.logfile.layout=org.apache.log4j.PatternLayout * log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n * @param args * @throws IOException */ public static void main(String[] args) throws IOException { Properties properties = new Properties(); //获取resources目录下的文件流 InputStream stream = MyConfiguration.class.getClassLoader().getResourceAsStream(\"log4j.properties\"); //加载文件获取并获取配合信息 properties.load(stream); String property = properties.getProperty(\"log4j.rootLogger\"); System.out.println(property); /*输出： INFO, stdout */ } Java 1.5之后支持XML配置文件,Properties中的数据也可以以XML格式保存，对应的加载和写出方法是loadFromXML()和storeToXML() storeToXML() 12345Properties props = new Properties();props.setProperty(\"Length\", \"100\");props.setProperty(\"Width\", \"50\")FileOutputStream fos = new FileOutputStream(\"properties.xml\");props.storeToXML(fos, null); loadFromXML() 1234567Properties props = new Properties();InputStream in = MyConfiguaration2.class.getClassLoader().getResourceAsStream(\"properties.xml\");props.loadFromXML(in)String length = props.getProperty(\"Length\");System.out.println(length); xml有指定格式 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;entry key=\"Width\"&gt;50&lt;/entry&gt; &lt;entry key=\"Length\"&gt;100&lt;/entry&gt;&lt;/properties&gt; java.util.Properties提供的能力有限，其他配置信息读写方法，如Apache Jakarta Commons工具集提供的Commons Configuration 【说明】：上面的MyConfiguration都是自定义的类,并且这些配置文件都在resources文件夹下 Hadoop Configuration详解$ Hadoop没有使用java.util.Properties管理配置文件，也没有使用Apache JakartaCommons Configuration 管理配置文件，而是使用了一套独有的配置文件管理系统，并提供自己的API，即使用org.apache.hadoop.conf.Configuration处理配置信息。 hadoop 配置文件格式$12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.9.2/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hadoop 配置文件参数说明$配置参数类型说明 参数名 String 参数值 boolean int long float，也可以是其他类型 参数说明 根元素 configuration configuration下的property元素 property下 name 参数名 value 参数值 description 参数描述 final 相当于java的final关键字，在资源合并时可以防止配置项被覆盖 合并资源$ 合并资源是是指将多个配置合并，产生一个配置文件，如core-site.xml和core-defualt.xml通过addResources()方法合并 1234567Configuration conf = new Configuration();//加载resources文件夹内容ClassLoader classLoader = HadoopConfiguaration.class.getClassLoader//添加合并资源conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-site.xml\")));conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-default.xml\")System.out.println(conf.get(\"fs.defaultFS\")); 【注意】如果第一个配置中存在final，则会以下出现警告,并且值不发生更改 1232019-11-12 17:31:34,548 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@cac736f:an attempt to override final parameter: fs.defaultFS; Ignoring. 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@1d7acb34:an attempt to override final parameter: fs.defaultFS; Ignoring. 测试用的core-default.xml文件 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://vmaster:8200&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.3.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;123&lt;/name&gt; &lt;value&gt;wer&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Configuation类的一般过程 构造Configuration对象 添加需要加载的资源 addResource()方法 然后通过set/get 方法访问/设置配置项 【说明】资源会在第一次使用时自动加载到对象中 Configuration类介绍$类图$ 说明$类图中，Configuration有7个非静态成员 布尔变量quietmode，用来设置加载配置的模式。如果quietmode为true (默认值)，则在加载解析配置文件的过程中，不输出日志信息。quietmode只是-一个方便开发人员调试的变量。 数组resources保存了所有通过addResource()方法添加Configuration对象的资源 Configuration.addResource()有如下4种形式: 1234public void addResource (InputStream in) //已打开的输入流public void addResource (Path file) //Hadoop文件路径public void addResource (String name) //CLASSPATH 资源 String形式public void addResource (URL url) //URL,统一资源定位符，如https://lyhcc.github.io 布尔变量loadDefaults用于确定是否加载默认资源，这些默认资源保存在defaultResources中。注意，defaultResources 是个静态成员变量，通过方法addDefaultResource()可以添加系统的默认资源。在HDFS中，会把hdfs-default.xml和hdfs-site.xml作为默认资源，并通过addDefaultResource()保存在成员变量defaultResources中;在MapReduce中，默认资源是mapred-default.xml和mapred-site.xml。1234567//下面的代码来自hadoop-1.x 的org.apache.hadoop.hdfs.server.datanode.DataNode static{ Configuration . addDe faultResource (\"hdfs-default . xml\") ; Conf igurat ion. addDe faultResource (\"hdfs-site. xml\") ;}//在hadoop2.x以后这代码被移到了Configuration类里面//hadoop-2.8.4中的1809行 properties 存放Hadoop配置文件解析后的键-值对，为java.util.Properties类型 finalParameters 类型是Set， 用来保存所有在配置文件中已经被声明为final的键-值对的键 overlay用于记录通过set()方式改变的配置项。也就是说，出现在overlay中的键-值对是应用设置的，而不是通过对配置资源解析得到的，为java.util.Properties类型 Configuration 是一一个类加载器变量，可以通过它来加载指定类，也可以通过它加载相关的资源。 上面提到addResource()可以通过字符串方式加载CLASSPATH资源，它其实通过Configuration中的getResource()将字符串转换成URL资源 123public URL getResource (String name){ return classLoader.getResource(name) ;} 2.8.4版本的Configuration类的1188行 Configuration类的过程$构造Configuration对象$资源加载$添加资源到Configuration对象的方法有两种 对象的addResource()方法 类的静态addDefaultResource()方法(设置了loadDefaults标志) 添加的资源并不会立即被加载，只是通过reloadConfiguration()方法清空properties和finalParameters。相关代码如下: 123456789101112131415 //以URL资源为例public void addResource(URL url) { this.addResourceObject(new Configuration.Resource(url)); } //添加资源 private synchronized void addResourceObject(Configuration.Resource resource){ this.resources.add(resource); this.restrictSystemProps |= resource.isParserRestricted(); this.reloadConfiguration(); } //资源重新加载触发函数 public synchronized void reloadConfiguration() { this.properties = null; this.finalParameters.clear(); } 以上是类的成员方法addResource()方法的调用。 静态方法dDefaultResource()也可以清空Configuration对象中的数据（非静态成员），只不过是通过需要通过REGISTRY作为媒介进行。能够调用是因为REGISTRY记录了系统所有的Configuration对象REGISTRY的定义以及为其添加参数的过程 12345678private static final WeakHashMap&lt;Configuration, Object&gt; REGISTRY = new WeakHashMap();public Configuration(boolean loadDefaults) { ... synchronized(Configuration.class) { REGISTRY.put(this, (Object)null); }} 成员变量properties中的数据只在被调用的时候才会被加载进来。在getProps方法中，properties为空时，会触发loadResources()执行 123456789101112131415161718192021222324protected synchronized Properties getProps() { if (this.properties == null) { this.properties = new Properties(); Map&lt;String, String[]&gt; backup = new ConcurrentHashMap(this.updatingResource); this.loadResources(this.properties, this.resources, this.quietmode); ... } return this.properties;}//加载默认资源private void loadResources(Properties properties, ArrayList&lt;Configuration.Resource&gt; resources, boolean quiet) { if (this.loadDefaults) { Iterator i$ = defaultResources.iterator(); while(i$.hasNext()) { String resource = (String)i$.next(); this.loadResource(properties, new Configuration.Resource(resource, false), quiet); } if (this.getResource(\"hadoop-site.xml\") != null) { this.loadResource(properties, new Configuration.Resource(\"hadoop-site.xml\", false), quiet); }} hadoop配置文件解析$ hadoop 的配置文件都是XML文件，JAXP(JAVA API for XML processing)是一种稳定的、可靠的XML处理API，支持两种XML处理方法 SAX解析(Simple API for XML) DOM解析(Documnet Object Model) hadoop使用的DOM解析 两种解析的区别 SAX 提供了一种流式的、事件驱动的XML处理方式 缺点：编写处理逻辑比较复杂 优势：适合处理大的XML文件 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档 Configurable接口$ Configurable是一个很简单的接口，位于org.apache.hadoop.conf包中 类图 hadoop 代码中存在大量实现了该接口的类，可以通过setConf方法设置配置参数简化创建和setConf的两个步骤java反射机制实现，利用org.apache.hadoop.util.ReflectionUtils的newInstance方法 1public static &lt;T&gt; T newInstance(Class&lt;T&gt; theClass, Configuration conf) 该方法调用了ReflectionUtils中的setConf方法 12345678910public static void setConf(Object theObject, Configuration conf) { if (conf != null) { if (theObject instanceof Configurable) { ((Configurable)theObject).setConf(conf); } setJobConf(theObject, conf); } }","link":"/post/fa571a7.html"},{"title":"Hadoop通信机制和内部协议之RPC","text":"Hadoop RPC RPC简介 RPC模型 RPC特性 RPC例子 其他开源RPC架构 Hadoop RPC$RPC简介$ 简要地说，RPC就是允许程序调用位于其他机器上的过程(也可以是同一台机器的不同进程)。RPC调用过程是透明的 传统过程调用：传统的过程调用中，主程序将参数压人栈内并调用过程，这时候主程序停止执行并开始执行相应的过程。被调用的过程从栈中获取参数，然后执行过程函数;执行完毕后，将返回参数入栈(或者保存在寄存器里)，并将控制权交还给调用方。调用方获取返回参数，并继续执行。 而RPC调用是进程间的过程调用 RPC模型$ 通行模块： 请求-响应 Stub程序： 用于保证RPC的透明性。在客户端，不在本地调用，而是将请求信息通过网络模块发送给法服务器端，服务器接收后进行解码。服务器中，Stub程序依次进行 解码（请求的参数）、调用相应的服务过程、编码返回结果等处理 调度程序： 调度来自通行模块的请求信息，根据其中标识选一个Stub程序运行 客户程序： 请求发出者 服务过程： 请求接收者 一个RPC的旅游： 客户端以本地调用方式产生本地Stub程序 该Stub程序将函数调用信息按照网络通信模块的要求封装成消息包，并交给通信模块发送到远程服务器端。 远程服务器端接收此消息后，将此消息发送给相应的Stub程序 Stub程序拆封消息，形成被调过程要求的形式，并调用对应函数 服务端执行被调用函数，并将结果返回给Stub程序 Stub程序将此结果封装成消息，通过网络通信模块逐级地传送给客户程序。 RPC特性$ 透明性 调用过程就像本地调用，察觉不到它的经历 高性能 ：Hadoop各个系统（如HDFS、MapReduce、YARN等）均采用了Master/Slave结构，其中，Master实际上是一个RPC server，它负责响应集群中所有Slave发送的服务请求。RPC Server性能要求高，为的是能够让多个客户端并发方位 易用性/可控性 Hadoop系统不采用Java内嵌的RPC（RMI,Remote Method Invocation）框架的主要原因是RPC是Hadoop底层核心模块之一，需要满足易用性、高性能、轻量级等特性 RPC例子$执行过程： CalculateClient对象本地调用产生Stub程序 经通信模块上传至服务器CalculateServer对象，在创建Server时设置了协议和业务逻辑（服务过程），处理过后根据上述RPC过程返回 客户端接收后打印到日志中 先定义一些常量$ 这里不需要太多的在意，直接使用在代码里面也行，在大的项目中为了使程序易于修改而这样设置 1234567891011/** * 静态变量声明类 */public interface Constants { public interface VersionID { public static final long RPC_VERSION = 7788L; } public static final String RPC_HOST = &quot;127.0.0.1&quot;; public static final int RPC_PORT = 8888;} 定义一个Service接口，协议类$12345678910111213import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.ipc.ProtocolInfo;@ProtocolInfo(protocolName = &quot;&quot;, protocolVersion = Constants.VersionID.RPC_VERSION)public interface CalculateService { //真实业务逻辑，加减法， public IntWritable add(IntWritable a, IntWritable b); public IntWritable sub(IntWritable a, IntWritable b); public Text echo(Text mt);} @ProtocolInfo(protocolName = “”, protocolVersion = Constants.VersionID.RPC_VERSION) 没有这句就不能将该类设置为协议，不过也可以通过继承VersionProtocol接口 Service接口的实现类$1234567891011121314151617181920212223242526272829303132333435import java.io.IOException;public class CalculateServiceImpl implements CalculateService { /** * 该方法没有也行 * */ public ProtocolSignature getProtocolSignature(String arg0, long arg1, int arg2) throws IOException{ return this.getProtocolSignature(arg0, arg1, arg2); } /** * 校验hadoop RFC版本号 * @param arg0 * @param arg1 * @return */ public long getProtocolVersion(String arg0, long arg1) throws IOException { return Constants.VersionID.RPC_VERSION; } @Override public IntWritable add(IntWritable a, IntWritable b) { return new IntWritable(a.get() + b.get()); } @Override public IntWritable sub(IntWritable a, IntWritable b) { return new IntWritable(a.get() - b.get()); } @Override public Text echo(Text mt) { return mt; }} Server和Client类$123456789101112131415161718192021222324252627282930import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;public class CalculateServer { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { try { //构造Server,并设置协议接口，主机、端口，真实业务逻辑 RPC.Server server = new RPC.Builder(new Configuration()) .setProtocol(CalculateService.class) .setBindAddress(Constants.RPC_HOST) .setPort(Constants.RPC_PORT) .setInstance(new CalculateServiceImpl()) .build(); //启动Server server.start(); LOG.info(&quot;Server has Started!&quot;); } catch (IOException e) { LOG.error(&quot;Server has Error&quot;); } }} 123456789101112131415161718192021222324252627282930313233343536import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.net.InetSocketAddress;public class CalculateClient { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { //格式化IP和端口 InetSocketAddress addr = new InetSocketAddress(Constants.RPC_HOST, Constants.RPC_PORT); //校验Hadoop RPC版本号 long protocolVersion = RPC.getProtocolVersion(CalculateService.class); try { //获取Server连接 CalculateService proxy = RPC.getProxy(CalculateService.class, protocolVersion, addr, new Configuration()); IntWritable add = proxy.add(new IntWritable(1), new IntWritable(2)); IntWritable sub = proxy.add(new IntWritable(3), new IntWritable(2)); LOG.info(&quot;1+2 = &quot; + add); LOG.info(&quot;3-2 = &quot; + sub); } catch (IOException e) { LOG.error(&quot;Client has error!&quot;); } }} 注意： 查看本程序运行结果需要一个日志文件，如果不想加，把LOG的相关语句换为打印输出就行在resource文件夹下创建 log4j.properties 12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 客户端运行结果$12342019-10-31 18:59:28,499 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-10-31 18:59:28,619 WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 1+2 = 3 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 3-2 = 5 其他开源RPC架构$ Java RMI Apache Thrift Google Protocol Buffer","link":"/post/56192.html"},{"title":"HDFS快照管理","text":"快照管理 快照影响 基本语法 案例实操 快照管理$ 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 快照影响$ 快照创建瞬间完成，所耗时间成本为O(1) 快照修改时才会使用额外的额外的内存空间，内存成本O(M),M表示修改过的文件或目录数 快照记录块和文件大小，不对DataNode中的块进行复制 说明：* 快照可以在HDFS任何目录下设置，一个目录最多容纳65536个并发快照 基本语法$12345678（1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能）（2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）（3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照）（4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照）（5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）（6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有已快照目录）（7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处）（8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 案例实操$ 开启/禁用指定目录的快照功能 指定创建目录的位置为 /tmp/snapshot(即快照的存储目录)，在指定目录之前必须创建目录，不然会报错 12hdfs dfsadmin -allowSnapshot /tmp/snapshot hdfs dfsadmin -disallowSnapshot /tmp/snapshot //禁用时，对应的目录不允许存在快照 对目录创建快照 只有被开启快照功能的目录才能创建快照 123hdfs dfs -createSnapshot /tmp/snapshot // 对目录创建快照hdfs dfs -createSnapshot /tmp/snapshot snapshot //重命名快照（注：快照是只读的，无法修改名）通过web访问hdfs://Master:9000/tmp/snapshot/.snapshot/s…..// 快照和源文件使用相同数据块 查看快照 12hdfs dfs -lsr /tmp/snapshot/.snapshot/ //查看快照目录的详细信息hdfs lsSnapshottableDir //查看所有允许快照的目录 更改快照名字 12345hdfs dfs -renameSnapshot /tmp/snapshot/ snapshot snapshot1 注：路径只是你创建得名字/tmp/snapshot，不要带后边得/tmp/snapshot/.snapshot/，不然会出现hdfs dfs -renameSnapshot /tmp/snapshot/.snapshot/ snapshot1 snapshotrenameSnapshot: Modification on a read-only snapshot is disallowed 比较两个快照目录的不同之处 1234[root@vmaster opt]# hdfs snapshotDiff /tmp/snapshot s1 s2Difference between snapshot s1 and snapshot s2 under directory /tmp/snapshot:M .+ ./p 符号的意义： 符号 含义 + 文件或者目录被创建 - 文件或目录被删除 M 文件或目录被修改 R 文件或目录被重命名 恢复快照 123451.自定义创建一个快照名：hdfs dfs -createSnapshot /HAHA1 snapshot12.展示原文件包含内容：Hadoop fs -ls /HAHA13.里面有五个文件、删除其中1~2个/HAHA1/.snapshot/snapshot14.回复快照：hdfs dfs -cp /HAHA1/.snapshot/snapshot1 /snapshot 删除快照 12dfs dfs -deleteSnapshot 快照目录 快照名称dfs dfs -deleteSnapshot /tmp/snapshot snapshot1","link":"/post/21534.html"},{"title":"Hadoop IPC 数据分帧和读写","text":"数据通信中定界的方法 数据通信中定界的方法$ 定长消息:通信双方发送的消息长度是固定的，接收者只需要简单地将数据读入对应的缓冲区中，就可以获得消息。 基于定界符:消息的结束由唯一标记指出，消息发送者在传输完数据后，添加一-个特殊的字节序列。这个特殊的标记不能在传输的数据中出现，接收者简单地扫描输入信息并查找定界符，并将定位符前面的数据形成消息交给上层应用。 显式长度:在具体消息前面附加一一个固定大小的字段，指示该消息包含多少字节。接收者首先以定长消息的方式接受长度字段，然后根据这个长度接收消息。 Hadoop IPC通信的定界方法 客户端-&gt;服务器端：显式长度 服务器-&gt;客户端：定长消息，通过Writable序列化","link":"/post/a9423fcb.html"},{"title":"Hadoop IPC 连接的建立","text":"相关参数说明 相关方法说明 服务器端 Hadoop 2.x 相关参数说明$ connections 用于保存ConnectionId到Connection的映射，位于org.apache.hadoop.ipc.Client中 1private ConcurrentMap&lt;Client.ConnectionId, Client.Connection&gt; connections; calls 当前正在处理的远程调用，位于org.apache.hadoop.ipc.Client.Connection中 1private Hashtable&lt;Integer, Client.Call&gt; calls = new Hashtable(); shouldCloseConnection 连接关闭标志 1private AtomicBoolean shouldCloseConnection = new AtomicBoolean(); 相关方法说明$ getConnection Client需要获取连接的时候，调用getConnection方法，该方法先检查connections中是否存在满足条件的IPC连接。有，则 复用 ，否则，创建新的连接复用是指connection相等（connection里面的三个参数相等则说明两个connection相等），就使用同一个connection 相关代码如下 123456789101112131415161718192021222324252627 private Client.Connection getConnection(Client.ConnectionId remoteId, Client.Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth) throws IOException { //首先，看看客户端是否还在运行 if (!this.running.get()) { throw new IOException(\"The client is stopped\"); } else { while(true) { //2. 查一下是否存在remoteId对应的连接connection Client.Connection connection = (Client.Connection)this.connections.get(remoteId); //connection==null，表明不存在，就需要创建一个新的IPC连接 if (connection == null) { //创建连接 connection = new Client.Connection(remoteId, serviceClass); Client.Connection existing = (Client.Connection)this.connections.putIfAbsent(remoteId, connection); if (existing != null) { connection = existing; } } //将IPC调用放入IPC连接中 if (connection.addCall(call)) { connection.setupIOstreams(fallbackToSimpleAuth); return connection; } this.connections.remove(remoteId, connection); } }} addCall 作用是将一个IPC调用放入IPC连接中&emsp;如果成员变量shouldCloseConnection为true, 返回false，这样可以防止将一个IPC调用放入一个已经关闭的IPC连接中。&emsp;否则，将调用放入IPC连接calls中再说一下，为什么会有这种情况？IPC来呢及可以在多个地方被触发,进入关闭过程，但知道Connection.close方法被调用，对应的connection才会在connections中删除。删除后的连接只有新建后才能将IPC调用传入连接中相关代码 12345678910 private synchronized boolean addCall(Client.Call call) { if (this.shouldCloseConnection.get()) { return false; } else { this.calls.put(call.id, call); //notify是唤醒等待的线程，因为这个方法会有多个地方调用，但进来的只能有一个 this.notify(); return true; }} setupIOstreams 该方法是使客户端和服务器通过Socket连接起来连接失败的话，会重传，最多maxRetries，可以设置${ipc.client.connect.max.retries} 12345678910111213141516171819202122232425262728293031323334353637 private synchronized void setupIOstreams(AtomicBoolean fallbackToSimpleAuth) { if (this.socket == null &amp;&amp; !this.shouldCloseConnection.get()) { ... if (Client.LOG.isDebugEnabled()) { Client.LOG.debug(\"Connecting to \" + this.server); } Span span = Tracer.getCurrentSpan(); if (span != null) { span.addTimelineAnnotation(\"IPC client connecting to \" + this.server); } short numRetries = 0; Random rand = null; while(true) { /** * 建立Socket连接，具体可以查看源代码Ctrl + 鼠标左键查看 * connection使用Socket连接设置了tcpNoDelay标志，禁用Nagle算法,无需等待直接发送 * 配置项${ipc.client.tcpnodelay} */ this.setupConnection(ticket); ... //与IPC服务器进行握手 this.writeConnectionContext(this.remoteId, this.authMethod); //更改最后访问时间lastActivity,该变量也是Client成员变量 this.touch(); ... //启动接受进程 this.start(); ... this.close(); } }} 以上是客户端，下面为IPC连接的另一端 服务器端$ 服务器建立IPC连接的代码分散在Listener和Server.Connection中Listener基于Java NIO开发的，是一个标准的NIO应用，其构造函数中打开服务器端口，创建Selector监听注意参数backlogLength，它由ipc.server.listen.queue.size参数指定，backlogLength是调用ServerSocket.bind()时可以额外提供的一个参数，用于指定在监听端口上排队请求的最大长度，队列满了以后的客户端请求，会被拒绝 run方法123456789101112131415161718192021222324252627282930public void run() { ... while(Server.this.running) { SelectionKey key = null; try { //select调用 this.getSelector().select(); //获取相关键 for(Iterator iter = this.getSelector().selectedKeys().iterator(); iter.hasNext(); key = null) { key = (SelectionKey)iter.next(); iter.remove(); try { //判断是不是可接受事件 if (key.isValid() &amp;&amp; key.isAcceptable()) { //doAccept方法,接受客户端请求、注册Socket到选择器上， //并且创建Reader，Reader里面的run方法处理 OP_READ //该方法中最主要的是ReadAndProcess方法 this.doAccept(key); } } catch (IOException var8) { } ... } }","link":"/post/28a780a5.html"}],"tags":[{"name":"什么是大数据","slug":"什么是大数据","link":"/tags/%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"分布式存储管理","slug":"分布式存储管理","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"name":"NoSSQL特点","slug":"NoSSQL特点","link":"/tags/NoSSQL%E7%89%B9%E7%82%B9/"},{"name":"十大经典算法优缺点","slug":"十大经典算法优缺点","link":"/tags/%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9/"},{"name":"Java Web过滤器和监听器","slug":"Java-Web过滤器和监听器","link":"/tags/Java-Web%E8%BF%87%E6%BB%A4%E5%99%A8%E5%92%8C%E7%9B%91%E5%90%AC%E5%99%A8/"},{"name":"AJAX 和JSON的使用","slug":"AJAX-和JSON的使用","link":"/tags/AJAX-%E5%92%8CJSON%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"name":"Tomcat 的安装与配置","slug":"Tomcat-的安装与配置","link":"/tags/Tomcat-%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"},{"name":"Shell编程 awk","slug":"Shell编程-awk","link":"/tags/Shell%E7%BC%96%E7%A8%8B-awk/"},{"name":"find、which、locate、whereis命令总结","slug":"find、which、locate、whereis命令总结","link":"/tags/find%E3%80%81which%E3%80%81locate%E3%80%81whereis%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"},{"name":"Shell 之Bash数学运算","slug":"Shell-之Bash数学运算","link":"/tags/Shell-%E4%B9%8BBash%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/"},{"name":"Shell 的函数的定义和使用","slug":"Shell-的函数的定义和使用","link":"/tags/Shell-%E7%9A%84%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E5%92%8C%E4%BD%BF%E7%94%A8/"},{"name":"Shell全局变量和局部变量","slug":"Shell全局变量和局部变量","link":"/tags/Shell%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%92%8C%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F/"},{"name":"Shell命令的替换","slug":"Shell命令的替换","link":"/tags/Shell%E5%91%BD%E4%BB%A4%E7%9A%84%E6%9B%BF%E6%8D%A2/"},{"name":"Shell变量的替换和测试","slug":"Shell变量的替换和测试","link":"/tags/Shell%E5%8F%98%E9%87%8F%E7%9A%84%E6%9B%BF%E6%8D%A2%E5%92%8C%E6%B5%8B%E8%AF%95/"},{"name":"Shell 的字符串处理","slug":"Shell-的字符串处理","link":"/tags/Shell-%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%84%E7%90%86/"},{"name":"Spring初始之HelloWorld","slug":"Spring初始之HelloWorld","link":"/tags/Spring%E5%88%9D%E5%A7%8B%E4%B9%8BHelloWorld/"},{"name":"Spring初识","slug":"Spring初识","link":"/tags/Spring%E5%88%9D%E8%AF%86/"},{"name":"Shell有类型变量","slug":"Shell有类型变量","link":"/tags/Shell%E6%9C%89%E7%B1%BB%E5%9E%8B%E5%8F%98%E9%87%8F/"},{"name":"Shell文件查找find命令","slug":"Shell文件查找find命令","link":"/tags/Shell%E6%96%87%E4%BB%B6%E6%9F%A5%E6%89%BEfind%E5%91%BD%E4%BB%A4/"},{"name":"文本处理三剑客(grep/sed/awk)","slug":"文本处理三剑客-grep-sed-awk","link":"/tags/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2-grep-sed-awk/"},{"name":"Java代理机制","slug":"Java代理机制","link":"/tags/Java%E4%BB%A3%E7%90%86%E6%9C%BA%E5%88%B6/"},{"name":"Java Proxy","slug":"Java-Proxy","link":"/tags/Java-Proxy/"},{"name":"hadoop配置文件解析","slug":"hadoop配置文件解析","link":"/tags/hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/"},{"name":"DOM解析","slug":"DOM解析","link":"/tags/DOM%E8%A7%A3%E6%9E%90/"},{"name":"Java NIO","slug":"Java-NIO","link":"/tags/Java-NIO/"},{"name":"Linux 工具包","slug":"Linux-工具包","link":"/tags/Linux-%E5%B7%A5%E5%85%B7%E5%8C%85/"},{"name":"Java远程调用","slug":"Java远程调用","link":"/tags/Java%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/"},{"name":"RMI","slug":"RMI","link":"/tags/RMI/"},{"name":"Java代理","slug":"Java代理","link":"/tags/Java%E4%BB%A3%E7%90%86/"},{"name":"Hadooop介绍","slug":"Hadooop介绍","link":"/tags/Hadooop%E4%BB%8B%E7%BB%8D/"},{"name":"Google三宝","slug":"Google三宝","link":"/tags/Google%E4%B8%89%E5%AE%9D/"},{"name":"Kafka初识","slug":"Kafka初识","link":"/tags/Kafka%E5%88%9D%E8%AF%86/"},{"name":"kafka通信","slug":"kafka通信","link":"/tags/kafka%E9%80%9A%E4%BF%A1/"},{"name":"Java NIO实例：回显服务器","slug":"Java-NIO实例：回显服务器","link":"/tags/Java-NIO%E5%AE%9E%E4%BE%8B%EF%BC%9A%E5%9B%9E%E6%98%BE%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Hadoop 压缩","slug":"Hadoop-压缩","link":"/tags/Hadoop-%E5%8E%8B%E7%BC%A9/"},{"name":"Hadoop 文件分片","slug":"Hadoop-文件分片","link":"/tags/Hadoop-%E6%96%87%E4%BB%B6%E5%88%86%E7%89%87/"},{"name":"MapReduce介绍","slug":"MapReduce介绍","link":"/tags/MapReduce%E4%BB%8B%E7%BB%8D/"},{"name":"hadoop序列化","slug":"hadoop序列化","link":"/tags/hadoop%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"MapReduce 通信协议","slug":"MapReduce-通信协议","link":"/tags/MapReduce-%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"},{"name":"Hadoop 协议","slug":"Hadoop-协议","link":"/tags/Hadoop-%E5%8D%8F%E8%AE%AE/"},{"name":"Windows操作系统配置文件","slug":"Windows操作系统配置文件","link":"/tags/Windows%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Java配置文件","slug":"Java配置文件","link":"/tags/Java%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Hadoop配置文件","slug":"Hadoop配置文件","link":"/tags/Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Hadoop IPC","slug":"Hadoop-IPC","link":"/tags/Hadoop-IPC/"},{"name":"Hadoop RPC","slug":"Hadoop-RPC","link":"/tags/Hadoop-RPC/"},{"name":"Hadoop 通信机制","slug":"Hadoop-通信机制","link":"/tags/Hadoop-%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/"},{"name":"HDFS快照","slug":"HDFS快照","link":"/tags/HDFS%E5%BF%AB%E7%85%A7/"},{"name":"Hadoop IPC通信中是如何知道数据长度的","slug":"Hadoop-IPC通信中是如何知道数据长度的","link":"/tags/Hadoop-IPC%E9%80%9A%E4%BF%A1%E4%B8%AD%E6%98%AF%E5%A6%82%E4%BD%95%E7%9F%A5%E9%81%93%E6%95%B0%E6%8D%AE%E9%95%BF%E5%BA%A6%E7%9A%84/"},{"name":"Hadoop IPC 数据分帧和读写","slug":"Hadoop-IPC-数据分帧和读写","link":"/tags/Hadoop-IPC-%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%A7%E5%92%8C%E8%AF%BB%E5%86%99/"},{"name":"Hadoop 如何建立 IPC连接","slug":"Hadoop-如何建立-IPC连接","link":"/tags/Hadoop-%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B-IPC%E8%BF%9E%E6%8E%A5/"}],"categories":[{"name":"BigData","slug":"BigData","link":"/categories/BigData/"},{"name":"DataMining","slug":"DataMining","link":"/categories/DataMining/"},{"name":"Java Web","slug":"Java-Web","link":"/categories/Java-Web/"},{"name":"Shell","slug":"Shell","link":"/categories/Shell/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"Hadoop","slug":"BigData/Hadoop","link":"/categories/BigData/Hadoop/"},{"name":"Kafka","slug":"Kafka","link":"/categories/Kafka/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Python可视化","slug":"Python/Python可视化","link":"/categories/Python/Python%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"Hadoop common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"MapReduce","slug":"BigData/Hadoop/MapReduce","link":"/categories/BigData/Hadoop/MapReduce/"},{"name":"Hadoop_common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"HDFS","slug":"BigData/Hadoop/HDFS","link":"/categories/BigData/Hadoop/HDFS/"},{"name":"IPC","slug":"BigData/Hadoop/Hadoop-common/IPC","link":"/categories/BigData/Hadoop/Hadoop-common/IPC/"}]}