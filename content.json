{"pages":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/about/about.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"分布式存储管理","text":"为什么直接采用关系模型的分布式数据库并不能适应大数据时代的？ 规模效应所带来的压力 传统数据库倾向于纵向扩展(Scale-Up)，即增加单台计算机的性能 适应大数据的数据库系统的应该具有良好的横向扩展(Scale-Out)，即为集群增加一台计算机 数据类型的多样化 传统数据类型： 结构化数据大数据时代的数据类型： 结构化数据 半结构化数据 非结构化数据 设计理念所带来的冲突 关系型数据库 One size fits all ,即面对不同问题不需要重新考虑数据管理问题 简单来说就是，单一模式可以适应所有变化 新理念 “One size fits one” 和 “One size fits domain” 数据库的事务特性 传统数据库ACID特性 A(Atom，原子性)，C(Consistency，一致性)，I(Isolation,隔离性)，D(Durability,持久性) 大数据时代的数据库BASE。 Basically Available(基本可用)，Soft State(柔性状态)，Eventually Consistency(最终一致性)根据分布式领域著名的CAP理论来看，ACID追求一致性C,而BASE更加关注A一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）新型数据库Spanner NoSQL 特点 模式自由（Schema-free） 支持简易备份（Easy Replication Support) 简单应用程序接口（Simple API） 最终一致性（或说支持BASE特性，不支持ACID特性） 支持海量数据（Huge Amount of Data）","link":"/post/4faba951.html"},{"title":"大数据介绍","text":"什么是大数据 百度百科的定义,大数据（BIG DATA），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产 在目前的业界尚未对大数据由清晰明确的定义, 它的第一次出现是在麦肯锡公司的报告中出现的, 在维基百科上的较为模糊的定义是很难运用软件的手段获取大量的内容信息, 对其处理后整理得出的数据集合。其他计算机学科的学者给出的定义是数据的尺度极为巨大, 常规的数据处理软件无法对数据识别、存储和应用的海量数据信息 维基百科的定义，大数据是指无法在可承受的时间范围内用常规软件工具进行捕捉、管理和处理的数据集合。 研究机构Gartner定义，“大数据”是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 数据单位1MB = 1024KB、1GB = 1024MB1TB = 1024GB、1PB = 1024TB 大数据的特征 容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息； 种类（Variety）：数据类型的多样性； 速度（Velocity）：指获得数据的速度； 可变性（Variability）：妨碍了处理和有效地管理数据的过程。 真实性（Veracity）：数据的质量 复杂性（Complexity）：数据量巨大，来源多渠道 价值（value）：合理运用大数据，以低成本创造高价值 大数据相关技术 数据采集： OLAP(联机分析处理)和数据挖掘的基础。ETL工具负责将分布的、异构的数据源进行抽取，抽取到中间层，进行清洗、转换、集成，（不过对于负责的逻辑处理不会这么干，用Spark或者其他的进行处理），最后放到数据仓库中存储，如Hive 数据存取： 关系型数据库、NoSQL(Not Only SQL,泛指非关系型数据库)，SQL等 基础架构： 云存储、分布式文件存储等 数据处理： 自然语言处理(Natural Language Processing, NLP) 数据分析： 假设检验、显著性检验、差异检验、差异分析、相关性分析、T检验、方差分析、卡方分析、偏相关性分析、距离分析、回归分析、简单回归分析、多元回归分析、逐步回归、预测和残差分析、岭回归、Logistic回归分析、曲线估计、因子分析、聚类分析、主成分分析、判别分析、对应分析、快速聚类和聚类法、对应分析、多元对应分析等 数据挖掘： 分类（Classification）、估计（Estimation）、预测（Prediction)、相关性分析或关联规则（Association）、复杂数据类型挖掘（Text、Web、图形图像、视频、音频等） 模型预测： 预测模型、机器学习、建模仿真 结果呈现： 云计算、标签云、关系图等 大数据带来的变革大数据小故事 &emsp;&emsp;最早关于大数据的故事发生在美国第二大的超市塔吉特百货（Target）。孕妇对于零售商来说是个含金量很高的顾客群体。但是他们一般会去专门的孕妇商店而不是在Target购买孕期用品。人们一提起Target，往往想到的都是清洁用品、袜子和手纸之类的日常生活用品，却忽视了Target有孕妇需要的一切。那么Target有什么办法可以把这部分细分顾客从孕妇产品专卖店的手里截留下来呢？ &emsp;&emsp;为此，Target的市场营销人员求助于Target的顾客数据分析部的高级经理Andrew Pole，要求他建立一个模型，在孕妇第2个妊娠期就把她们给确认出来。在美国出生记录是公开的，等孩子出生了，新生儿母亲就会被铺天盖地的产品优惠广告包围，那时候Target再行动就晚了，因此必须赶在孕妇第2个妊娠期行动起来。如果Target能够赶在所有零售商之前知道哪位顾客怀孕了，市场营销部门就可以早早的给他们发出量身定制的孕妇优惠广告，早早圈定宝贵的顾客资源。 &emsp;&emsp;可是怀孕是很私密的信息，如何能够准确地判断哪位顾客怀孕了呢？Andrew Pole想到了Target有一个迎婴聚会（baby shower）的登记表。Andrew Pole开始对这些登记表里的顾客的消费数据进行建模分析，不久就发现了许多非常有用的数据模式。比如模型发现，许多孕妇在第2个妊娠期的开始会买许多大包装的无香味护手霜；在怀孕的最初20周大量购买补充钙、镁、锌的善存片之类的保健品。最后Andrew Pole选出了25种典型商品的消费数据构建了“怀孕预测指数”，通过这个指数，Target能够在很小的误差范围内预测到顾客的怀孕情况，因此Target就能早早地把孕妇优惠广告寄发给顾客。 &emsp;&emsp;那么，顾客收到这样的广告会不会吓坏了呢？Target很聪明地避免了这种情况，它把孕妇用品的优惠广告夹杂在其他一大堆与怀孕不相关的商品优惠广告当中，这样顾客就不知道Target知道她怀孕了。百密一疏的是，Target的这种优惠广告间接地令一个蒙在鼓里的父亲意外发现他高中生的女儿怀孕了，此事甚至被《纽约时报》报道了，结果Target大数据的巨大威力轰动了全美。 &emsp;&emsp;根据Andrew Pole的大数据模型,Target制订了全新的广告营销方案，结果Target的孕期用品销售呈现了爆炸性的增长。Andrew Pole的大数据分析技术从孕妇这个细分顾客群开始向其他各种细分客户群推广，从Andrew Pole加入Target的2002年到2010年间，Target的销售额从440亿美元增长到了670亿美元。 &emsp;&emsp;我们可以想象的是，许多孕妇在浑然不觉的情况下成了Target常年的忠实拥泵，许多孕妇产品专卖店也在浑然不知的情况下破产。浑然不觉的背景里，大数据正在推动一股强劲的商业革命暗涌，商家们早晚要面对的一个问题就是：究竟是在浑然不觉中崛起，还是在浑然不觉中灭亡 其他故事 Google根据搜索关键字分析流感病毒H1N1 2008年金融危机 大数据初步学习路线 技术 工具 JAVA 面向对象的编程语言 Linux 类Unix操作系统 Hadoop生态圈 1、HDFS 解决存储问题存储极大数目的信息（terabytes or petabytes），将数据保存到大量的节点当中。支持很大单个文件。提供高可靠性，是指一个或多个节点故障，系统仍然可以继续工作提供数据快速访问 2、MapReduce 解决计算问题它有个特点就是不管多大的数据只要给它时间它就能把数据跑完，但是时间可能不是很快所以它叫数据的批处理 3、Yarn 资源调度器 4、ZooKeeper 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 5、Flume 数据采集工具 6、Hive 基于Hadoop的数据仓库工具 7、Hbase 分布式应用程序协调服务一般用于存储一些相互协作的一些信息 8、Sqoop 数据传递工具，如将数据从关系型数据库导入Hive Scala 多范式编程语言、面向对象和函数式编程的特性 Spark 目前企业常用的批处理离线数据/实时计算引擎它是用来弥补基于MapReduce处理数据速度上的缺点，它很是流氓，直接将数据存在内存中 【注意】 MapReduce运行时也是需要将代码数据加载到内存中的，只不过Spark都是基于内存操作 Flink 目前最火的流式处理框架、既支持流处理、也支持批处理 Elasticsearch 大数据分布式弹性搜索引擎 大数据处理流程 大数据处理模型按照数据的三状态定义 水库里一平如镜的水—&gt;静止数据(data at rest) 水处理系统中上下翻滚的水—&gt;正在使用的数据(data in use) 汹涌而来的新水流—&gt;动态的水(data in motion) “快”说的是两层面 “动态数据” 来得快 “正在使用的数据” 处理得快 批处理 MapReduce 流处理 Spark Streaming 科普 根据国际数据公司（IDC）的《数据宇宙》报告显示：2008年全球数量为0.5ZB，2010年为1.2ZB，人类正式进入ZB时代。更为惊人的是，2020年以前全球数据量仍将保持每年40%多的高速增长，大约每两年就翻一倍，这与IT界的摩尔定律极为相似，姑且称之为“大数据爆炸定律”。","link":"/post/2260.html"},{"title":"DOM解析","text":"说明：这里主要分析hadoop的DOM解析 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档处理步骤 获得用于创建DOM解析器的工厂对象1DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); 可以设置一下参数[可选]12345678docBuilderFactory.setIgnoringComments(true);docBuilderFactory.setNamespaceAware(true);boolean useXInclude = !wrapper.isParserRestricted();try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28);} 获得解析XML的DocumentBuilder对象1DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); 获取根节点下的所有节点1NodeList props = root.getChildNodes(); 遍历节点123456789101112for(int i = 0; i &lt; props.getLength(); ++i) { //获取节点 Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; //prop.getTagName()获取节点内的值 if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { ... } hadoop配置文件解析的特别说明 对DocumentBuilderFactory做的处理 忽略XML文档中的注释1docBuilderFactory.setIgnoringComments(true); 支持XML命名空间1docBuilderFactory.setNamespaceAware(true); 支持XML包含机制12345try { docBuilderFactory.setXIncludeAware(useXInclude);} catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \")for parser \" + docBuilderFactory, var28);} XInclude机制允许将XML文档分解为多个可管理的块，然后将-一个或多个较小的文档组装成一个大型文档。 hadoop 配置文件解析完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157private Configuration.Resource loadResource(Properties properties, Configuration.Resource wrapper, boolean quiet) { String name = \"Unknown\"; try { Object resource = wrapper.getResource(); name = wrapper.getName(); DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory.newInstance(); docBuilderFactory.setIgnoringComments(true); docBuilderFactory.setNamespaceAware(true); boolean useXInclude = !wrapper.isParserRestricted(); try { docBuilderFactory.setXIncludeAware(useXInclude); } catch (UnsupportedOperationException var28) { LOG.error(\"Failed to set setXIncludeAware(\" + useXInclude + \") for parser \" + docBuilderFactory, var28); } if (wrapper.isParserRestricted()) { docBuilderFactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true); } DocumentBuilder builder = docBuilderFactory.newDocumentBuilder(); Document doc = null; Element root = null; boolean returnCachedProperties = false; if (resource instanceof URL) { doc = this.parse(builder, (URL)resource); } else if (resource instanceof String) { URL url = this.getResource((String)resource); doc = this.parse(builder, url); } else if (resource instanceof Path) { File file = (new File(((Path)resource).toUri().getPath())).getAbsoluteFile(); if (file.exists()) { if (!quiet) { LOG.debug(\"parsing File \" + file); } doc = this.parse(builder, new BufferedInputStream(new FileInputStream(file)), ((Path)resource).toString()); } } else if (resource instanceof InputStream) { doc = this.parse(builder, (InputStream)resource, (String)null); returnCachedProperties = true; } else if (resource instanceof Properties) { this.overlay(properties, (Properties)resource); } else if (resource instanceof Element) { root = (Element)resource; } if (root == null) { if (doc == null) { if (quiet) { return null; } throw new RuntimeException(resource + \" not found\"); } root = doc.getDocumentElement(); } Properties toAddTo = properties; if (returnCachedProperties) { toAddTo = new Properties(); } if (!\"configuration\".equals(root.getTagName())) { LOG.fatal(\"bad conf file: top-level element not &lt;configuration&gt;\"); } NodeList props = root.getChildNodes(); Configuration.DeprecationContext deprecations = (Configuration.DeprecationContext)deprecationContext.get(); for(int i = 0; i &lt; props.getLength(); ++i) { Node propNode = props.item(i); if (propNode instanceof Element) { Element prop = (Element)propNode; if (\"configuration\".equals(prop.getTagName())) { this.loadResource(toAddTo, new Configuration.Resource(prop, name, wrapper.isParserRestricted()), quiet); } else { if (!\"property\".equals(prop.getTagName())) { if (wrapper.isParserRestricted() &amp;&amp; \"http://www.w3.org/2001/XInclude\".equals(prop.getNamespaceURI())) { throw new RuntimeException(\"Error parsing resource \" + wrapper + \": XInclude is not supported for restricted resources\"); } LOG.warn(\"Unexpected tag in conf file \" + wrapper + \": expected &lt;property&gt; but found &lt;\" + prop.getTagName() + \"&gt;\"); } NodeList fields = prop.getChildNodes(); String attr = null; String value = null; boolean finalParameter = false; LinkedList&lt;String&gt; source = new LinkedList(); //遍历所有节点，并根据情况设置对象的成员变量properties和finalParameters for(int j = 0; j &lt; fields.getLength(); ++j) { Node fieldNode = fields.item(j); if (fieldNode instanceof Element) { Element field = (Element)fieldNode; if (\"name\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { attr = StringInterner.weakIntern(((Text)field.getFirstChild()).getData().trim()); } if (\"value\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { value = StringInterner.weakIntern(((Text)field.getFirstChild()).getData()); } if (\"final\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData()); } if (\"source\".equals(field.getTagName()) &amp;&amp; field.hasChildNodes()) { source.add(StringInterner.weakIntern(((Text)field.getFirstChild()).getData())); } } } source.add(name); if (attr != null) { if (deprecations.getDeprecatedKeyMap().containsKey(attr)) { Configuration.DeprecatedKeyInfo keyInfo = (Configuration.DeprecatedKeyInfo)deprecations.getDeprecatedKeyMap().get(attr); keyInfo.clearAccessed(); String[] arr$ = keyInfo.newKeys; int len$ = arr$.length; for(int i$ = 0; i$ &lt; len$; ++i$) { String key = arr$[i$]; this.loadProperty(toAddTo, name, key, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } else { this.loadProperty(toAddTo, name, attr, value, finalParameter, (String[])source.toArray(new String[source.size()])); } } } } } if (returnCachedProperties) { this.overlay(properties, toAddTo); return new Configuration.Resource(toAddTo, name, wrapper.isParserRestricted()); } else { return null; } } catch (IOException var29) { LOG.fatal(\"error parsing conf \" + name, var29); throw new RuntimeException(var29); } catch (DOMException var30) { LOG.fatal(\"error parsing conf \" + name, var30); throw new RuntimeException(var30); } catch (SAXException var31) { LOG.fatal(\"error parsing conf \" + name, var31); throw new RuntimeException(var31); } catch (ParserConfigurationException var32) { LOG.fatal(\"error parsing conf \" + name, var32); throw new RuntimeException(var32); }}","link":"/post/e9fcf702.html"},{"title":"Hadoop介绍","text":"起源 Google 在大数据方面的三大论文 （谷歌三宝）在github大的当前目录下三宝的介绍 Hadoop 三大发行版本 Apache、Cloudera、Hortonworks Apache版本最原始、最基础：适合零基础 大公司在用 Cloudera Cloudera’s DistributionIncluding Apache Hadoop 简称CDH中小型公司用、简单方便、自带可视化 Hortonworks 文档较好 注：Cloudera 和Hortonworks 在2018年10月，国庆期间宣布合并硬件要求内存 最大支持内存查询：win + R输入 wmic memphysical get maxcapacity计算 MaxCapacity/1024/1024GB 硬盘:500G+","link":"/post/8a9ad9ba.html"},{"title":"Hadoop通信机制和内部协议之RPC","text":"Hadoop RPCRPC模型 通行模块： 请求-响应 Stub程序： 用于保证RPC的透明性。在客户端，不在本地调用，而是将请求信息通过网络模块发送给法服务器端，服务器接收后进行解码。服务器中，Stub程序依次进行 解码（请求的参数）、调用相应的服务过程、编码返回结果等处理 调度程序： 调度来自通行模块的请求信息，根据其中标识选一个Stub程序运行 客户程序： 请求发出者 服务过程： 请求接收者 一个RPC的旅游： 客户端以本地调用方式产生本地Stub程序 该Stub程序将函数调用信息按照网络通信模块的要求封装成消息包，并交给通信模块发送到远程服务器端。 远程服务器端接收此消息后，将此消息发送给相应的Stub程序 Stub程序拆封消息，形成被调过程要求的形式，并调用对应函数 服务端执行被调用函数，并将结果返回给Stub程序 Stub程序将此结果封装成消息，通过网络通信模块逐级地传送给客户程序。 RPC特性 透明性 调用过程就像本地调用，察觉不到它的经历 高性能 ：Hadoop各个系统（如HDFS、MapReduce、YARN等）均采用了Master/Slave结构，其中，Master实际上是一个RPC server，它负责响应集群中所有Slave发送的服务请求。RPC Server性能要求高，为的是能够让多个客户端并发方位 易用性/可控性 Hadoop系统不采用Java内嵌的RPC（RMI,Remote Method Invocation）框架的主要原因是RPC是Hadoop底层核心模块之一，需要满足易用性、高性能、轻量级等特性 RPC例子执行过程： CalculateClient对象本地调用产生Stub程序 经通信模块上传至服务器CalculateServer对象，在创建Server时设置了协议和业务逻辑（服务过程），处理过后根据上述RPC过程返回 客户端接收后打印到日志中 先定义一些常量 这里不需要太多的在意，直接使用在代码里面也行，在大的项目中为了使程序易于修改而这样设置 1234567891011/** * 静态变量声明类 */public interface Constants { public interface VersionID { public static final long RPC_VERSION = 7788L; } public static final String RPC_HOST = &quot;127.0.0.1&quot;; public static final int RPC_PORT = 8888;} 定义一个Service接口，协议类12345678910111213import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.ipc.ProtocolInfo;@ProtocolInfo(protocolName = &quot;&quot;, protocolVersion = Constants.VersionID.RPC_VERSION)public interface CalculateService { //真实业务逻辑，加减法， public IntWritable add(IntWritable a, IntWritable b); public IntWritable sub(IntWritable a, IntWritable b); public Text echo(Text mt);} @ProtocolInfo(protocolName = “”, protocolVersion = Constants.VersionID.RPC_VERSION) 没有这句就不能将该类设置为协议，不过也可以通过继承VersionProtocol接口 Service接口的实现类1234567891011121314151617181920212223242526272829303132333435import java.io.IOException;public class CalculateServiceImpl implements CalculateService { /** * 该方法没有也行 * */ public ProtocolSignature getProtocolSignature(String arg0, long arg1, int arg2) throws IOException{ return this.getProtocolSignature(arg0, arg1, arg2); } /** * 校验hadoop RFC版本号 * @param arg0 * @param arg1 * @return */ public long getProtocolVersion(String arg0, long arg1) throws IOException { return Constants.VersionID.RPC_VERSION; } @Override public IntWritable add(IntWritable a, IntWritable b) { return new IntWritable(a.get() + b.get()); } @Override public IntWritable sub(IntWritable a, IntWritable b) { return new IntWritable(a.get() - b.get()); } @Override public Text echo(Text mt) { return mt; }} Server和Client类123456789101112131415161718192021222324252627282930import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;public class CalculateServer { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { try { //构造Server,并设置协议接口，主机、端口，真实业务逻辑 RPC.Server server = new RPC.Builder(new Configuration()) .setProtocol(CalculateService.class) .setBindAddress(Constants.RPC_HOST) .setPort(Constants.RPC_PORT) .setInstance(new CalculateServiceImpl()) .build(); //启动Server server.start(); LOG.info(&quot;Server has Started!&quot;); } catch (IOException e) { LOG.error(&quot;Server has Error&quot;); } }} 123456789101112131415161718192021222324252627282930313233343536import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.ipc.RPC;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.net.InetSocketAddress;public class CalculateClient { private static final Logger LOG = LoggerFactory.getLogger(CalculateServer.class); public static void main(String[] args) { //格式化IP和端口 InetSocketAddress addr = new InetSocketAddress(Constants.RPC_HOST, Constants.RPC_PORT); //校验Hadoop RPC版本号 long protocolVersion = RPC.getProtocolVersion(CalculateService.class); try { //获取Server连接 CalculateService proxy = RPC.getProxy(CalculateService.class, protocolVersion, addr, new Configuration()); IntWritable add = proxy.add(new IntWritable(1), new IntWritable(2)); IntWritable sub = proxy.add(new IntWritable(3), new IntWritable(2)); LOG.info(&quot;1+2 = &quot; + add); LOG.info(&quot;3-2 = &quot; + sub); } catch (IOException e) { LOG.error(&quot;Client has error!&quot;); } }} 注意： 查看本程序运行结果需要一个日志文件，如果不想加，把LOG的相关语句换为打印输出就行在resource文件夹下创建 log4j.properties 12345678log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 客户端运行结果12342019-10-31 18:59:28,499 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-10-31 18:59:28,619 WARN [org.apache.hadoop.util.NativeCodeLoader] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 1+2 = 3 2019-10-31 18:59:29,734 INFO [hadooprfc.calculate.CalculateServer] - 3-2 = 5 其他开源RPC架构 Java RMI Apache Thrift Google Protocol Buffer","link":"/post/56192.html"},{"title":"Hadoop通信机制和内部协议之协议","text":"概述MapReduce核心协议 名称 描述 ClientProtocol 继承于Version基类，查看作业情况监控当前集群等 RefreshUserMappingProtocol 刷新用户到用户组映射关系到超级用户代理组列表 RefreshAuthorizationPolicyProtocol 刷新HDFS和MapReduce服务几倍访问控制列表 ResourceManagerAdministrationProtocol 继承于GetUserMappingProtocol基类，刷新队列列表，节点列表 ## ClientProtocol通信协议 ClientProtocol协议是JobClient和JobTracker之间进行交流的枢纽。JobClient 可以使用该协议中的函数来提交-一个作业(Job) 并执行，以了解当前系统的状态 提交作业协议中JobClient通过Hadoop RPC的submitjob()函数提交作业(Job)，函数所包含的参数有作业ID (JobID)，然后JobClient通过getNewJoblD0函数为作业(Job) 获得一个唯一的ID。 操作作业当用户提交作业(Job) 后，可以通过调用函数来控制该作业的执行流程，如设置提交作业的优先级(setlobPriority()函数) 、停止一个作业(killJob()函数) 、停止一个任务(illTask()函数)。 查看状态从实现源代码来看，该通信协议还提供了一系列函数来 查看状态，如查看集群当前状态(getClusterMetrics()函数)、查看当前任务状态(getJobTrackerStatus()函数) 、获取所有任务(getllobs()函数)等。 RefreshUserMappingProtocol RefreshU serMappingsProtocol 协议用于更新 HDFS 和 MapReduce 级别的用户到用户组映射关系及超级用户代理组列表 refreshUserToGroupsMappings() 函数和refreshSuperUserGroupsConfiguration()函数来实现，这两个函数均是通过调用Hadoop RPC来完成具体的逻辑。 RefreshAuthorizationPolicyProtocol RefreshAuthorizationPol icyProtocol 协议用于刷新当前使用的授权策略 通过调用 Hadoop RPC 远程调用 refreshServiceAcl（）函数，实现基于 HDFS 和MapReduce 级别的授权策略 ResourceManagerAdministrationProtocol ResourceManagerAdministrationProtocol 协议用于更新队列列表、节点 列表 、节点资源等 该协议继承于 GetUserMappingsProtocol 基类 ，通过 Hadoop RPC 远程调用来实现节点更新、资源更新 、添加标签等操作 说明：在IDE中导入hadoop源码加载进去后，按Ctrl+鼠标左键进入即可查看源码","link":"/post/13550.html"},{"title":"配置文件","text":"Windows操作系统配置文件 配置设置文件（INI）文件是windows操作系统中的一种特殊的ASCII文件，以ini为文件扩展名,作为它的主要文件配置文件标准。该文件也被称为初始化文件initialization file和概要文件profile。应用程序可以拥有自己的配置文件，存储应用设置信息，也可以访问windows的基本系统配置文件win.ini中存储的配置信息 INI配置信息分为两部分 节，节标题放在方括号中, [section] 项，一个等式，key=value 1234567;注释;节 [section] ;参数（键=值） name=value INI文件片段 1234[0x0419]1100=Ошибка инициализации программы установки1101=%s1102=%1 Идет подготовка к запуску мастера %2, выполняющего установку программы. Ждите. Windows 提供的API 12345678910111213141516DWORD GetPrivateProfileString( LPCTSTR lpAppName, // If this parameter is NULL, the GetPrivateProfileString function copies all section names in the file to the supplied buffer. LPCTSTR lpKeyName, // If this parameter is NULL, all key names in the section specified by the lpAppNameparameter are copied to the buffer specified by the lpReturnedString parameter. LPCTSTR lpDefault, // If the lpKeyName key cannot be found in the initialization file, GetPrivateProfileString copies the default string to the lpReturnedString buffer. LPTSTR lpReturnedString, // destination buffer DWORD nSize, // size of destination buffer LPCTSTR lpFileName // The name of the initialization file);UINT GetPrivateProfileInt( LPCTSTR lpAppName, //节 LPCTSTR lpKeyName,//项 INT nDefault, //The default value to return if the key name cannot be found in the initialization file. LPCTSTR lpFileName //INI文件名); Java配置文件 JDK提供了java.util.Properties类，用于处理简单的配置文件。Properties继承自Hashtable相对于INI文件，Properties处理得配合文件格式非常简单 Properties的使用 非XML文件格式12345678//通过指定的键搜索属性public String getProperty(String key)//功能同上，参数defaultValue提供了默认值public String getProperty(String key, String defaultValue)//最终调用Hashtable 的方法putpublic synchronized object setProperty (String key, String value) Properties中的属性通过load)方法加载，该方法从输入流中读取键-值对，而store()方法法则将Properties表中的属性列表写入输出流。使用输入流和输出流，Properties对象但可以保存在文件中，而且还可以保存在其他支持流的系统中，如Web服务器。 123456789101112131415161718192021222324/** * log4j.properties内容如下 * log4j.rootLogger=INFO, stdout * log4j.appender.stdout=org.apache.log4j.ConsoleAppender * log4j.appender.stdout.layout=org.apache.log4j.PatternLayout * log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n * log4j.appender.logfile=org.apache.log4j.FileAppender * log4j.appender.logfile.File=target/spring.log * log4j.appender.logfile.layout=org.apache.log4j.PatternLayout * log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n * @param args * @throws IOException */ public static void main(String[] args) throws IOException { Properties properties = new Properties(); //获取resources目录下的文件流 InputStream stream = MyConfiguration.class.getClassLoader().getResourceAsStream(\"log4j.properties\"); //加载文件获取并获取配合信息 properties.load(stream); String property = properties.getProperty(\"log4j.rootLogger\"); System.out.println(property); /*输出： INFO, stdout */ } Java 1.5之后支持XML配置文件,Properties中的数据也可以以XML格式保存，对应的加载和写出方法是loadFromXML()和storeToXML() storeToXML() 12345Properties props = new Properties();props.setProperty(\"Length\", \"100\");props.setProperty(\"Width\", \"50\")FileOutputStream fos = new FileOutputStream(\"properties.xml\");props.storeToXML(fos, null); loadFromXML() 1234567Properties props = new Properties();InputStream in = MyConfiguaration2.class.getClassLoader().getResourceAsStream(\"properties.xml\");props.loadFromXML(in)String length = props.getProperty(\"Length\");System.out.println(length); xml有指定格式 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;entry key=\"Width\"&gt;50&lt;/entry&gt; &lt;entry key=\"Length\"&gt;100&lt;/entry&gt;&lt;/properties&gt; java.util.Properties提供的能力有限，其他配置信息读写方法，如Apache Jakarta Commons工具集提供的Commons Configuration 【说明】：上面的MyConfiguration都是自定义的类,并且这些配置文件都在resources文件夹下 Hadoop Configuration详解 Hadoop没有使用java.util.Properties管理配置文件，也没有使用Apache JakartaCommons Configuration 管理配置文件，而是使用了一套独有的配置文件管理系统，并提供自己的API，即使用org.apache.hadoop.conf.Configuration处理配置信息。 hadoop 配置文件格式12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;&lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.9.2/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hadoop 配置文件参数说明配置参数类型说明 参数名 String 参数值 boolean int long float，也可以是其他类型 参数说明 根元素 configuration configuration下的property元素 property下 name 参数名 value 参数值 description 参数描述 final 相当于java的final关键字，在资源合并时可以防止配置项被覆盖 合并资源 合并资源是是指将多个配置合并，产生一个配置文件，如core-site.xml和core-defualt.xml通过addResources()方法合并 1234567Configuration conf = new Configuration();//加载resources文件夹内容ClassLoader classLoader = HadoopConfiguaration.class.getClassLoader//添加合并资源conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-site.xml\")));conf.addResource(Objects.requireNonNull(classLoader.getResourceAsSt(\"core-default.xml\")System.out.println(conf.get(\"fs.defaultFS\")); 【注意】如果第一个配置中存在final，则会以下出现警告,并且值不发生更改 1232019-11-12 17:31:34,548 WARN [org.apache.hadoop.util.Shell] - Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@cac736f:an attempt to override final parameter: fs.defaultFS; Ignoring. 2019-11-12 17:31:34,777 WARN [org.apache.hadoop.conf.Configuration] - java.io.BufferedInputStream@1d7acb34:an attempt to override final parameter: fs.defaultFS; Ignoring. 测试用的core-default.xml文件 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;!-- 指定HDFS中NameNode的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://vmaster:8200&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.3.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;123&lt;/name&gt; &lt;value&gt;wer&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Configuation类的一般过程 构造Configuration对象 添加需要加载的资源 addResource()方法 然后通过set/get 方法访问/设置配置项 【说明】资源会在第一次使用时自动加载到对象中 Configuration类介绍类图 说明类图中，Configuration有7个非静态成员 布尔变量quietmode，用来设置加载配置的模式。如果quietmode为true (默认值)，则在加载解析配置文件的过程中，不输出日志信息。quietmode只是-一个方便开发人员调试的变量。 数组resources保存了所有通过addResource()方法添加Configuration对象的资源 Configuration.addResource()有如下4种形式: 1234public void addResource (InputStream in) //已打开的输入流public void addResource (Path file) //Hadoop文件路径public void addResource (String name) //CLASSPATH 资源 String形式public void addResource (URL url) //URL,统一资源定位符，如https://lyhcc.github.io 布尔变量loadDefaults用于确定是否加载默认资源，这些默认资源保存在defaultResources中。注意，defaultResources 是个静态成员变量，通过方法addDefaultResource()可以添加系统的默认资源。在HDFS中，会把hdfs-default.xml和hdfs-site.xml作为默认资源，并通过addDefaultResource()保存在成员变量defaultResources中;在MapReduce中，默认资源是mapred-default.xml和mapred-site.xml。1234567//下面的代码来自hadoop-1.x 的org.apache.hadoop.hdfs.server.datanode.DataNode static{ Configuration . addDe faultResource (\"hdfs-default . xml\") ; Conf igurat ion. addDe faultResource (\"hdfs-site. xml\") ;}//在hadoop2.x以后这代码被移到了Configuration类里面//hadoop-2.8.4中的1809行 properties 存放Hadoop配置文件解析后的键-值对，为java.util.Properties类型 finalParameters 类型是Set， 用来保存所有在配置文件中已经被声明为final的键-值对的键 overlay用于记录通过set()方式改变的配置项。也就是说，出现在overlay中的键-值对是应用设置的，而不是通过对配置资源解析得到的，为java.util.Properties类型 Configuration 是一一个类加载器变量，可以通过它来加载指定类，也可以通过它加载相关的资源。 上面提到addResource()可以通过字符串方式加载CLASSPATH资源，它其实通过Configuration中的getResource()将字符串转换成URL资源 123public URL getResource (String name){ return classLoader.getResource(name) ;} 2.8.4版本的Configuration类的1188行 Configuration类的过程构造Configuration对象资源加载添加资源到Configuration对象的方法有两种 对象的addResource()方法 类的静态addDefaultResource()方法(设置了loadDefaults标志) 添加的资源并不会立即被加载，只是通过reloadConfiguration()方法清空properties和finalParameters。相关代码如下: 123456789101112131415 //以URL资源为例public void addResource(URL url) { this.addResourceObject(new Configuration.Resource(url)); } //添加资源 private synchronized void addResourceObject(Configuration.Resource resource){ this.resources.add(resource); this.restrictSystemProps |= resource.isParserRestricted(); this.reloadConfiguration(); } //资源重新加载触发函数 public synchronized void reloadConfiguration() { this.properties = null; this.finalParameters.clear(); } 以上是类的成员方法addResource()方法的调用。 静态方法dDefaultResource()也可以清空Configuration对象中的数据（非静态成员），只不过是通过需要通过REGISTRY作为媒介进行。能够调用是因为REGISTRY记录了系统所有的Configuration对象REGISTRY的定义以及为其添加参数的过程 12345678private static final WeakHashMap&lt;Configuration, Object&gt; REGISTRY = new WeakHashMap();public Configuration(boolean loadDefaults) { ... synchronized(Configuration.class) { REGISTRY.put(this, (Object)null); }} 成员变量properties中的数据只在被调用的时候才会被加载进来。在getProps方法中，properties为空时，会触发loadResources()执行 123456789101112131415161718192021222324protected synchronized Properties getProps() { if (this.properties == null) { this.properties = new Properties(); Map&lt;String, String[]&gt; backup = new ConcurrentHashMap(this.updatingResource); this.loadResources(this.properties, this.resources, this.quietmode); ... } return this.properties;}//加载默认资源private void loadResources(Properties properties, ArrayList&lt;Configuration.Resource&gt; resources, boolean quiet) { if (this.loadDefaults) { Iterator i$ = defaultResources.iterator(); while(i$.hasNext()) { String resource = (String)i$.next(); this.loadResource(properties, new Configuration.Resource(resource, false), quiet); } if (this.getResource(\"hadoop-site.xml\") != null) { this.loadResource(properties, new Configuration.Resource(\"hadoop-site.xml\", false), quiet); }} hadoop配置文件解析 hadoop 的配置文件都是XML文件，JAXP(JAVA API for XML processing)是一种稳定的、可靠的XML处理API，支持两种XML处理方法 SAX解析(Simple API for XML) DOM解析(Documnet Object Model) hadoop使用的DOM解析 两种解析的区别 SAX 提供了一种流式的、事件驱动的XML处理方式 缺点：编写处理逻辑比较复杂 优势：适合处理大的XML文件 DOM 的工作方式是： 首先一次性将XML文档加入内存 然后在内存创建一个“树形结构”，也就是对象模型 然后使用对象提供的接口访问文档，进而操作文档 Configurable接口 Configurable是一个很简单的接口，位于org.apache.hadoop.conf包中 类图 hadoop 代码中存在大量实现了该接口的类，可以通过setConf方法设置配置参数简化创建和setConf的两个步骤java反射机制实现，利用org.apache.hadoop.util.ReflectionUtils的newInstance方法 1public static &lt;T&gt; T newInstance(Class&lt;T&gt; theClass, Configuration conf) 该方法调用了ReflectionUtils中的setConf方法 12345678910public static void setConf(Object theObject, Configuration conf) { if (conf != null) { if (theObject instanceof Configurable) { ((Configurable)theObject).setConf(conf); } setJobConf(theObject, conf); } }","link":"/post/fa571a7.html"},{"title":"hadoop序列化","text":"序列化介绍 序列化是一种将对象的状态信息转化成可以存储或者传输的过程不是某一种编程语言所独有的特性 Java序列化 Java通过实现Serializable接口 1234567import java. io.Serializable ;／＊＊定义一个可以序列化的 App 信息类. */public class Appinfo implements Serializable{ ／／序列化标识 private static final long serialVersionUID = 11 ;} Hadoop 不使用Java序列化原因 Java 自带的序列化机制占用内存空间大，额外的开销会导致速度降低，Hadoop对序列化的要求较高，需要保证序列化速度快、体积小、占用带宽低等特性 Hadoop 序列化机制是将对象序列化到流中，而 Java 序列化机制是不断创建新对象 Hadoop 序列化 在 Hadoop 序列化机制中，org.apache.hadoop.io包中定义了大量的可序列化对象均实现Wriable接口的两个函数， 12(1) write：将对象写入字节流：(2) readFields：从字节流中解析出对象。 优势1231. 减少垃圾回收：从流中反序列化数据到当前对象，重复使用当前对象，减少了垃圾回收GC ;2. 减少网络流量 ： 序列化和反序列化对象类型不变 ，因此可以只保存必要的数据来减少网络流量；3. 提升 I/O 效率 ： 由于序列化和反序列化的数据量减少了，配合Hadoop压缩机制，可以提升I/O效率。","link":"/post/20979.html"},{"title":"MapReduce介绍","text":"MapReduce的定义 &emsp;&emsp;Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架。&emsp;&emsp;Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 MapReduce优缺点优点 MapReduce 易于编程。它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 良好的扩展性。当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 高容错性。MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop内部完成的。 适合PB级以上海量数据的离线处理。这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，MapReduce很难做到。缺点 MapReduce不擅长做实时计算、流式计算、DAG（有向图）计算。 实时计算。MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 流式计算。流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。 DAG（有向图）计算。多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 MapReduce核心思想 分布式的运算程序往往需要分成至少2个阶段。第一个阶段的maptask并发实例，完全并行运行，互不相干。 第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出。 MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行。 MapReduce进程 一个完整的mapreduce程序在分布式运行时有三类实例进程： MrAppMaster：负责整个程序的过程调度及状态协调。 MapTask：负责map阶段的整个数据处理流程。 ReduceTask：负责reduce阶段的整个数据处理流程。 MapReduce编程规范 用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端) Mapper阶段 12345（1）用户自定义的Mapper要继承自己的父类 （2）Mapper的输入数据是KV对的形式（KV的类型可自定义） （3）Mapper中的业务逻辑写在map()方法中 （4）Mapper的输出数据是KV对的形式（KV的类型可自定义） （5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 Reduce阶段 1234（1）用户自定义的Reducer要继承自己的父类 （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV （3）Reducer的业务逻辑写在reduce()方法中 （4）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 第一代和第二代MapReduce的区别","link":"/post/5429.html"},{"title":"HDFS快照管理","text":"快照管理 快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。 快照影响 快照创建瞬间完成，所耗时间成本为O(1) 快照修改时才会使用额外的额外的内存空间，内存成本O(M),M表示修改过的文件或目录数 快照记录块和文件大小，不对DataNode中的块进行复制 说明：* 快照可以在HDFS任何目录下设置，一个目录最多容纳65536个并发快照 基本语法12345678（1）hdfs dfsadmin -allowSnapshot 路径 （功能描述：开启指定目录的快照功能）（2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）（3）hdfs dfs -createSnapshot 路径 （功能描述：对目录创建快照）（4）hdfs dfs -createSnapshot 路径 名称 （功能描述：指定名称创建快照）（5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）（6）hdfs lsSnapshottableDir （功能描述：列出当前用户所有已快照目录）（7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处）（8）hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt; （功能描述：删除快照） 案例实操 开启/禁用指定目录的快照功能 指定创建目录的位置为 /tmp/snapshot(即快照的存储目录)，在指定目录之前必须创建目录，不然会报错 12hdfs dfsadmin -allowSnapshot /tmp/snapshot hdfs dfsadmin -disallowSnapshot /tmp/snapshot //禁用时，对应的目录不允许存在快照 对目录创建快照 只有被开启快照功能的目录才能创建快照 123hdfs dfs -createSnapshot /tmp/snapshot // 对目录创建快照hdfs dfs -createSnapshot /tmp/snapshot snapshot //重命名快照（注：快照是只读的，无法修改名）通过web访问hdfs://Master:9000/tmp/snapshot/.snapshot/s…..// 快照和源文件使用相同数据块 查看快照 12hdfs dfs -lsr /tmp/snapshot/.snapshot/ //查看快照目录的详细信息hdfs lsSnapshottableDir //查看所有允许快照的目录 更改快照名字 12345hdfs dfs -renameSnapshot /tmp/snapshot/ snapshot snapshot1 注：路径只是你创建得名字/tmp/snapshot，不要带后边得/tmp/snapshot/.snapshot/，不然会出现hdfs dfs -renameSnapshot /tmp/snapshot/.snapshot/ snapshot1 snapshotrenameSnapshot: Modification on a read-only snapshot is disallowed 比较两个快照目录的不同之处 1234[root@vmaster opt]# hdfs snapshotDiff /tmp/snapshot s1 s2Difference between snapshot s1 and snapshot s2 under directory /tmp/snapshot:M .+ ./p 符号的意义： 符号 含义 + 文件或者目录被创建 - 文件或目录被删除 M 文件或目录被修改 R 文件或目录被重命名 恢复快照 123451.自定义创建一个快照名：hdfs dfs -createSnapshot /HAHA1 snapshot12.展示原文件包含内容：Hadoop fs -ls /HAHA13.里面有五个文件、删除其中1~2个/HAHA1/.snapshot/snapshot14.回复快照：hdfs dfs -cp /HAHA1/.snapshot/snapshot1 /snapshot 删除快照 12dfs dfs -deleteSnapshot 快照目录 快照名称dfs dfs -deleteSnapshot /tmp/snapshot snapshot1","link":"/post/21534.html"}],"tags":[{"name":"分布式存储管理","slug":"分布式存储管理","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%AE%A1%E7%90%86/"},{"name":"NoSSQL特点","slug":"NoSSQL特点","link":"/tags/NoSSQL%E7%89%B9%E7%82%B9/"},{"name":"什么是大数据","slug":"什么是大数据","link":"/tags/%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"hadoop配置文件解析","slug":"hadoop配置文件解析","link":"/tags/hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/"},{"name":"DOM解析","slug":"DOM解析","link":"/tags/DOM%E8%A7%A3%E6%9E%90/"},{"name":"Hadooop介绍","slug":"Hadooop介绍","link":"/tags/Hadooop%E4%BB%8B%E7%BB%8D/"},{"name":"Google三宝","slug":"Google三宝","link":"/tags/Google%E4%B8%89%E5%AE%9D/"},{"name":"Hadoop RPC","slug":"Hadoop-RPC","link":"/tags/Hadoop-RPC/"},{"name":"Hadoop 通信机制","slug":"Hadoop-通信机制","link":"/tags/Hadoop-%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/"},{"name":"MapReduce 通信协议","slug":"MapReduce-通信协议","link":"/tags/MapReduce-%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"},{"name":"Hadoop 协议","slug":"Hadoop-协议","link":"/tags/Hadoop-%E5%8D%8F%E8%AE%AE/"},{"name":"Windows操作系统配置文件","slug":"Windows操作系统配置文件","link":"/tags/Windows%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Java配置文件","slug":"Java配置文件","link":"/tags/Java%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"Hadoop配置文件","slug":"Hadoop配置文件","link":"/tags/Hadoop%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/"},{"name":"hadoop序列化","slug":"hadoop序列化","link":"/tags/hadoop%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"MapReduce介绍","slug":"MapReduce介绍","link":"/tags/MapReduce%E4%BB%8B%E7%BB%8D/"},{"name":"HDFS快照","slug":"HDFS快照","link":"/tags/HDFS%E5%BF%AB%E7%85%A7/"}],"categories":[{"name":"BigData","slug":"BigData","link":"/categories/BigData/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"Hadoop","slug":"BigData/Hadoop","link":"/categories/BigData/Hadoop/"},{"name":"Hadoop_common","slug":"BigData/Hadoop/Hadoop-common","link":"/categories/BigData/Hadoop/Hadoop-common/"},{"name":"MapReduce","slug":"BigData/Hadoop/MapReduce","link":"/categories/BigData/Hadoop/MapReduce/"},{"name":"HDFS","slug":"BigData/Hadoop/HDFS","link":"/categories/BigData/Hadoop/HDFS/"}]}